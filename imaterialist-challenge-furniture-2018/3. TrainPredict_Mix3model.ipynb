{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TrainPredict_Mix3model\n",
    "## Result:\n",
    "- Kaggle score:\n",
    "\n",
    "## Tensorboard:\n",
    "- Input at command: tensorboard --logdir=./log\n",
    "- Input at browser: http://127.0.0.1:6006\n",
    "\n",
    "## Reference:\n",
    "- https://www.kaggle.com/codename007/a-very-extensive-landmark-exploratory-analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: ic_furniture2018_TrainPredict_Mix3model_20180416_134540\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'ic_furniture2018'\n",
    "step_name = 'TrainPredict_Mix3model'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = project_name + '_' + step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import zipfile\n",
    "import pickle\n",
    "import math\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_folder: \t\t\t/data1/kaggle/imaterialist-challenge-furniture-2018/input\n",
      "output_folder: \t\t\t/data1/kaggle/imaterialist-challenge-furniture-2018/output\n",
      "model_folder: \t\t\t/data1/kaggle/imaterialist-challenge-furniture-2018/model\n",
      "feature_folder: \t\t/data1/kaggle/imaterialist-challenge-furniture-2018/feature\n",
      "post_pca_feature_folder: \t/data1/kaggle/imaterialist-challenge-furniture-2018/post_pca_feature\n",
      "log_folder: \t\t\t/data1/kaggle/imaterialist-challenge-furniture-2018/log\n",
      "\n",
      "train_json_file: \t\t/data1/kaggle/imaterialist-challenge-furniture-2018/input/train.json\n",
      "val_json_file: \t\t\t/data1/kaggle/imaterialist-challenge-furniture-2018/input/validation.json\n",
      "test_json_file: \t\t/data1/kaggle/imaterialist-challenge-furniture-2018/input/test.json\n",
      "\n",
      "train_csv_file: \t\t/data1/kaggle/imaterialist-challenge-furniture-2018/input/train.csv\n",
      "val_csv_file: \t\t\t/data1/kaggle/imaterialist-challenge-furniture-2018/input/validation.csv\n",
      "test_csv_file: \t\t\t/data1/kaggle/imaterialist-challenge-furniture-2018/input/test.csv\n",
      "\n",
      "sample_submission_csv_file: \t/data1/kaggle/imaterialist-challenge-furniture-2018/input/sample_submission_randomlabel.csv\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "post_pca_feature_folder = os.path.join(cwd, 'post_pca_feature')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t%s' % input_folder)\n",
    "print('output_folder: \\t\\t\\t%s' % output_folder)\n",
    "print('model_folder: \\t\\t\\t%s' % model_folder)\n",
    "print('feature_folder: \\t\\t%s' % feature_folder)\n",
    "print('post_pca_feature_folder: \\t%s' % post_pca_feature_folder)\n",
    "print('log_folder: \\t\\t\\t%s' % log_folder)\n",
    "\n",
    "org_train_folder = os.path.join(input_folder, 'org_train')\n",
    "org_val_folder = os.path.join(input_folder, 'org_val')\n",
    "org_test_folder = os.path.join(input_folder, 'org_test')\n",
    "train_folder = os.path.join(input_folder, 'data_train')\n",
    "val_folder = os.path.join(input_folder, 'data_val')\n",
    "test_folder = os.path.join(input_folder, 'data_test')\n",
    "test_sub_folder = os.path.join(test_folder, 'test')\n",
    "\n",
    "if not os.path.exists(post_pca_feature_folder):\n",
    "    os.mkdir(post_pca_feature_folder)\n",
    "    print('Create folder: %s' % post_pca_feature_folder)\n",
    "\n",
    "train_json_file = os.path.join(input_folder, 'train.json')\n",
    "val_json_file = os.path.join(input_folder, 'validation.json')\n",
    "test_json_file = os.path.join(input_folder, 'test.json')\n",
    "print('\\ntrain_json_file: \\t\\t%s' % train_json_file)\n",
    "print('val_json_file: \\t\\t\\t%s' % val_json_file)\n",
    "print('test_json_file: \\t\\t%s' % test_json_file)\n",
    "\n",
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "val_csv_file = os.path.join(input_folder, 'validation.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "print('\\ntrain_csv_file: \\t\\t%s' % train_csv_file)\n",
    "print('val_csv_file: \\t\\t\\t%s' % val_csv_file)\n",
    "print('test_csv_file: \\t\\t\\t%s' % test_csv_file)\n",
    "\n",
    "sample_submission_csv_file = os.path.join(input_folder, 'sample_submission_randomlabel.csv')\n",
    "print('\\nsample_submission_csv_file: \\t%s' % sample_submission_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv.shape is (194828, 3).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://img13.360buyimg.com/imgzone/jfs/t2857/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>http://www.tengdakeli.cn/350/timg01/uploaded/i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  label_id                                                url\n",
       "0         1         5  https://img13.360buyimg.com/imgzone/jfs/t2857/...\n",
       "1         2         5  http://www.tengdakeli.cn/350/timg01/uploaded/i..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_csv.shape is (6400, 3).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>http://www.ghs.net/public/images/fb/3d/51/3beb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>https://img.alicdn.com/imgextra/TB2chFei9YH8KJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  label_id                                                url\n",
       "0         1        38  http://www.ghs.net/public/images/fb/3d/51/3beb...\n",
       "1         2        63  https://img.alicdn.com/imgextra/TB2chFei9YH8KJ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_csv.shape is (12800, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://img13.360buyimg.com/imgzone/jfs/t13174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>http://img35.ddimg.cn/79/22/1258168705-1_u.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                                url\n",
       "0         1  https://img13.360buyimg.com/imgzone/jfs/t13174...\n",
       "1         2     http://img35.ddimg.cn/79/22/1258168705-1_u.jpg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_csv.shape is (12800, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://img13.360buyimg.com/imgzone/jfs/t13174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>http://img35.ddimg.cn/79/22/1258168705-1_u.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                                url\n",
       "0         1  https://img13.360buyimg.com/imgzone/jfs/t13174...\n",
       "1         2     http://img35.ddimg.cn/79/22/1258168705-1_u.jpg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission_csv.shape is (12800, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  predicted\n",
       "0   1         57\n",
       "1   2         74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(train_csv_file)\n",
    "print('train_csv.shape is {0}.'.format(train_csv.shape))\n",
    "display(train_csv.head(2))\n",
    "\n",
    "val_csv = pd.read_csv(val_csv_file)\n",
    "print('val_csv.shape is {0}.'.format(val_csv.shape))\n",
    "display(val_csv.head(2))\n",
    "\n",
    "test_csv = pd.read_csv(test_csv_file)\n",
    "print('test_csv.shape is {0}.'.format(test_csv.shape))\n",
    "display(test_csv.head(2))\n",
    "\n",
    "test_csv = pd.read_csv(test_csv_file)\n",
    "print('test_csv.shape is {0}.'.format(test_csv.shape))\n",
    "display(test_csv.head(2))\n",
    "\n",
    "sample_submission_csv = pd.read_csv(sample_submission_csv_file)\n",
    "print('sample_submission_csv.shape is {0}.'.format(sample_submission_csv.shape))\n",
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(id_2_train_label_id_dict)=194828\n",
      "id: 1, \tlandmark_id:5\n",
      "id: 2, \tlandmark_id:5\n",
      "2_5.jpg\n"
     ]
    }
   ],
   "source": [
    "train_id = train_csv['image_id']\n",
    "train_label_id = train_csv['label_id']\n",
    "\n",
    "id_2_train_label_id_dict = dict(zip(train_id, train_label_id))\n",
    "print('len(id_2_train_label_id_dict)=%d' % len(id_2_train_label_id_dict))\n",
    "\n",
    "index = 0\n",
    "print('id: %s, \\tlandmark_id:%s' % (train_id[index], id_2_train_label_id_dict[train_id[index]]))\n",
    "index = 1\n",
    "print('id: %s, \\tlandmark_id:%s' % (train_id[index], id_2_train_label_id_dict[train_id[index]]))\n",
    "\n",
    "image_file = '%s_%s.jpg' % (train_id[index], id_2_train_label_id_dict[train_id[index]])\n",
    "print(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(id_2_val_label_id_dict)=6400\n",
      "id: 1, \tlandmark_id:38\n",
      "id: 2, \tlandmark_id:63\n",
      "2_63.jpg\n"
     ]
    }
   ],
   "source": [
    "val_id = val_csv['image_id']\n",
    "val_label_id = val_csv['label_id']\n",
    "\n",
    "id_2_val_label_id_dict = dict(zip(val_id, val_label_id))\n",
    "print('len(id_2_val_label_id_dict)=%d' % len(id_2_val_label_id_dict))\n",
    "\n",
    "index = 0\n",
    "print('id: %s, \\tlandmark_id:%s' % (val_id[index], id_2_val_label_id_dict[val_id[index]]))\n",
    "index = 1\n",
    "print('id: %s, \\tlandmark_id:%s' % (val_id[index], id_2_val_label_id_dict[val_id[index]]))\n",
    "\n",
    "image_file = '%s_%s.jpg' % (val_id[index], id_2_val_label_id_dict[val_id[index]])\n",
    "print(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "id: 2\n",
      "2.jpg\n"
     ]
    }
   ],
   "source": [
    "test_id = test_csv['image_id']\n",
    "\n",
    "index = 0\n",
    "print('id: %s' % (test_id[index]))\n",
    "index = 1\n",
    "print('id: %s' % (test_id[index]))\n",
    "\n",
    "image_file = '%s.jpg' % (test_id[index])\n",
    "print(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 ms, sys: 4 ms, total: 56 ms\n",
      "Wall time: 74.8 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "\n",
    "def load_h5_data(data_str, feature_folder, file_reg, model_name, time_str):\n",
    "    x_data = {}\n",
    "    y_data = {}\n",
    "    \n",
    "    feature_model = os.path.join(feature_folder, file_reg % (model_name, time_str))\n",
    "    for filename in [feature_model]:\n",
    "        with h5py.File(filename, 'r') as h:\n",
    "            x_data = np.array(h[data_str])\n",
    "            y_data = np.array(h['%s_labels' % data_str])\n",
    "    return x_data, y_data\n",
    "\n",
    "def load_h5_test(feature_folder, file_reg, model_name, time_str):\n",
    "    x_test = {}\n",
    "    \n",
    "    feature_model = os.path.join(feature_folder, file_reg % (model_name, time_str))\n",
    "    for filename in [feature_model]:\n",
    "        with h5py.File(filename, 'r') as h:\n",
    "            x_test = np.array(h['test'])\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_300_20180415-150022.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_200_20180415-140848.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180415-131901.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_300_20180415-111333.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_450_20180409-115039.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_450_20180409-191753.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_450_20180410-032144.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_450_20180410-112448.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_450_20180410-193018.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180331-163122.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180401-185347.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180401-224426.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180402-023138.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180402-062014.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180402-101021.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180402-140156.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180402-175506.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180402-214919.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180403-014551.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180406-070324.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180406-102130.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180406-123522.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_20180406-144857.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180406-230352.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-010446.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-031951.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-053509.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-075132.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-100849.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-122723.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-144712.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-170832.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180407-193230.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_300_20180415-150022.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_200_20180415-140848.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180415-131901.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_300_20180415-111333.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_450_20180409-115039.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_450_20180409-191753.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_450_20180410-032144.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_450_20180410-112448.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_450_20180410-193018.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180331-163122.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180401-185347.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180401-224426.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180402-023138.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180402-062014.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180402-101021.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180402-140156.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180402-175506.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180402-214919.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180403-014551.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180406-070324.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180406-102130.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180406-123522.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_20180406-144857.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180406-230352.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-010446.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-031951.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-053509.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-075132.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-100849.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-122723.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-144712.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-170832.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180407-193230.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_300_20180415-150022.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_200_20180415-140848.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180415-131901.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_300_20180415-111333.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_450_20180409-115039.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_450_20180409-191753.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_450_20180410-032144.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_450_20180410-112448.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_450_20180410-193018.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180331-163122.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180401-185347.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180401-224426.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180402-023138.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180402-062014.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180402-101021.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180402-140156.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180402-175506.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180402-214919.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180403-014551.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180406-070324.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180406-102130.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180406-123522.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_20180406-144857.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180406-230352.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-010446.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-031951.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-053509.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-075132.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-100849.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-122723.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-144712.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-170832.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180407-193230.h5\n",
      "True\n",
      "  300_20180415-150022\n",
      "  200_20180415-140848\n",
      "  150_20180415-131901\n",
      "  300_20180415-111333\n",
      "  450_20180409-115039\n",
      "  450_20180409-191753\n",
      "  450_20180410-032144\n",
      "  450_20180410-112448\n",
      "  450_20180410-193018\n",
      "  20180331-163122\n",
      "  20180401-185347\n",
      "  20180401-224426\n",
      "  20180402-023138\n",
      "  20180402-062014\n",
      "  20180402-101021\n",
      "  20180402-140156\n",
      "  20180402-175506\n",
      "  20180402-214919\n",
      "  20180403-014551\n",
      "  20180406-070324\n",
      "  20180406-102130\n",
      "  20180406-123522\n",
      "  20180406-144857\n",
      "  150_20180406-230352\n",
      "  150_20180407-010446\n",
      "  150_20180407-031951\n",
      "  150_20180407-053509\n",
      "  150_20180407-075132\n",
      "  150_20180407-100849\n",
      "  150_20180407-122723\n",
      "  150_20180407-144712\n",
      "  150_20180407-170832\n",
      "  150_20180407-193230\n"
     ]
    }
   ],
   "source": [
    "def is_files_existed(feature_folder, file_reg, model_names, time_strs):\n",
    "    for model_name in model_names:\n",
    "        for time_str in time_strs:\n",
    "            file_name = file_reg % (model_name, time_str)\n",
    "            file_path = os.path.join(feature_folder, file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                print('File not existed: %s' % file_path)\n",
    "                return False\n",
    "            else:\n",
    "                print('File existed: %s' % file_path)\n",
    "    return True\n",
    "\n",
    "# Test\n",
    "file_reg = 'feature_%s_%s.h5'\n",
    "model_names = [\n",
    "#     'MobileNet', \n",
    "#     'VGG16',\n",
    "#     'VGG19',\n",
    "#     'ResNet50',\n",
    "#     'DenseNet121',\n",
    "#     'DenseNet169',\n",
    "#     'DenseNet201',\n",
    "    'Xception',\n",
    "    'InceptionV3',\n",
    "    'InceptionResNetV2'\n",
    "]\n",
    "time_strs = [\n",
    "    '300_20180415-150022',\n",
    "    '200_20180415-140848',\n",
    "    '150_20180415-131901',\n",
    "    '300_20180415-111333',   \n",
    "    \n",
    "    '450_20180409-115039',\n",
    "    '450_20180409-191753',\n",
    "    '450_20180410-032144',\n",
    "    '450_20180410-112448',\n",
    "    '450_20180410-193018',\n",
    "    \n",
    "    '20180331-163122',\n",
    "    '20180401-185347',\n",
    "    '20180401-224426',\n",
    "    '20180402-023138',\n",
    "    '20180402-062014',\n",
    "    '20180402-101021',\n",
    "    '20180402-140156',\n",
    "    '20180402-175506',\n",
    "    '20180402-214919',\n",
    "    '20180403-014551',\n",
    "    \n",
    "    '20180406-070324',\n",
    "    '20180406-102130',\n",
    "    '20180406-123522',\n",
    "    '20180406-144857',\n",
    "    \n",
    "    '150_20180406-230352',\n",
    "    '150_20180407-010446',\n",
    "    '150_20180407-031951',\n",
    "    '150_20180407-053509',\n",
    "    '150_20180407-075132',\n",
    "    '150_20180407-100849',\n",
    "    '150_20180407-122723',\n",
    "    '150_20180407-144712',\n",
    "    '150_20180407-170832',\n",
    "    '150_20180407-193230'\n",
    "]\n",
    "\n",
    "print('*'*100)\n",
    "print(is_files_existed(feature_folder, file_reg, model_names, time_strs))\n",
    "\n",
    "\n",
    "def time_str_generator(time_strs):\n",
    "    while(1):\n",
    "        for time_str in time_strs:\n",
    "            print('  ' + time_str)\n",
    "            yield time_str\n",
    "            \n",
    "# Test\n",
    "time_str_gen = time_str_generator(time_strs)\n",
    "for i in range(len(time_strs)):\n",
    "    next(time_str_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 128\n",
      "********************************************************************************\n",
      "len(train_data): 194828\n",
      "batch_size: 1024\n",
      "steps_per_epoch_train: 190\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180415-131901.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180415-131901.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180415-131901.h5\n",
      "  150_20180415-131901\n",
      "(1024, 5632) (1024, 128)\n",
      "(1024, 5632) (1024, 128)\n",
      "********************************************************************************\n",
      "len(val_data): 6400\n",
      "batch_size: 1024\n",
      "steps_per_epoch_val: 6\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_Xception_150_20180415-131901.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionV3_150_20180415-131901.h5\n",
      "File existed: /data1/kaggle/imaterialist-challenge-furniture-2018/feature/feature_InceptionResNetV2_150_20180415-131901.h5\n",
      "  150_20180415-131901\n",
      "(1024, 5632) (1024, 128)\n",
      "(1024, 5632) (1024, 128)\n",
      "data_dim: 5632\n",
      "********************************************************************************\n",
      "(191261, 5632)\n",
      "191261\n",
      "(6301, 5632)\n",
      "6301\n",
      "CPU times: user 16.4 s, sys: 17.7 s, total: 34.1 s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_time_str_feature_data(data_str, feature_folder, file_reg, model_names, time_str):\n",
    "    x_data_time_strs = []\n",
    "    y_data_time_strs = None\n",
    "    for model_name in model_names:\n",
    "        x_data_time_str, y_data_time_str = load_h5_data(data_str, feature_folder, file_reg, model_name, time_str)\n",
    "        # Around data to 3 decimals to calculate computation\n",
    "        x_data_time_str = np.round(x_data_time_str, decimals=3)\n",
    "        x_data_time_strs.append(x_data_time_str)\n",
    "        y_data_time_strs = y_data_time_str\n",
    "    x_data_time_strs = np.concatenate(x_data_time_strs, axis=-1)\n",
    "#     print(x_data_time_strs.shape)\n",
    "#     print(y_data_time_strs.shape)\n",
    "    return x_data_time_strs, y_data_time_strs\n",
    "\n",
    "def data_generator_folder(data_str, feature_folder, file_reg, model_names, time_strs, batch_size, num_classes):\n",
    "    assert is_files_existed(feature_folder, file_reg, model_names, time_strs)\n",
    "\n",
    "    time_str_gen = time_str_generator(time_strs)\n",
    "    x_data, y_data = load_time_str_feature_data(data_str, feature_folder, file_reg, model_names, next(time_str_gen))\n",
    "#     x_data, y_data = shuffle(x_data, y_data, random_state=2018)\n",
    "    x_data, y_data = shuffle(x_data, y_data, random_state=None)\n",
    "    len_x_data = len(x_data)\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    while(1):\n",
    "        end_index = start_index + batch_size\n",
    "        if end_index < len_x_data:\n",
    "#             print(start_index, end_index, end=' ')\n",
    "            x_batch = x_data[start_index: end_index, :]\n",
    "            y_batch = y_data[start_index: end_index]\n",
    "            y_batch_cat = to_categorical(y_batch, num_classes)\n",
    "            \n",
    "            start_index = start_index + batch_size\n",
    "#             print(x_batch.shape, y_batch_cat.shape)\n",
    "            yield x_batch, y_batch_cat\n",
    "        else:\n",
    "            end_index = end_index-len_x_data\n",
    "#             print(start_index, end_index, end=' ')\n",
    "            x_data_old = np.array(x_data[start_index:, :], copy=True)\n",
    "            y_data_old = np.array(y_data[start_index:], copy=True)\n",
    "            # Load new datas\n",
    "            x_data, y_data = load_time_str_feature_data(data_str, feature_folder, file_reg, model_names, next(time_str_gen))\n",
    "#             x_data, y_data = shuffle(x_data, y_data, random_state=2018)\n",
    "            x_data, y_data = shuffle(x_data, y_data, random_state=None)\n",
    "            len_x_data = len(x_data)\n",
    "            gc.collect()\n",
    "            x_batch = np.vstack((x_data_old, x_data[:end_index, :]))\n",
    "            y_batch = np.concatenate([y_data_old, y_data[:end_index]])\n",
    "            y_batch_cat = to_categorical(y_batch, num_classes)\n",
    "            \n",
    "            start_index = end_index\n",
    "#             print(x_batch.shape, y_batch_cat.shape)\n",
    "            yield x_batch, y_batch_cat\n",
    "        \n",
    "    \n",
    "# x_train = np.concatenate([x_train_Xception, x_train_InceptionV3, x_train_InceptionResNetV2], axis=-1)\n",
    "\n",
    "num_classes = len(list(set(train_label_id)))\n",
    "print('num_classes: %s' % num_classes)\n",
    "\n",
    "# Test\n",
    "file_reg = 'feature_%s_%s.h5'\n",
    "model_names = [\n",
    "#     'MobileNet', \n",
    "#     'VGG16',\n",
    "#     'VGG19',\n",
    "#     'ResNet50',\n",
    "#     'DenseNet121',\n",
    "#     'DenseNet169',\n",
    "#     'DenseNet201',\n",
    "    'Xception',\n",
    "    'InceptionV3',\n",
    "    'InceptionResNetV2'\n",
    "]\n",
    "time_strs = [\n",
    "#     '300_20180415-150022',\n",
    "#     '200_20180415-140848',\n",
    "    '150_20180415-131901',\n",
    "#     '300_20180415-111333',\n",
    "\n",
    "#     '450_20180409-115039',\n",
    "#     '450_20180409-191753',\n",
    "#     '450_20180410-032144',\n",
    "#     '450_20180410-112448',\n",
    "#     '450_20180410-193018',\n",
    "    \n",
    "#     '20180331-163122',\n",
    "#     '20180401-185347',\n",
    "#     '20180401-224426',\n",
    "#     '20180402-023138',\n",
    "#     '20180402-062014',\n",
    "#     '20180402-101021',\n",
    "#     '20180402-140156',\n",
    "#     '20180402-175506',\n",
    "#     '20180402-214919',\n",
    "#     '20180403-014551',\n",
    "    \n",
    "#     '20180406-070324',\n",
    "#     '20180406-102130',\n",
    "#     '20180406-123522',\n",
    "#     '20180406-144857',\n",
    "    \n",
    "#     '150_20180406-230352',\n",
    "#     '150_20180407-010446',\n",
    "#     '150_20180407-031951',\n",
    "#     '150_20180407-053509',\n",
    "#     '150_20180407-075132',\n",
    "#     '150_20180407-100849',\n",
    "#     '150_20180407-122723',\n",
    "#     '150_20180407-144712',\n",
    "#     '150_20180407-170832',\n",
    "#     '150_20180407-193230'\n",
    "]\n",
    "\n",
    "print('*' * 80)\n",
    "len_train_csv = train_csv.shape[0]\n",
    "steps_per_epoch_train = int(len_train_csv/batch_size) * len(time_strs)\n",
    "print('len(train_data): %s' % len_train_csv)\n",
    "print('batch_size: %s' % batch_size)\n",
    "print('steps_per_epoch_train: %s' % steps_per_epoch_train)\n",
    "\n",
    "train_gen = data_generator_folder('train', feature_folder, file_reg, model_names, time_strs, batch_size, num_classes)\n",
    "batch_data = next(train_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "batch_data = next(train_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "# for i in range(steps_per_epoch_train*5):\n",
    "#     next(train_gen)\n",
    "\n",
    "print('*' * 80)\n",
    "len_val_csv = val_csv.shape[0]\n",
    "steps_per_epoch_val = int(len_val_csv/batch_size)\n",
    "print('len(val_data): %s' % len_val_csv)\n",
    "print('batch_size: %s' % batch_size)\n",
    "print('steps_per_epoch_val: %s' % steps_per_epoch_val)\n",
    "val_gen = data_generator_folder('val', feature_folder, file_reg, model_names, time_strs, batch_size, num_classes)\n",
    "batch_data = next(val_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "batch_data = next(val_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "\n",
    "data_dim = batch_data[0].shape[1]\n",
    "print('data_dim: %s' % data_dim)\n",
    "\n",
    "print('*' * 80)\n",
    "x_train, y_train = load_time_str_feature_data('train', feature_folder, file_reg, model_names, time_strs[0])\n",
    "x_val, y_val = load_time_str_feature_data('val', feature_folder, file_reg, model_names, time_strs[0])\n",
    "print(x_train.shape)\n",
    "print(len(y_train))\n",
    "print(x_val.shape)\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12652, 5632)\n"
     ]
    }
   ],
   "source": [
    "def load_time_str_feature_test(feature_folder, file_reg, model_names, time_str):\n",
    "    x_test_time_strs = []\n",
    "    for model_name in model_names:\n",
    "        file_name = file_reg % (model_name, time_str)\n",
    "        file_path = os.path.join(feature_folder, file_name)\n",
    "        x_test_time_str = load_h5_test(feature_folder, file_reg, model_name, time_str)\n",
    "        x_test_time_str = np.round(x_test_time_str, decimals=3)\n",
    "        x_test_time_strs.append(x_test_time_str)\n",
    "    x_test_time_strs = np.concatenate(x_test_time_strs, axis=-1)\n",
    "#     print(x_test_time_strs.shape)\n",
    "    return x_test_time_strs\n",
    "\n",
    "x_test = load_time_str_feature_test(feature_folder, file_reg, model_names, time_strs[0])\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, Flatten, Conv2D, MaxPooling2D, BatchNormalization, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 0.0003  0.0003 \n",
      "log_dir:/data1/kaggle/imaterialist-challenge-furniture-2018/log/ic_furniture2018_Train-Predict_Mix3model_20180416_133125\n"
     ]
    }
   ],
   "source": [
    "def get_lr(x):\n",
    "#     lr = round(3e-4 * 0.96 ** x, 6)\n",
    "#     if lr < 1e-5:\n",
    "#         lr = 1e-5\n",
    "    \n",
    "#     if x <= 25:\n",
    "#         lr = 3e-4\n",
    "#     elif x > 25 and x <= 30:\n",
    "#         lr = 1e-4\n",
    "#     else:\n",
    "#         lr = 5e-5\n",
    "\n",
    "    lr = 3e-4\n",
    "    print(lr, end='  ')\n",
    "    return lr\n",
    "\n",
    "for i in range(30):\n",
    "    print(get_lr(i), end=' ')\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "annealer = LearningRateScheduler(get_lr)\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='acc', min_delta=0.001, patience=0.0005, verbose=0, mode='auto')\n",
    "\n",
    "log_dir = os.path.join(log_folder, run_name)\n",
    "print('\\nlog_dir:' + log_dir)\n",
    "tensorBoard = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = []\n",
    "# callbacks = [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4096)              23072768  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               524416    \n",
      "=================================================================\n",
      "Total params: 40,411,264\n",
      "Trainable params: 40,394,880\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(Dense(4096, input_shape=(data_dim,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "model.compile(optimizer=Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "cpu_amount = cpu_count()\n",
    "print(cpu_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "  150_20180415-131901\n",
      "186/190 [============================>.] - ETA: 0s - loss: 2.2884 - acc: 0.4752  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 2.2781 - acc: 0.4768  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 21s 108ms/step - loss: 2.2750 - acc: 0.4773 - val_loss: 1.5301 - val_acc: 0.5978\n",
      "Epoch 2/35\n",
      "182/190 [===========================>..] - ETA: 0s - loss: 1.5338 - acc: 0.6055  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 1.5259 - acc: 0.6070  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 92ms/step - loss: 1.5237 - acc: 0.6074 - val_loss: 1.3249 - val_acc: 0.6388\n",
      "Epoch 3/35\n",
      "179/190 [===========================>..] - ETA: 0s - loss: 1.3133 - acc: 0.6507  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 1.3057 - acc: 0.6518  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 1.3044 - acc: 0.6521 - val_loss: 1.2752 - val_acc: 0.6665\n",
      "Epoch 4/35\n",
      "177/190 [==========================>...] - ETA: 1s - loss: 1.1638 - acc: 0.6802  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 1.1566 - acc: 0.6813  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 92ms/step - loss: 1.1555 - acc: 0.6815 - val_loss: 1.2172 - val_acc: 0.6729\n",
      "Epoch 5/35\n",
      "173/190 [==========================>...] - ETA: 1s - loss: 1.0570 - acc: 0.7035  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 1.0472 - acc: 0.7053  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 1.0464 - acc: 0.7055 - val_loss: 1.2204 - val_acc: 0.6725\n",
      "Epoch 6/35\n",
      "169/190 [=========================>....] - ETA: 1s - loss: 0.9750 - acc: 0.7218  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.9663 - acc: 0.7238  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.9659 - acc: 0.7239 - val_loss: 1.2076 - val_acc: 0.6800\n",
      "Epoch 7/35\n",
      "166/190 [=========================>....] - ETA: 1s - loss: 0.9040 - acc: 0.7379  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.8927 - acc: 0.7403  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 17s 91ms/step - loss: 0.8914 - acc: 0.7406 - val_loss: 1.1895 - val_acc: 0.6878\n",
      "Epoch 8/35\n",
      "165/190 [=========================>....] - ETA: 2s - loss: 0.8393 - acc: 0.7537  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7559  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 0.8280 - acc: 0.7561 - val_loss: 1.1685 - val_acc: 0.6916\n",
      "Epoch 9/35\n",
      "160/190 [========================>.....] - ETA: 2s - loss: 0.7892 - acc: 0.7656  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.7768 - acc: 0.7690  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 92ms/step - loss: 0.7764 - acc: 0.7690 - val_loss: 1.1683 - val_acc: 0.6981\n",
      "Epoch 10/35\n",
      "156/190 [=======================>......] - ETA: 2s - loss: 0.7389 - acc: 0.7777  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.7245 - acc: 0.7814  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 19s 98ms/step - loss: 0.7244 - acc: 0.7814 - val_loss: 1.1901 - val_acc: 0.6893\n",
      "Epoch 11/35\n",
      "154/190 [=======================>......] - ETA: 2s - loss: 0.6980 - acc: 0.7886  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.6838 - acc: 0.7925  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 0.6835 - acc: 0.7925 - val_loss: 1.1349 - val_acc: 0.7051\n",
      "Epoch 12/35\n",
      "150/190 [======================>.......] - ETA: 3s - loss: 0.6596 - acc: 0.7978  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.6448 - acc: 0.8021  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 17s 92ms/step - loss: 0.6445 - acc: 0.8021 - val_loss: 1.1569 - val_acc: 0.6953\n",
      "Epoch 13/35\n",
      "148/190 [======================>.......] - ETA: 3s - loss: 0.6184 - acc: 0.8088  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.6044 - acc: 0.8128  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 0.6040 - acc: 0.8130 - val_loss: 1.1922 - val_acc: 0.6921\n",
      "Epoch 14/35\n",
      "143/190 [=====================>........] - ETA: 3s - loss: 0.5938 - acc: 0.815  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.8198  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.5771 - acc: 0.8199 - val_loss: 1.1803 - val_acc: 0.7035\n",
      "Epoch 15/35\n",
      "140/190 [=====================>........] - ETA: 4s - loss: 0.5539 - acc: 0.8263  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.8307  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.5393 - acc: 0.8307 - val_loss: 1.2111 - val_acc: 0.7044\n",
      "Epoch 16/35\n",
      "136/190 [====================>.........] - ETA: 2s - loss: 0.5272 - acc: 0.8336  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8382  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.5117 - acc: 0.8383 - val_loss: 1.2049 - val_acc: 0.7007\n",
      "Epoch 17/35\n",
      "134/190 [====================>.........] - ETA: 4s - loss: 0.5050 - acc: 0.8400  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.4869 - acc: 0.8454  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.4863 - acc: 0.8456 - val_loss: 1.1865 - val_acc: 0.7093\n",
      "Epoch 18/35\n",
      "131/190 [===================>..........] - ETA: 4s - loss: 0.4763 - acc: 0.8480  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8530  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.4601 - acc: 0.8531 - val_loss: 1.1946 - val_acc: 0.7087\n",
      "Epoch 19/35\n",
      "127/190 [===================>..........] - ETA: 5s - loss: 0.4529 - acc: 0.8547  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8589  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 17s 92ms/step - loss: 0.4380 - acc: 0.8590 - val_loss: 1.2394 - val_acc: 0.7018\n",
      "Epoch 20/35\n",
      "124/190 [==================>...........] - ETA: 5s - loss: 0.4346 - acc: 0.8592  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8652  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.4163 - acc: 0.8652 - val_loss: 1.2446 - val_acc: 0.7004\n",
      "Epoch 21/35\n",
      "121/190 [==================>...........] - ETA: 6s - loss: 0.4095 - acc: 0.8670  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8726  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 0.3908 - acc: 0.8727 - val_loss: 1.2314 - val_acc: 0.7087\n",
      "Epoch 22/35\n",
      "119/190 [=================>............] - ETA: 6s - loss: 0.3987 - acc: 0.8706  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8772  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.3783 - acc: 0.8773 - val_loss: 1.2546 - val_acc: 0.7017\n",
      "Epoch 23/35\n",
      "115/190 [=================>............] - ETA: 6s - loss: 0.3792 - acc: 0.8768  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8828  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 0.3604 - acc: 0.8828 - val_loss: 1.2769 - val_acc: 0.7039\n",
      "Epoch 24/35\n",
      "111/190 [================>.............] - ETA: 7s - loss: 0.3622 - acc: 0.8811  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8880  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.3423 - acc: 0.8880 - val_loss: 1.2618 - val_acc: 0.7049\n",
      "Epoch 25/35\n",
      "106/190 [===============>..............] - ETA: 4s - loss: 0.3441 - acc: 0.8863  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8931  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.3252 - acc: 0.8932 - val_loss: 1.2820 - val_acc: 0.7051\n",
      "Epoch 26/35\n",
      "105/190 [===============>..............] - ETA: 7s - loss: 0.3283 - acc: 0.8919  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8971  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.3123 - acc: 0.8971 - val_loss: 1.2542 - val_acc: 0.7153\n",
      "Epoch 27/35\n",
      "102/190 [===============>..............] - ETA: 8s - loss: 0.3132 - acc: 0.8967  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9028  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.2954 - acc: 0.9028 - val_loss: 1.2993 - val_acc: 0.7039\n",
      "Epoch 28/35\n",
      " 98/190 [==============>...............] - ETA: 9s - loss: 0.3017 - acc: 0.9007  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9065  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 0.2836 - acc: 0.9065 - val_loss: 1.3126 - val_acc: 0.7052\n",
      "Epoch 29/35\n",
      " 95/190 [==============>...............] - ETA: 9s - loss: 0.2907 - acc: 0.9035  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9100  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 94ms/step - loss: 0.2720 - acc: 0.9101 - val_loss: 1.3693 - val_acc: 0.6978\n",
      "Epoch 30/35\n",
      " 92/190 [=============>................] - ETA: 9s - loss: 0.2797 - acc: 0.9073  150_20180415-131901\n",
      "188/190 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9142  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.2601 - acc: 0.9142 - val_loss: 1.3770 - val_acc: 0.7000\n",
      "Epoch 31/35\n",
      " 89/190 [=============>................] - ETA: 10s - loss: 0.2699 - acc: 0.9118  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9166  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 18s 93ms/step - loss: 0.2535 - acc: 0.9167 - val_loss: 1.3363 - val_acc: 0.7057\n",
      "Epoch 32/35\n",
      " 85/190 [============>.................] - ETA: 11s - loss: 0.2580 - acc: 0.9141  150_20180415-131901\n",
      "189/190 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9210  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "  150_20180415-131901\n",
      "190/190 [==============================] - 19s 97ms/step - loss: 0.2402 - acc: 0.9210 - val_loss: 1.3576 - val_acc: 0.7028\n",
      "Epoch 33/35\n",
      " 53/190 [=======>......................] - ETA: 6s - loss: 0.2461 - acc: 0.9199"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hist = model.fit_generator(train_gen,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=35, #Increase this when not on Kaggle kernel\n",
    "    verbose=1,  #1 for ETA, 0 for silent\n",
    "    callbacks=callbacks,\n",
    "    max_queue_size=batch_size,\n",
    "    workers=cpu_amount,\n",
    "    use_multiprocessing=False,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=steps_per_epoch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_acc = model.evaluate_generator(val_gen, steps=steps_per_epoch_val * len(time_strs))\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_acc = run_name + '_' + str(int(final_acc*10000)).zfill(4)\n",
    "print(run_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = pd.DataFrame(hist.history)\n",
    "histories['epoch'] = hist.epoch\n",
    "print(histories.columns)\n",
    "histories_file = os.path.join(model_folder,  'hist_%s.csv' % run_name_acc)\n",
    "histories.to_csv(histories_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(hist.history['loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='r')\n",
    "plt.show()\n",
    "plt.plot(hist.history['acc'], color='b')\n",
    "plt.plot(hist.history['val_acc'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(model, run_name):\n",
    "    cwd = os.getcwd()\n",
    "    modelPath = os.path.join(cwd, 'model')\n",
    "    if not os.path.isdir(modelPath):\n",
    "        os.mkdir(modelPath)\n",
    "    weigthsFile = os.path.join(modelPath, run_name + '.h5')\n",
    "    model.save(weigthsFile)\n",
    "save_network(model, run_name_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_proba = model.predict(x_train)\n",
    "print(y_train_proba.shape)\n",
    "y_train_pred = np.argmax(y_train_proba, -1)\n",
    "print(y_train_pred.shape)\n",
    "print(accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_val_proba = model.predict(x_val)\n",
    "print(y_val_proba.shape)\n",
    "y_val_pred = np.argmax(y_val_proba, -1)\n",
    "print(y_val_pred.shape)\n",
    "print(accuracy_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_test_proba = model.predict(x_test)\n",
    "print(y_test_proba.shape)\n",
    "y_test_pred = np.argmax(y_test_proba, -1)\n",
    "print(y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里证明os.listdir()得到的图片名称list不正确\n",
    "files = os.listdir(os.path.join(test_folder, 'test'))\n",
    "print(files[:10])\n",
    "\n",
    "# 这里证明ImageDataGenerator()得到的图片名称list才是正确\n",
    "gen = ImageDataGenerator()\n",
    "image_size = (299, 299)\n",
    "test_generator  = gen.flow_from_directory(test_folder, image_size, shuffle=False, batch_size=batch_size)\n",
    "print('test_generator')\n",
    "print(len(test_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_proba(y_train_proba, y_train, y_val_proba, y_val, y_test_proba, test_filenames, file_name):\n",
    "    test_filenames = [n.encode('utf8') for n in test_filenames]\n",
    "    print(test_filenames[:10])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('y_train_proba', data=y_train_proba)\n",
    "        h.create_dataset('y_train', data=y_train)\n",
    "        h.create_dataset('y_val_proba', data=y_val_proba)\n",
    "        h.create_dataset('y_val', data=y_val)\n",
    "        h.create_dataset('y_test_proba', data=y_test_proba)\n",
    "        h.create_dataset('test_filenames', data=test_filenames)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_proba(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        y_train_proba = np.array(h['y_train_proba'])\n",
    "        y_train = np.array(h['y_train'])\n",
    "        y_val_proba = np.array(h['y_val_proba'])\n",
    "        y_val = np.array(h['y_val'])\n",
    "        y_test_proba = np.array(h['y_test_proba'])\n",
    "        test_filenames = np.array(h['test_filenames'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    test_filenames = [n.decode('utf8') for n in test_filenames]\n",
    "    print(test_filenames[:10])\n",
    "    \n",
    "    return y_train_proba, y_train, y_val_proba, y_val, y_test_proba, test_filenames\n",
    "\n",
    "\n",
    "y_proba_file = os.path.join(model_folder, 'proba_%s.p' % run_name_acc)\n",
    "save_proba(y_train_proba, y_train, y_val_proba, y_val, y_test_proba, test_generator.filenames, y_proba_file)\n",
    "y_train_proba, y_train, y_val_proba, y_val, y_test_proba, test_filenames = load_proba(y_proba_file)\n",
    "\n",
    "print(y_train_proba.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val_proba.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test_proba.shape)\n",
    "print(len(test_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_indexes = np.argmax(y_test_proba, -1)\n",
    "print(max_indexes.shape)\n",
    "\n",
    "test_dict = {}\n",
    "for pair in zip(test_filenames, max_indexes):\n",
    "    image_name, indx = pair[0], int(pair[1])\n",
    "    image_name = image_name.split('/')[-1]\n",
    "    image_id = int(image_name.split('.')[0])\n",
    "#     print(pair[0], image_name, image_id, indx, indx+1, type(image_id), type(indx))\n",
    "    test_dict[image_id] = indx + 1\n",
    "\n",
    "#确认图片的id是否能与ImageDataGenerator()对应上\n",
    "for name in test_generator.filenames[:10]:\n",
    "    image_name = name.split('/')[-1]\n",
    "    image_id = int(image_name.split('.')[0])\n",
    "#     print('%s\\t%s\\t%s' % (name, image_id, test_dict[image_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len_sample_submission_csv = len(sample_submission_csv)\n",
    "print('len(len_sample_submission_csv)=%d' % len_sample_submission_csv)\n",
    "count = 0\n",
    "for i in range(len_sample_submission_csv):\n",
    "    image_id = int(sample_submission_csv.iloc[i, 0])\n",
    "    if image_id in test_dict:\n",
    "        pred_label = test_dict[image_id]\n",
    "#         print('%s\\t%s' % (image_id, pred_label))\n",
    "        sample_submission_csv.iloc[i, 1] = pred_label\n",
    "    else:\n",
    "#         print('%s\\t%s' % (image_id, 20))\n",
    "        sample_submission_csv.iloc[i, 1] = 20 # 属于20的类最多，所以全都设置成这个类，可能会比设置成其他得到的结果好\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(int(count/1000), end=' ')\n",
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(sample_submission_csv['predicted'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = os.path.join(output_folder, 'pred_%s.csv' % run_name_acc)\n",
    "sample_submission_csv.to_csv(pred_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_name_acc)\n",
    "\n",
    "t1 = time.time()\n",
    "print('time cost: %.2f s' % (t1-t0))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
