{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FeatureExtraction_data\n",
    "\n",
    "Reference:\n",
    "- https://www.kaggle.com/asraful70/talkingdata-new-features-in-lightgbm-lb-0-9784\n",
    "- https://www.kaggle.com/danieleewww/talkingdata-added-new-features-in-lightg-50cf9b/code\n",
    "- https://www.kaggle.com/anttip/talkingdata-wordbatch-fm-ftrl-lb-0-9769\n",
    "- https://www.kaggle.com/pranav84/talkingdata-eda-to-model-evaluation-lb-0-9683\n",
    "- https://www.kaggle.com/aharless/kaggle-runnable-version-of-baris-kanber-s-lightgbm\n",
    "- https://www.kaggle.com/pranav84/lgb-entire-dataset-in-2-hrs-lb-0-9718\n",
    "- https://www.kaggle.com/panjianning/talkingdata-simple-lightgbm-0-9772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'TalkingdataAFD2018'\n",
    "step_name = 'FeatureExtraction_data'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = '%s_%s_%s' % (project_name, step_name, time_str)\n",
    "print('run_name: %s' % run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 9\n",
    "# print('date: ', date)\n",
    "\n",
    "is_debug = False\n",
    "print('is_debug: %s' % is_debug)\n",
    "\n",
    "\n",
    "if is_debug:\n",
    "    test_n_rows = 1 * 10000\n",
    "else:\n",
    "    test_n_rows = None\n",
    "#     test_n_rows = 18790469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_rows = {\n",
    "    0: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 1 * 10000\n",
    "    },\n",
    "    1: {\n",
    "        'n_skiprows': 1 * 10000,\n",
    "        'n_rows': 2 * 10000\n",
    "    },\n",
    "    6: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 9308568\n",
    "    },\n",
    "    7: {\n",
    "        'n_skiprows': 1 + 9308568,\n",
    "        'n_rows': 59633310\n",
    "    },\n",
    "    8: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310,\n",
    "        'n_rows': 62945075\n",
    "    },\n",
    "    9: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310 + 62945075,\n",
    "        'n_rows': 53016937\n",
    "    }\n",
    "}\n",
    "# n_skiprows = day_rows[date]['n_skiprows']\n",
    "# n_rows = day_rows[date]['n_rows']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import h5py\n",
    "import pickle\n",
    "import math\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_num = np.random.randint(10000)\n",
    "print('random_num: %s' % random_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t%s' % input_folder)\n",
    "print('output_folder: \\t\\t\\t%s' % output_folder)\n",
    "print('model_folder: \\t\\t\\t%s' % model_folder)\n",
    "print('feature_folder: \\t\\t%s' % feature_folder)\n",
    "print('log_folder: \\t\\t\\t%s' % log_folder)\n",
    "\n",
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "train_sample_csv_file = os.path.join(input_folder, 'train_sample.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_csv_file = os.path.join(input_folder, 'sample_submission.csv')\n",
    "\n",
    "print('\\ntrain_csv_file: \\t\\t%s' % train_csv_file)\n",
    "print('train_sample_csv_file: \\t\\t%s' % train_sample_csv_file)\n",
    "print('test_csv_file: \\t\\t\\t%s' % test_csv_file)\n",
    "print('sample_submission_csv_file: \\t%s' % sample_submission_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "    'ip'            : 'uint32',\n",
    "    'app'           : 'uint16',\n",
    "    'device'        : 'uint16',\n",
    "    'os'            : 'uint16',\n",
    "    'channel'       : 'uint16',\n",
    "    'is_attributed' : 'uint8',\n",
    "    'click_id'      : 'uint32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_csv = pd.read_csv(sample_submission_csv_file)\n",
    "print('sample_submission_csv.shape: \\t', sample_submission_csv.shape)\n",
    "display(sample_submission_csv.head(2))\n",
    "\n",
    "print('train_csv: %.2f Mb' % (sys.getsizeof(sample_submission_csv)/1024./1024.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_click_time(df):\n",
    "    df['day'] = df['click_time'].dt.day.astype('uint8')\n",
    "    df['hour'] = df['click_time'].dt.hour.astype('uint8')\n",
    "    df['minute'] = df['click_time'].dt.minute.astype('uint8')\n",
    "    df['second'] = df['click_time'].dt.second.astype('uint8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prev_click(df, group_cols, agg_type='float32'):\n",
    "    agg_suffix = 'prevClick'\n",
    "    new_feature = new_feature = '{}_{}'.format('_'.join(group_cols), agg_suffix)\n",
    "    all_features = group_cols + ['click_time']\n",
    "    df[new_feature] = (df.click_time - df[all_features].groupby(group_cols).click_time.shift(+1) ).dt.seconds.astype(agg_type)\n",
    "    return df\n",
    "    \n",
    "def do_next_click(df, group_cols, agg_type='float32'):\n",
    "    agg_suffix = 'nextClick'\n",
    "    new_feature = new_feature = '{}_{}'.format('_'.join(group_cols), agg_suffix)\n",
    "    all_features = group_cols + ['click_time']\n",
    "    df[new_feature] = (df[all_features].groupby(group_cols).click_time.shift(-1) - df.click_time).dt.seconds.astype(agg_type)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below a function is written to extract count feature by aggregating different cols\n",
    "def do_count( df, group_cols, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name='{}_count'.format('_'.join(group_cols))\n",
    "    if show_agg:\n",
    "        print( \"Aggregating by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "    \n",
    "##  Below a function is written to extract unique count feature from different cols\n",
    "def do_countuniq( df, group_cols, counted, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_countuniq'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"Counting unqiue \", counted, \" by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "### Below a function is written to extract cumulative count feature  from different cols    \n",
    "def do_cumcount( df, group_cols, counted,agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_cumcount'.format(('_'.join(group_cols)),(counted)) \n",
    "    if show_agg:\n",
    "        print( \"Cumulative count by \", group_cols , '... and saved in', agg_name  )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n",
    "    df[agg_name]=gp.values\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "### Below a function is written to extract mean feature  from different cols\n",
    "def do_mean( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_mean'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"Calculating mean of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "def do_var( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_var'.format(('_'.join(group_cols)),(counted)) \n",
    "    if show_agg:\n",
    "        print( \"Calculating variance of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature(x_data, y_data, file_name):\n",
    "    print(y_data[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('x_data', data=x_data)\n",
    "        h.create_dataset('y_data', data=y_data)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_feature(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        x_data = np.array(h['x_data'])\n",
    "        y_data = np.array(h['y_data'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    print(y_data[:5])\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def save_test_feature(x_test, click_ids, file_name):\n",
    "    print(click_ids[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('x_test', data=x_test)\n",
    "        h.create_dataset('click_ids', data=click_ids)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_test_feature(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        x_test = np.array(h['x_test'])\n",
    "        click_ids = np.array(h['click_ids'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    print(click_ids[:5])\n",
    "    \n",
    "    return x_test, click_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_map(feature_map, file_name):\n",
    "    print(feature_map[:5])\n",
    "    feature_map_encode = []\n",
    "    for item in feature_map:\n",
    "        feature_name_encode = item[1].encode('UTF-8')\n",
    "        feature_map_encode.append((item[0], feature_name_encode))\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('feature_map', data=feature_map_encode)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_feature_map(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        feature_map_encode = np.array(h['feature_map'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    feature_map = []\n",
    "    for item in feature_map_encode:\n",
    "        feature_name = item[1].decode('UTF-8')\n",
    "        feature_map.append((int(item[0]), feature_name))\n",
    "    print(feature_map[:5])\n",
    "    \n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feature(train_csv):\n",
    "    train_csv = do_click_time(train_csv)\n",
    "    \n",
    "    for cols in do_prev_click_cols:\n",
    "        print('>> ', cols)\n",
    "        train_csv = do_prev_click( train_csv, cols ); gc.collect()\n",
    "    \n",
    "    for cols in do_next_click_cols:\n",
    "        print('>> ', cols)\n",
    "        train_csv = do_next_click( train_csv, cols ); gc.collect()\n",
    "    \n",
    "    for cols in do_count_cols:\n",
    "        print('>> ', cols)\n",
    "        train_csv = do_count( train_csv, cols ); gc.collect()\n",
    "        \n",
    "    for cols in do_countuniq_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_countuniq( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "        \n",
    "    for cols in do_cumcount_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_cumcount( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "        \n",
    "    for cols in do_mean_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_mean( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "        \n",
    "    for cols in do_var_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_var( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "    \n",
    "    train_csv.drop(['click_time'], axis=1, inplace=True)\n",
    "    print(train_csv.shape)\n",
    "\n",
    "    display(train_csv.head())\n",
    "\n",
    "    print(train_csv.columns)\n",
    "    print('data_size: %.2f Mb' % (sys.getsizeof(train_csv)/1024./1024.))\n",
    "    return train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = [\n",
    "    # 5 choice 2\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'device'],\n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'channel'],\n",
    "    ['app', 'device'],\n",
    "    ['app', 'os'],\n",
    "    ['app', 'channel'],\n",
    "    ['device', 'os'],\n",
    "    ['device', 'channel'],\n",
    "    ['os', 'channel'],\n",
    "    # 5 choice 3\n",
    "    ['device', 'os', 'channel'],\n",
    "    ['app', 'os', 'channel'],\n",
    "    ['app', 'device', 'channel'],\n",
    "    ['app', 'device', 'os'],\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'device', 'os'],\n",
    "    ['ip', 'app', 'channel'],\n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'app', 'device'],\n",
    "]\n",
    "\n",
    "template_hour = [\n",
    "    # 5 choice 2\n",
    "    ['ip', 'app', 'hour'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['ip', 'os', 'hour'],\n",
    "    ['ip', 'channel', 'hour'],\n",
    "    ['app', 'device', 'hour'],\n",
    "    ['app', 'os', 'hour'],\n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['device', 'os', 'hour'],\n",
    "    ['device', 'channel', 'hour'],\n",
    "    ['os', 'channel', 'hour'],\n",
    "    # 5 choice 3\n",
    "    ['device', 'os', 'channel', 'hour'],\n",
    "    ['app', 'os', 'channel', 'hour'],\n",
    "    ['app', 'device', 'channel', 'hour'],\n",
    "    ['app', 'device', 'os', 'hour'],\n",
    "    ['ip', 'os', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    ['ip', 'app', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'os', 'hour'],\n",
    "    ['ip', 'app', 'device', 'hour'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_prev_click_cols = [\n",
    "    ['ip', 'device'],\n",
    "    ['ip', 'app', 'device'],\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'device', 'os'],\n",
    "    ['ip', 'app', 'device', 'os', 'channel'],\n",
    "    ['ip', 'app', 'os', 'channel'],\n",
    "    ['ip', 'device', 'os', 'channel'],\n",
    "    \n",
    "    \n",
    "    \n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'channel'], # ref\n",
    "]\n",
    "\n",
    "do_next_click_cols = [\n",
    "    ['ip', 'device'],\n",
    "    ['ip', 'app', 'device'],\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'device', 'os'], # ref\n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'device', 'os'], # ref\n",
    "    ['ip', 'device', 'os', 'channel'],\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'app', 'os', 'channel'],\n",
    "    \n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    \n",
    "    ['ip', 'app', 'device', 'os', 'channel'], # ref\n",
    "    ['device', 'channel'], # ref\n",
    "    ['app', 'device', 'channel'], # ref\n",
    "    ['device', 'hour'], # ref\n",
    "    \n",
    "#     ['ip', 'device'],\n",
    "#     ['ip', 'app', 'device', 'channel'],\n",
    "#     ['ip', 'os'],\n",
    "#     ['ip', 'app', 'channel'],\n",
    "#     ['ip' ,'channel'],\n",
    "]\n",
    "\n",
    "do_count_cols = [\n",
    "    ['ip', 'device'],\n",
    "    ['app', 'channel'],\n",
    "    ['device', 'os', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['app', 'device', 'os'],\n",
    "    ['app', 'os', 'channel', 'hour'],\n",
    "    ['app', 'os'],\n",
    "    \n",
    "    \n",
    "    ['app', 'hour'],\n",
    "    ['ip', 'day', 'hour'], # ref\n",
    "    ['ip', 'app'], # ref\n",
    "    ['ip', 'app', 'os'], # ref\n",
    "]\n",
    "do_countuniq_cols = [\n",
    "    ['ip', 'app'], # ref\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'device', 'os'], # ref\n",
    "    \n",
    "    \n",
    "    ['ip', 'channel'], # ref\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    \n",
    "    ['ip', 'day', 'hour'], # ref\n",
    "    ['ip', 'app', 'os'], # ref\n",
    "    ['ip', 'device'],\n",
    "    ['app', 'channel'],\n",
    "]\n",
    "do_cumcount_cols = [\n",
    "    ['app', 'os', 'hour'],\n",
    "    ['app', 'device', 'channel'],\n",
    "    ['app', 'device'],\n",
    "    ['app', 'device', 'os'],\n",
    "    ['device', 'os'],\n",
    "    \n",
    "    \n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['os', 'channel'],\n",
    "    ['device', 'channel', 'hour'],\n",
    "    ['device', 'os', 'channel'],\n",
    "    ['os', 'channel', 'hour'],\n",
    "    ['device', 'os', 'hour'],\n",
    "    ['app', 'device', 'channel', 'hour'],\n",
    "    ['app', 'os', 'channel'],\n",
    "    \n",
    "    ['ip', 'os'], # ref\n",
    "    ['ip', 'device', 'os'], # ref\n",
    "]\n",
    "do_mean_cols = [\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'channel'],\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'device', 'os'],\n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['ip', 'channel'],\n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['os', 'channel', 'hour'],\n",
    "    ['app', 'os', 'channel'],\n",
    "    ['device', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'hour'],\n",
    "    ['ip', 'os', 'hour'],\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    ['ip', 'os', 'channel', 'hour'],\n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['app', 'device', 'os', 'hour'],\n",
    "    \n",
    "\n",
    "    \n",
    "#     ['ip', 'os', 'channel'],\n",
    "#     ['ip', 'app', 'os'],\n",
    "#     ['ip', 'device', 'channel']\n",
    "]\n",
    "do_var_cols = [\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'channel'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'device', 'os'],\n",
    "    ['ip', 'channel'],\n",
    "    ['ip', 'os'],\n",
    "    ['app', 'os', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    ['device', 'os', 'channel', 'hour'],\n",
    "    ['os', 'channel'],\n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'device'],\n",
    "    ['app', 'os', 'hour'],\n",
    "    \n",
    "    ['ip', 'app', 'hour'],\n",
    "    ['ip', 'app', 'device', 'hour'],\n",
    "#     ['ip', 'os', 'hour'],\n",
    "    ['app', 'os', 'channel'],\n",
    "#     ['ip', 'channel']\n",
    "]\n",
    "\n",
    "for cols in do_count_cols:\n",
    "    print(cols[:-1], cols[-1])\n",
    "\n",
    "feature_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# test_csv = pd.read_csv(\n",
    "#     test_csv_file, \n",
    "#     nrows=test_n_rows, \n",
    "#     usecols=test_columns,\n",
    "#     dtype=dtypes,\n",
    "#     parse_dates=['click_time']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# print('test_csv.shape: \\t\\t', test_csv.shape)\n",
    "# display(test_csv.head(2))\n",
    "# print('test_csv:  %.2f Mb' % (sys.getsizeof(test_csv)/1024./1024.))\n",
    "# # print('*' * 80)\n",
    "\n",
    "# click_ids = test_csv['click_id']\n",
    "# test_csv.drop(['click_id'], axis=1, inplace=True)\n",
    "# display(click_ids.head())\n",
    "\n",
    "# display(test_csv.head())\n",
    "# # print('*' * 80)\n",
    "\n",
    "# test_csv = do_feature(test_csv)\n",
    "    \n",
    "# y_proba_file = os.path.join(feature_folder, 'feature_%s_test.p' % run_name)\n",
    "# feature_files.append(y_proba_file)\n",
    "# save_test_feature(\n",
    "#     test_csv, \n",
    "#     click_ids, \n",
    "#     y_proba_file\n",
    "# )\n",
    "# x_test, click_ids = load_test_feature(y_proba_file)\n",
    "\n",
    "# print(x_test.shape)\n",
    "# print(len(click_ids))\n",
    "\n",
    "# feature_map = []\n",
    "# print('[')\n",
    "# for i, col in enumerate(test_csv.columns):\n",
    "#     feature_map.append((i, col))\n",
    "#     print('  (%s,\\t\"%s\")' % (i, col))\n",
    "# print(']')\n",
    "# feature_map_file_name = y_proba_file = os.path.join(feature_folder, 'feature_map_%s.p' % run_name)\n",
    "# save_feature_map(feature_map, feature_map_file_name)\n",
    "# feature_map1 = load_feature_map(feature_map_file_name)\n",
    "# print(len(feature_map1))\n",
    "# print(feature_map1[:5])\n",
    "\n",
    "# # del test_csv\n",
    "# # del x_test\n",
    "# # del click_ids\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in day_rows.keys():\n",
    "    key_str = str(key)\n",
    "    print('date key: %s' % key_str)\n",
    "    if is_debug and key > 1:\n",
    "        print('is_debug=%s, skip date: %s' % (is_debug, key_str))\n",
    "        continue\n",
    "    if not is_debug and key <= 1:\n",
    "        print('is_debug=%s, skip date: %s' % (is_debug, key_str))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# for key in day_rows.keys():\n",
    "print('*' * 80)\n",
    "key = date\n",
    "key_str = str(key)\n",
    "print('date key: %s' % key_str)\n",
    "\n",
    "n_skiprows = day_rows[key]['n_skiprows']\n",
    "n_rows = day_rows[key]['n_rows']\n",
    "\n",
    "train_csv = pd.read_csv(\n",
    "    train_csv_file, \n",
    "    skiprows=range(1, n_skiprows), \n",
    "    nrows=n_rows, \n",
    "    usecols=train_columns,\n",
    "    dtype=dtypes,\n",
    "    parse_dates=['click_time']\n",
    ")\n",
    "\n",
    "print('train_csv.shape: \\t\\t', train_csv.shape)\n",
    "display(train_csv.head(2))\n",
    "print('train_csv: %.2f Mb' % (sys.getsizeof(train_csv)/1024./1024.))\n",
    "#     print('*' * 80)\n",
    "\n",
    "y_data = train_csv['is_attributed']\n",
    "train_csv.drop(['is_attributed'], axis=1, inplace=True)\n",
    "display(y_data.head())\n",
    "\n",
    "display(train_csv.head())\n",
    "#     print('*' * 80)\n",
    "\n",
    "train_csv = do_feature(train_csv)\n",
    "\n",
    "y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s.p' % (run_name, key_str))\n",
    "feature_files.append(y_proba_file)\n",
    "save_feature(\n",
    "    train_csv, \n",
    "    y_data, \n",
    "    y_proba_file\n",
    ")\n",
    "x_data, y_data = load_feature(y_proba_file)\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "print('[')\n",
    "for i, col in enumerate(train_csv.columns):\n",
    "    print('  (%s,\\t\"%s\")' % (i, col))\n",
    "print('')\n",
    "#     del train_csv\n",
    "del x_data\n",
    "del y_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_data.shape)\n",
    "# print(y_data.shape)\n",
    "# print(x_test.shape)\n",
    "# print(click_ids.shape)\n",
    "for name in feature_files:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time cost: %.2f s' % (time.time() - t0))\n",
    "print(run_name)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
