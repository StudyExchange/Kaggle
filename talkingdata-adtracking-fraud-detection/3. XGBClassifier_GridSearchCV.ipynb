{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. XGBClassifier_GridSearchCV\n",
    "**Start from the most basic features, and try to improve step by step.**\n",
    "\n",
    "Kaggle score: \n",
    "\n",
    "Reference:\n",
    "- XGBoost Parameters, http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters\n",
    "- Python API Reference, http://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "- sklearn.model_selection.GridSearchCV, http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
    "- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "- https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'TalkingdataAFD2018'\n",
    "step_name = 'XGBClassifier_GridSearchCV'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = '%s_%s_%s' % (project_name, step_name, time_str)\n",
    "print('run_name: %s' % run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 7\n",
    "print('date: ', date)\n",
    "\n",
    "test_n_rows = None\n",
    "# test_n_rows = 18790469\n",
    "# test_n_rows = 10*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_rows = {\n",
    "    0: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 10 * 10000\n",
    "    },\n",
    "    6: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 9308568\n",
    "    },\n",
    "    7: {\n",
    "        'n_skiprows': 1 + 9308568,\n",
    "        'n_rows': 59633310\n",
    "    },\n",
    "    8: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310,\n",
    "        'n_rows': 62945075\n",
    "    },\n",
    "    9: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310 + 62945075,\n",
    "        'n_rows': 53016937\n",
    "    }\n",
    "}\n",
    "n_skiprows = day_rows[date]['n_skiprows']\n",
    "n_rows = day_rows[date]['n_rows']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import h5py\n",
    "import pickle\n",
    "import math\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_num = np.random.randint(10000)\n",
    "print('random_num: %s' % random_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t%s' % input_folder)\n",
    "print('output_folder: \\t\\t\\t%s' % output_folder)\n",
    "print('model_folder: \\t\\t\\t%s' % model_folder)\n",
    "print('log_folder: \\t\\t\\t%s' % log_folder)\n",
    "\n",
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "train_sample_csv_file = os.path.join(input_folder, 'train_sample.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_csv_file = os.path.join(input_folder, 'sample_submission.csv')\n",
    "\n",
    "print('\\ntrain_csv_file: \\t\\t%s' % train_csv_file)\n",
    "print('train_sample_csv_file: \\t\\t%s' % train_sample_csv_file)\n",
    "print('test_csv_file: \\t\\t\\t%s' % test_csv_file)\n",
    "print('sample_submission_csv_file: \\t%s' % sample_submission_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "    'ip'            : 'uint32',\n",
    "    'app'           : 'uint16',\n",
    "    'device'        : 'uint16',\n",
    "    'os'            : 'uint16',\n",
    "    'channel'       : 'uint16',\n",
    "    'is_attributed' : 'uint8',\n",
    "    'click_id'      : 'uint32'\n",
    "}\n",
    "\n",
    "train_csv = pd.read_csv(\n",
    "    train_csv_file, \n",
    "    skiprows=range(1, n_skiprows), \n",
    "    nrows=n_rows, \n",
    "    usecols=train_columns,\n",
    "    dtype=dtypes,\n",
    "    parse_dates=['click_time']\n",
    ")\n",
    "test_csv = pd.read_csv(\n",
    "    test_csv_file, \n",
    "    nrows=test_n_rows, \n",
    "    usecols=test_columns,\n",
    "    dtype=dtypes,\n",
    "    parse_dates=['click_time']\n",
    ")\n",
    "sample_submission_csv = pd.read_csv(sample_submission_csv_file)\n",
    "\n",
    "print('train_csv.shape: \\t\\t', train_csv.shape)\n",
    "print('test_csv.shape: \\t\\t', test_csv.shape)\n",
    "print('sample_submission_csv.shape: \\t', sample_submission_csv.shape)\n",
    "print('train_csv.dtypes: \\n', train_csv.dtypes)\n",
    "\n",
    "display(train_csv.head(2))\n",
    "display(test_csv.head(2))\n",
    "display(sample_submission_csv.head(2))\n",
    "\n",
    "print('train_csv: %.2f Mb' % (sys.getsizeof(train_csv)/1024./1024.))\n",
    "print('test_csv:  %.2f Mb' % (sys.getsizeof(test_csv)/1024./1024.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = train_csv['is_attributed']\n",
    "train_csv.drop(['is_attributed'], axis=1, inplace=True)\n",
    "display(y_data.head())\n",
    "\n",
    "click_ids = test_csv['click_id']\n",
    "test_csv.drop(['click_id'], axis=1, inplace=True)\n",
    "display(click_ids.head())\n",
    "\n",
    "\n",
    "display(train_csv.head())\n",
    "display(test_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv['day'] = train_csv['click_time'].dt.day.astype('uint8')\n",
    "train_csv['hour'] = train_csv['click_time'].dt.hour.astype('uint8')\n",
    "train_csv['minute'] = train_csv['click_time'].dt.minute.astype('uint8')\n",
    "train_csv['second'] = train_csv['click_time'].dt.second.astype('uint8')\n",
    "print('train_csv.shape: \\t', train_csv.shape)\n",
    "display(train_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv['day'] = test_csv['click_time'].dt.day.astype('uint8')\n",
    "test_csv['hour'] = test_csv['click_time'].dt.hour.astype('uint8')\n",
    "test_csv['minute'] = test_csv['click_time'].dt.minute.astype('uint8')\n",
    "test_csv['second'] = test_csv['click_time'].dt.second.astype('uint8')\n",
    "print('test_csv.shape: \\t', test_csv.shape)\n",
    "display(test_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[3,6,6],[4,5,1]])\n",
    "print(arr)\n",
    "np.ravel_multi_index(arr, (7,6))\n",
    "print(arr)\n",
    "print(np.ravel_multi_index(arr, (7,6), order='F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_add_counts(df, cols, tag=\"_count\"):\n",
    "    arr_slice = df[cols].values\n",
    "    unq, unqtags, counts = np.unique(np.ravel_multi_index(arr_slice.T, arr_slice.max(0) + 1), return_inverse=True, return_counts=True)\n",
    "    df[\"_\".join(cols) + tag] = counts[unqtags]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_add_uniques(df, cols, tag=\"_unique\"):\n",
    "    gp = df[cols] \\\n",
    "        .groupby(by=cols[0:len(cols) - 1])[cols[len(cols) - 1]] \\\n",
    "        .nunique() \\\n",
    "        .reset_index() \\\n",
    "        .rename(index=str, columns={cols[len(cols) - 1]: \"_\".join(cols)+tag})\n",
    "    df = df.merge(gp, on=cols[0:len(cols) - 1], how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_csv = df_add_counts(train_csv, ['ip', 'day', 'hour'])\n",
    "# train_csv = df_add_counts(train_csv, ['ip', 'app'])\n",
    "# train_csv = df_add_counts(train_csv, ['ip', 'app', 'os'])\n",
    "# train_csv = df_add_counts(train_csv, ['ip', 'device'])\n",
    "# train_csv = df_add_counts(train_csv, ['app', 'channel'])\n",
    "# train_csv = df_add_uniques(train_csv, ['ip', 'channel'])\n",
    "\n",
    "# display(train_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_csv = df_add_counts(test_csv, ['ip', 'day', 'hour'])\n",
    "# test_csv = df_add_counts(test_csv, ['ip', 'app'])\n",
    "# test_csv = df_add_counts(test_csv, ['ip', 'app', 'os'])\n",
    "# test_csv = df_add_counts(test_csv, ['ip', 'device'])\n",
    "# test_csv = df_add_counts(test_csv, ['app', 'channel'])\n",
    "# test_csv = df_add_uniques(test_csv, ['ip', 'channel'])\n",
    "\n",
    "# display(test_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_next_prev_Click( df,agg_suffix, agg_type='float32'):\n",
    "    print('Extracting new features...')\n",
    "    df['hour'] = pd.to_datetime(df.click_time).dt.hour.astype('int8')\n",
    "    df['day'] = pd.to_datetime(df.click_time).dt.day.astype('int8')\n",
    "    \n",
    "    #### New added\n",
    "    df['minute'] = pd.to_datetime(df.click_time).dt.minute.astype('int8')\n",
    "    predictors.append('minute')\n",
    "    df['second'] = pd.to_datetime(df.click_time).dt.second.astype('int8')\n",
    "    predictors.append('second')\n",
    "    print(f\">> \\nExtracting {agg_suffix} time calculation features...\\n\")\n",
    "    \n",
    "    GROUP_BY_NEXT_CLICKS = [\n",
    "    \n",
    "    # V1\n",
    "    # {'groupby': ['ip']},\n",
    "    # {'groupby': ['ip', 'app']},\n",
    "    # {'groupby': ['ip', 'channel']},\n",
    "    # {'groupby': ['ip', 'os']},\n",
    "    \n",
    "    # V3\n",
    "    {'groupby': ['ip', 'app', 'device', 'os', 'channel']},\n",
    "    {'groupby': ['ip', 'os', 'device']},\n",
    "    {'groupby': ['ip', 'os', 'device', 'app']}\n",
    "    ]\n",
    "\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in GROUP_BY_NEXT_CLICKS:\n",
    "    \n",
    "       # Name of new feature\n",
    "        new_feature = '{}_{}'.format('_'.join(spec['groupby']),agg_suffix)    \n",
    "    \n",
    "        # Unique list of features to select\n",
    "        all_features = spec['groupby'] + ['click_time']\n",
    "\n",
    "        # Run calculation\n",
    "        print(f\">> Grouping by {spec['groupby']}, and saving time to {agg_suffix} in: {new_feature}\")\n",
    "        if agg_suffix==\"nextClick\":\n",
    "            df[new_feature] = (df[all_features].groupby(spec[\n",
    "            'groupby']).click_time.shift(-1) - df.click_time).dt.seconds.astype(agg_type)\n",
    "        elif agg_suffix== \"prevClick\":\n",
    "            df[new_feature] = (df.click_time - df[all_features].groupby(spec[\n",
    "                'groupby']).click_time.shift(+1) ).dt.seconds.astype(agg_type)\n",
    "        predictors.append(new_feature)\n",
    "        gc.collect()\n",
    "#         print('predictors',predictors)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below a function is written to extract count feature by aggregating different cols\n",
    "def do_count( df, group_cols, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name='{}count'.format('_'.join(group_cols))  \n",
    "    if show_agg:\n",
    "        print( \"\\nAggregating by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "    \n",
    "##  Below a function is written to extract unique count feature from different cols\n",
    "def do_countuniq( df, group_cols, counted, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_countuniq'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"\\nCounting unqiue \", counted, \" by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "### Below a function is written to extract cumulative count feature  from different cols    \n",
    "def do_cumcount( df, group_cols, counted,agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_cumcount'.format(('_'.join(group_cols)),(counted)) \n",
    "    if show_agg:\n",
    "        print( \"\\nCumulative count by \", group_cols , '... and saved in', agg_name  )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n",
    "    df[agg_name]=gp.values\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "### Below a function is written to extract mean feature  from different cols\n",
    "def do_mean( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_mean'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"\\nCalculating mean of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "def do_var( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_var'.format(('_'.join(group_cols)),(counted)) \n",
    "    if show_agg:\n",
    "        print( \"\\nCalculating variance of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(train_csv)\n",
    "train_df=train_csv.append(test_csv)\n",
    "predictors=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "train_df = do_next_prev_Click( train_df,agg_suffix='nextClick', agg_type='float32'  ); gc.collect()\n",
    "train_df = do_next_prev_Click( train_df,agg_suffix='prevClick', agg_type='float32'  ); gc.collect()  ## Removed temporarily due RAM sortage. \n",
    "train_df = do_countuniq( train_df, ['ip'], 'channel' ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip', 'device', 'os'], 'app'); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip', 'day'], 'hour' ); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip'], 'app'); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip', 'app'], 'os'); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['ip'], 'device'); gc.collect()\n",
    "train_df = do_countuniq( train_df, ['app'], 'channel'); gc.collect()\n",
    "train_df = do_cumcount( train_df, ['ip'], 'os'); gc.collect()\n",
    "train_df = do_cumcount( train_df, ['ip', 'device', 'os'], 'app'); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'day', 'hour'] ); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'app']); gc.collect()\n",
    "train_df = do_count( train_df, ['ip', 'app', 'os']); gc.collect()\n",
    "# train_df = do_var( train_df, ['ip', 'day', 'channel'], 'hour'); gc.collect()\n",
    "train_df = do_var( train_df, ['ip', 'app', 'os'], 'hour'); gc.collect()\n",
    "# train_df = do_var( train_df, ['ip', 'app', 'channel'], 'day'); gc.collect()\n",
    "# train_df = do_mean( train_df, ['ip', 'app', 'channel'], 'hour' ); gc.collect()\n",
    "\n",
    "print(train_df.head(5))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = train_df[:len_train]\n",
    "test_csv = train_df[len_train:]\n",
    "\n",
    "del train_df; gc.collect()\n",
    "\n",
    "print(train_csv.shape)\n",
    "print(test_csv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_csv.columns)\n",
    "print(test_csv.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_useless_features = ['click_time']\n",
    "train_csv.drop(train_useless_features, axis=1, inplace=True)\n",
    "\n",
    "test_useless_features = ['click_time']\n",
    "test_csv.drop(test_useless_features, axis=1, inplace=True)\n",
    "\n",
    "display(train_csv.head())\n",
    "display(test_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_csv, y_data, test_size=0.05, random_state=2017)\n",
    "x_test = test_csv\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print('Time cost: %.2f s' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.columns)\n",
    "print(x_val.columns)\n",
    "print(x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_val))\n",
    "print(type(y_val))\n",
    "print(type(x_test))\n",
    "\n",
    "x_train = x_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "x_val = x_val.as_matrix()\n",
    "y_val = y_val.as_matrix()\n",
    "x_test = x_test.as_matrix()\n",
    "\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_val))\n",
    "print(type(y_val))\n",
    "print(type(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=20, \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=5000, \n",
    "    silent=False, \n",
    "    objective='gpu:binary:logistic', \n",
    "    booster='gbtree', \n",
    "#     n_jobs=1, \n",
    "    nthread=None, \n",
    "    gamma=0, \n",
    "    min_child_weight=1, \n",
    "    max_delta_step=0, \n",
    "    subsample=0.5, \n",
    "    colsample_bytree=0.7, \n",
    "    colsample_bylevel=0.7, \n",
    "    reg_alpha=0, \n",
    "    reg_lambda=1, \n",
    "    scale_pos_weight=97, \n",
    "    base_score=0.5, \n",
    "    random_state=0, \n",
    "    seed=None, \n",
    "    missing=None,\n",
    "    # booster params\n",
    "    num_boost_round=50,\n",
    "    early_stopping_rounds=30,\n",
    "    tree_method='gpu_hist',\n",
    "    predictor='gpu_predictor',\n",
    "    eval_metric=['auc'],\n",
    "\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'reg_alpha':[0.3], \n",
    "    'reg_lambda':[0.8],\n",
    "    'scale_pos_weight': [1, 98, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, verbose=2, cv=3)\n",
    "grid_search.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*' * 80)\n",
    "y_train_proba = grid_search.predict_proba(x_train)\n",
    "print(y_train_proba.shape)\n",
    "print(y_train_proba[:10])\n",
    "y_train_pred = (y_train_proba[:, 1]>=0.5).astype(int)\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "roc_train = roc_auc_score(y_train, y_train_proba[:, 1])\n",
    "print('acc_train: %.4f \\t roc_train: %.4f' % (acc_train, roc_train))\n",
    "\n",
    "# y_train_pred = grid_search.predict(x_train)\n",
    "# acc_train = accuracy_score(y_train, y_train_pred)\n",
    "# roc_train = roc_auc_score(y_train, y_train_proba[:, 1])\n",
    "# print('acc_train: %.4f \\t roc_train: %.4f' % (acc_train, roc_train))\n",
    "\n",
    "y_val_proba = grid_search.predict_proba(x_val)\n",
    "print(y_val_proba.shape)\n",
    "print(y_val_proba[:10])\n",
    "y_val_pred = (y_val_proba[:, 0]>=0.5).astype(int)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "roc_val = roc_auc_score(y_val, y_val_proba[:, 1])\n",
    "print('acc_val:   %.4f \\t roc_val:   %.4f' % (acc_val, roc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train; gc.collect()\n",
    "del x_val; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_acc = run_name + '_' + str(int(roc_val*10000)).zfill(4)\n",
    "print(run_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "kf = KFold(105, n_folds=10)\n",
    "for train_index, test_index in kf:\n",
    "    print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(len(x_test), n_folds=10)\n",
    "y_test_proba = []\n",
    "for train_index, test_index in kf:\n",
    "    y_test_proba_fold = grid_search.predict_proba(x_test[test_index])\n",
    "    y_test_proba.append(y_test_proba_fold)\n",
    "    print(y_test_proba_fold.shape)\n",
    "    \n",
    "y_test_proba = np.concatenate(y_test_proba, axis=0)\n",
    "\n",
    "print(y_test_proba.shape)\n",
    "print(y_test_proba[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_proba(y_train_proba, y_train, y_val_proba, y_val, y_test_proba, click_ids, file_name):\n",
    "    print(click_ids[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('y_train_proba', data=y_train_proba)\n",
    "        h.create_dataset('y_train', data=y_train)\n",
    "        h.create_dataset('y_val_proba', data=y_val_proba)\n",
    "        h.create_dataset('y_val', data=y_val)\n",
    "        h.create_dataset('y_test_proba', data=y_test_proba)\n",
    "        h.create_dataset('click_ids', data=click_ids)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_proba(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        y_train_proba = np.array(h['y_train_proba'])\n",
    "        y_train = np.array(h['y_train'])\n",
    "        y_val_proba = np.array(h['y_val_proba'])\n",
    "        y_val = np.array(h['y_val'])\n",
    "        y_test_proba = np.array(h['y_test_proba'])\n",
    "        click_ids = np.array(h['click_ids'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    print(click_ids[:5])\n",
    "    \n",
    "    return y_train_proba, y_train, y_val_proba, y_val, y_test_proba, click_ids\n",
    "\n",
    "\n",
    "y_proba_file = os.path.join(model_folder, 'proba_%s.p' % run_name_acc)\n",
    "save_proba(\n",
    "    y_train_proba[:, 1], \n",
    "    y_train, \n",
    "    y_val_proba[:, 1], \n",
    "    y_val, \n",
    "    y_test_proba[:, 1], \n",
    "    np.array(sample_submission_csv['click_id']), \n",
    "    y_proba_file\n",
    ")\n",
    "y_train_proba_true, y_train, y_val_proba_true, y_val, y_test_proba_true, click_ids = load_proba(y_proba_file)\n",
    "\n",
    "print(y_train_proba_true.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val_proba_true.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test_proba_true.shape)\n",
    "print(len(click_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "submission_csv_file = os.path.join(output_folder, 'pred_%s.csv' % run_name_acc)\n",
    "print(submission_csv_file)\n",
    "submission_csv = pd.DataFrame({ 'click_id': click_ids , 'is_attributed': y_test_proba_true })\n",
    "submission_csv.to_csv(submission_csv_file, index = False)\n",
    "display(submission_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time cost: %.2f s' % (time.time() - t0))\n",
    "\n",
    "print('random_num: ', random_num)\n",
    "print('date: ', date)\n",
    "print(run_name_acc)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
