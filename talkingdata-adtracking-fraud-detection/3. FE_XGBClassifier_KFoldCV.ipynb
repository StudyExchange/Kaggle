{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FE_XGBClassifier_KFoldCV\n",
    "Kaggle score:\n",
    "\n",
    "pip install jupyter notebook tqdm pillow h5py seaborn scikit-learn scikit-image xgboost\n",
    "\n",
    "Abstract:\n",
    "- date 7, 8, 9少feature的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'TalkingdataAFD2018'\n",
    "step_name = 'FE_XGBClassifier_KFoldCV'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = '%s_%s_%s' % (project_name, step_name, time_str)\n",
    "print('run_name: %s' % run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_run_name = 'TalkingdataAFD2018_FeatureExtraction_20180501_185800'\n",
    "date = 100\n",
    "print('date: ', date)\n",
    "\n",
    "\n",
    "is_debug = False\n",
    "print('is_debug: %s' % is_debug)\n",
    "\n",
    "# epoch = 3\n",
    "# batch_size = 2000 * 10000\n",
    "# skip_data_len = (epoch - 1) * batch_size\n",
    "# data_len = batch_size\n",
    "# print('Echo: %s, Data rows: [%s, %s]' % (epoch, skip_data_len, skip_data_len + data_len))\n",
    "\n",
    "# epoch = 2\n",
    "# batch_size = 4000 * 10000\n",
    "# skip_data_len = 59633310 - batch_size\n",
    "# data_len = batch_size\n",
    "# print('batch_size: %s' % batch_size)\n",
    "# print('epoch: %s, data rows: [%s, %s]' % (epoch, skip_data_len, skip_data_len + data_len))\n",
    "\n",
    "# run_name = '%s_date%s%s' % (run_name, date, epoch)\n",
    "run_name = '%s_date%s' % (run_name, date)\n",
    "\n",
    "print(run_name)\n",
    "\n",
    "if is_debug:\n",
    "    test_n_rows = 1 * 10000\n",
    "else:\n",
    "    test_n_rows = None\n",
    "#     test_n_rows = 18790469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_rows = {\n",
    "    0: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 1 * 1000\n",
    "    },\n",
    "    1: {\n",
    "        'n_skiprows': 1 * 1000,\n",
    "        'n_rows': 2 * 1000\n",
    "    },\n",
    "    6: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 2 * 1000 # 9308568\n",
    "    },\n",
    "    7: {\n",
    "        'n_skiprows': 1 + 9308568,\n",
    "        'n_rows': 2 * 1000 # 59633310\n",
    "    },\n",
    "    8: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310,\n",
    "        'n_rows': 2 * 1000 # 62945075\n",
    "    },\n",
    "    9: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310 + 62945075,\n",
    "        'n_rows': 2 * 1000 # 53016937\n",
    "    }\n",
    "}\n",
    "# n_skiprows = day_rows[date]['n_skiprows']\n",
    "# n_rows = day_rows[date]['n_rows']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import h5py\n",
    "import pickle\n",
    "import math\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_num = np.random.randint(10000)\n",
    "cpu_amount = cpu_count()\n",
    "\n",
    "print('cpu_amount: %s' % (cpu_amount - 2))\n",
    "print('random_num: %s' % random_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t%s' % input_folder)\n",
    "print('output_folder: \\t\\t\\t%s' % output_folder)\n",
    "print('model_folder: \\t\\t\\t%s' % model_folder)\n",
    "print('feature_folder: \\t\\t%s' % feature_folder)\n",
    "print('log_folder: \\t\\t\\t%s' % log_folder)\n",
    "\n",
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "train_sample_csv_file = os.path.join(input_folder, 'train_sample.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_csv_file = os.path.join(input_folder, 'sample_submission.csv')\n",
    "\n",
    "print('\\ntrain_csv_file: \\t\\t%s' % train_csv_file)\n",
    "print('train_sample_csv_file: \\t\\t%s' % train_sample_csv_file)\n",
    "print('test_csv_file: \\t\\t\\t%s' % test_csv_file)\n",
    "print('sample_submission_csv_file: \\t%s' % sample_submission_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_csv = pd.read_csv(sample_submission_csv_file)\n",
    "print('sample_submission_csv.shape: \\t', sample_submission_csv.shape)\n",
    "display(sample_submission_csv.head(2))\n",
    "\n",
    "print('train_csv: %.2f Mb' % (sys.getsizeof(sample_submission_csv)/1024./1024.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature(x_data, y_data, file_name):\n",
    "    print(y_data[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: %s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('x_data', data=x_data)\n",
    "        h.create_dataset('y_data', data=y_data)\n",
    "    print('File saved:   %s' % file_name)\n",
    "\n",
    "def load_feature(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        x_data = np.array(h['x_data'])\n",
    "        y_data = np.array(h['y_data'])\n",
    "    print('File loaded:  %s' % file_name)\n",
    "    print(y_data[:5])\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def save_test_feature(x_test, click_ids, file_name):\n",
    "    print(click_ids[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: %s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('x_test', data=x_test)\n",
    "        h.create_dataset('click_ids', data=click_ids)\n",
    "    print('File saved:   %s' % file_name)\n",
    "\n",
    "def load_test_feature(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        x_test = np.array(h['x_test'])\n",
    "        click_ids = np.array(h['click_ids'])\n",
    "    print('File loaded:  %s' % file_name)\n",
    "    print(click_ids[:5])\n",
    "    \n",
    "    return x_test, click_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_map(feature_map, file_name):\n",
    "    print(feature_map[:5])\n",
    "    feature_map_encode = []\n",
    "    for item in feature_map:\n",
    "        feature_name_encode = item[1].encode('UTF-8')\n",
    "        feature_map_encode.append((item[0], feature_name_encode))\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('feature_map', data=feature_map_encode)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_feature_map(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        feature_map_encode = np.array(h['feature_map'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    feature_map = []\n",
    "    for item in feature_map_encode:\n",
    "        feature_name = item[1].decode('UTF-8')\n",
    "        feature_map.append((int(item[0]), feature_name))\n",
    "    print(feature_map[:5])\n",
    "    \n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(data):\n",
    "    if isinstance(data, list):\n",
    "        print(len(data), '\\t\\t%.2f Mb' % (sys.getsizeof(data)/1024./1024.))\n",
    "    else:\n",
    "        print(data.shape, '\\t%.2f Mb' % (sys.getsizeof(data)/1024./1024.))\n",
    "\n",
    "test_np = np.ones((5000, 10))\n",
    "describe(test_np)\n",
    "describe(list(range(5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "feature_files = []\n",
    "x_train = []\n",
    "y_train = []\n",
    "if date == 100:\n",
    "    for key in [7, 8, 9]:\n",
    "        y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s.p' % (feature_run_name, key))\n",
    "        feature_files.append(y_proba_file)\n",
    "        x_train_date, y_train_date = load_feature(y_proba_file)\n",
    "        x_train.append(x_train_date)\n",
    "        y_train.append(y_train_date)\n",
    "    x_train = np.concatenate(x_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "else:\n",
    "    y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s.p' % (feature_run_name, date))\n",
    "    feature_files.append(y_proba_file)\n",
    "    x_train, y_train = load_feature(y_proba_file)\n",
    "\n",
    "# Use date 6 as validation dataset\n",
    "y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s.p' % (feature_run_name, 6))\n",
    "feature_files.append(y_proba_file)\n",
    "x_val, y_val = load_feature(y_proba_file)\n",
    "\n",
    "\n",
    "\n",
    "print('*' * 80)\n",
    "describe(x_train)\n",
    "describe(y_train)\n",
    "describe(x_val)\n",
    "describe(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_data[skip_data_len: data_len], y_data[skip_data_len: data_len], test_size=0.1, random_state=random_num, shuffle=True)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=random_num, shuffle=False)\n",
    "\n",
    "# x_train = x_train[skip_data_len: skip_data_len + data_len]\n",
    "# y_train = y_train[skip_data_len: skip_data_len + data_len]\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=random_num)\n",
    "describe(x_train)\n",
    "describe(y_train)\n",
    "describe(x_val)\n",
    "describe(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBClassifier, Booster\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(file_path, is_print=False):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        if is_print:\n",
    "            print('File removed: %s' % file_path)\n",
    "    else:\n",
    "        if is_print:\n",
    "            print('File do not exists: %s' % file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp_model_file(kfold_indx=None):\n",
    "    if kfold_indx is None:\n",
    "        return os.path.join(model_folder, 'temp_%s.xgb_model' % run_name)\n",
    "    return os.path.join(model_folder, 'temp_%s_p%s.xgb_model' % (run_name, kfold_indx))\n",
    "\n",
    "def get_model_file(kfold_indx=None):\n",
    "    if kfold_indx is None:\n",
    "        return os.path.join(model_folder, '%s.xgb_model' % run_name)\n",
    "    return os.path.join(model_folder, '%s_p%s.xgb_model' % (run_name, kfold_indx))\n",
    "\n",
    "def get_xgb_model_from_file(file_path, is_print=True):\n",
    "    model = XGBClassifier(\n",
    "        max_depth=3, \n",
    "        learning_rate=0.1, \n",
    "        n_estimators=1000, \n",
    "        silent=True, \n",
    "        objective='gpu:binary:logistic', \n",
    "        booster='gbtree', \n",
    "        n_jobs=cpu_amount, \n",
    "        nthread=None, \n",
    "        gamma=0, \n",
    "        min_child_weight=1, \n",
    "        max_delta_step=0, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=1, \n",
    "        colsample_bylevel=1, \n",
    "        reg_alpha=1, \n",
    "        reg_lambda=2, \n",
    "        scale_pos_weight=97, \n",
    "        base_score=0.5, \n",
    "        random_state=random_num, \n",
    "        seed=None, \n",
    "        missing=None,\n",
    "        # booster params\n",
    "        num_boost_round=60,\n",
    "        early_stopping_rounds=30,\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric=['auc'],\n",
    "        verbose_eval=False,\n",
    "#         n_gpus=8,\n",
    "    )\n",
    "    booster = Booster()\n",
    "    booster.load_model(file_path)\n",
    "    model._Booster = booster\n",
    "    if is_print:\n",
    "        print('XGBoost model loaded: %s' % file_path)\n",
    "    return model\n",
    "\n",
    "def partial_predict_proba(model, x_data, n_folds=100):\n",
    "    kf = KFold(len(x_data), n_folds=n_folds)\n",
    "    y_data_probas = []\n",
    "    print('|', end='')\n",
    "    for train_index, val_index in kf:\n",
    "        print('*', end='')\n",
    "        y_data_proba_fold = model.predict_proba(x_data[val_index])\n",
    "        y_data_probas.append(y_data_proba_fold)\n",
    "    y_data_probas = np.concatenate(y_data_probas, axis=0)\n",
    "    print('|')\n",
    "    return y_data_probas[:, 1]\n",
    "\n",
    "def partial_validation(model, x_data, y_data):\n",
    "    y_data_proba = partial_predict_proba(model, x_data)\n",
    "    y_data_pred = (y_data_proba>=0.5).astype(int)\n",
    "    acc = accuracy_score(y_data, y_data_pred)\n",
    "    roc = roc_auc_score(y_data, y_data_proba)\n",
    "    return acc, roc\n",
    "\n",
    "def partial_fit(x_train, y_train, x_val, y_val, temp_xgb_model_file, batch_size=10*10000):\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=3, \n",
    "        learning_rate=0.1, \n",
    "        n_estimators=1000, \n",
    "        silent=True, \n",
    "        objective='gpu:binary:logistic', \n",
    "        booster='gbtree', \n",
    "        n_jobs=cpu_amount, \n",
    "        nthread=None, \n",
    "        gamma=0, \n",
    "        min_child_weight=1, \n",
    "        max_delta_step=0, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=1, \n",
    "        colsample_bylevel=1, \n",
    "        reg_alpha=1, \n",
    "        reg_lambda=2, \n",
    "        scale_pos_weight=97, \n",
    "        base_score=0.5, \n",
    "        random_state=random_num, \n",
    "        seed=None, \n",
    "        missing=None,\n",
    "        # booster params\n",
    "        num_boost_round=60,\n",
    "        early_stopping_rounds=30,\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric=['auc'],\n",
    "        verbose_eval=False,\n",
    "#         n_gpus=8,\n",
    "    )\n",
    "\n",
    "    len_train = len(x_train)\n",
    "    n_folds = len_train // batch_size\n",
    "    print('continue_XGBC: len(x_train)=%s, n_folders=%s' % (len_train, n_folds))\n",
    "    kf = KFold(len_train, n_folds=n_folds)\n",
    "    epoch = 2\n",
    "    for j in range(epoch):\n",
    "        x_train, y_train = shuffle(x_train, y_train, random_state=None)\n",
    "        for i, indxes in enumerate(kf):\n",
    "            print('%s/%s' % (i+1, n_folds), end='')\n",
    "            kf_start = time.time()\n",
    "            train_index = indxes[0]\n",
    "            val_index = indxes[1]\n",
    "            x_train_patial = x_train[val_index]\n",
    "            y_train_patial = y_train[val_index]\n",
    "            # describe(x_train_patial)\n",
    "            # describe(y_train_patial)\n",
    "            # describe(x_val_patial)\n",
    "            # describe(y_val_patial)\n",
    "\n",
    "            temp_model_file = None\n",
    "            if os.path.exists(temp_xgb_model_file):\n",
    "                temp_model_file = temp_xgb_model_file\n",
    "                print('+', end='')\n",
    "    #             print('xgb_model loaded: %s' % temp_model_file)\n",
    "            else:\n",
    "                print('-', end='')\n",
    "    #             print('File do not exists: %s' % temp_model_file)\n",
    "            clf.fit(\n",
    "                x_train_patial, \n",
    "                y_train_patial,\n",
    "            #     sample_weight=None, \n",
    "                eval_set=[(x_train_patial, y_train_patial)], \n",
    "                eval_metric=['auc'], \n",
    "                early_stopping_rounds=20, \n",
    "                verbose=False, \n",
    "                xgb_model=temp_model_file\n",
    "            )\n",
    "            clf.get_booster().save_model(temp_xgb_model_file)\n",
    "            del train_index\n",
    "            del val_index\n",
    "            del x_train_patial\n",
    "            del y_train_patial\n",
    "            # print('saved xgb_model')\n",
    "            kf_time_cost = time.time() - kf_start\n",
    "            print(': %.2fs...' % kf_time_cost, end='')\n",
    "            gc.collect()\n",
    "            time.sleep(2)\n",
    "            gc.collect()\n",
    "        print('')\n",
    "\n",
    "        acc_train, roc_train = partial_validation(clf, x_train, y_train)\n",
    "        acc_val, roc_val = partial_validation(clf, x_val, y_val)\n",
    "        print((acc_train, roc_train, acc_val, roc_val))\n",
    "    del clf\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_val\n",
    "    del y_val\n",
    "    return acc_train, roc_train, acc_val, roc_val\n",
    "\n",
    "def fit(x_train, y_train, x_val, y_val, n_folds=4, batch_size=10*10000):\n",
    "    len_train = len(x_train)\n",
    "    n_folds = n_folds\n",
    "    print('kfold_XGBC: len(x_train)=%s, n_folders=%s' % (len_train, n_folds))\n",
    "    kf = KFold(len_train, n_folds=n_folds)\n",
    "    \n",
    "    results = []\n",
    "    for i, indxes in enumerate(kf):\n",
    "        print('KFold: %s/%s' % (i+1, n_folds))\n",
    "        temp_model_file = get_temp_model_file(i)\n",
    "        remove_file(temp_model_file, True)\n",
    "        \n",
    "        train_index = indxes[0]\n",
    "        val_index = indxes[1]\n",
    "        kf_start = time.time()\n",
    "        x_train_patial = x_train[train_index]\n",
    "        y_train_patial = y_train[train_index]\n",
    "        x_val_patial = x_train[val_index]\n",
    "        y_val_patial = y_train[val_index]\n",
    "        # describe(x_train_patial)\n",
    "        # describe(y_train_patial)\n",
    "        # describe(x_val_patial)\n",
    "        # describe(y_val_patial)\n",
    "        acc_train, roc_train, acc_val, roc_val = partial_fit(x_train_patial, y_train_patial, x_val_patial, y_val_patial, temp_model_file, batch_size)\n",
    "        model = get_xgb_model_from_file(temp_model_file)\n",
    "        kfold_acc_val1, kfold_roc_val1 = partial_validation(model, x_val, y_val)\n",
    "        model_file = get_model_file(i)\n",
    "        model.get_booster().save_model(model_file)\n",
    "        time_cost = time.time() - kf_start\n",
    "        kfold_result = {\n",
    "            'model_file': model_file,\n",
    "            'acc_train': acc_train,\n",
    "            'roc_train': roc_train,\n",
    "            'acc_val': acc_val,\n",
    "            'roc_val': roc_val,\n",
    "            'time_cost': time_cost\n",
    "        }\n",
    "        results.append(kfold_result)\n",
    "        remove_file(temp_model_file)\n",
    "        del train_index\n",
    "        del val_index\n",
    "        del x_train_patial\n",
    "        del y_train_patial\n",
    "        del x_val_patial\n",
    "        del y_val_patial\n",
    "        gc.collect()\n",
    "        time.sleep(2)\n",
    "        gc.collect()\n",
    "        print('model_file: %s, acc_train: %.4f, roc_train: %.4f, acc_val: %.4f, roc_val: %.4f, kfold_acc_val: %.4f, kfold_roc_val: %.4f, time_cost: %.2f s' \\\n",
    "                % (model_file, acc_train, roc_train, acc_val, roc_val, kfold_acc_val, kfold_roc_val, time_cost))\n",
    "    return results\n",
    "\n",
    "def get_model_files():\n",
    "    file_names = os.listdir(model_folder)\n",
    "    model_files = []\n",
    "    for file_name in file_names:\n",
    "        if file_name.endswith('.xgb_model') and file_name.startswith('%s' % run_name):\n",
    "            file_path = os.path.join(model_folder, file_name)\n",
    "            model_files.append(file_path)\n",
    "\n",
    "    return model_files\n",
    "\n",
    "def predict_proba(x_test):\n",
    "    y_data_probas = []\n",
    "    model_files = get_model_files()\n",
    "    if len(model_files) == 0:\n",
    "        raise Exception('Do not any find xgboost model file!')\n",
    "    for model_file in model_files:\n",
    "        model = get_xgb_model_from_file(model_file)\n",
    "        y_data_proba = partial_predict_proba(model, x_test)\n",
    "        y_data_proba = y_data_proba[:, np.newaxis]\n",
    "        print(y_data_proba.shape)\n",
    "        y_data_probas.append(y_data_proba)\n",
    "    y_data_probas = np.concatenate(y_data_probas, axis=-1)\n",
    "    print(y_data_probas.shape)\n",
    "    y_data_probas_mean = np.mean(y_data_probas, axis=-1)\n",
    "    print(y_data_probas_mean.shape)\n",
    "    return y_data_probas_mean\n",
    "\n",
    "def ensemble_validation(x_data, y_data):\n",
    "    y_data_proba = predict_proba(x_data)\n",
    "    y_data_pred = (y_data_proba>=0.5).astype(int)\n",
    "    acc = accuracy_score(y_data, y_data_pred)\n",
    "    roc = roc_auc_score(y_data, y_data_proba)\n",
    "    return acc, roc\n",
    "\n",
    "def validation(y_data, y_data_proba):\n",
    "    y_data_pred = (y_data_proba>=0.5).astype(int)\n",
    "    acc = accuracy_score(y_data, y_data_pred)\n",
    "    roc = roc_auc_score(y_data, y_data_proba)\n",
    "    return acc, roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = 200 #len(x_train)\n",
    "batch_size = 100 * 10000\n",
    "\n",
    "results = fit(x_train[:data_rows], y_train[:data_rows], x_val, y_val, 2, batch_size)\n",
    "y_val_proba = predict_proba(x_val)\n",
    "acc_val, roc_val = validation(y_val, y_val_proba)\n",
    "describe(y_val_proba)\n",
    "\n",
    "print('acc_val: %.4f, roc_val: %.4f' % (acc_val, roc_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = get_xgb_model_from_file(get_model_files()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,int(x_train.shape[1]/2)))\n",
    "plot_importance(xgboost_model, height=0.5, ax=ax, max_num_features=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_file_name = os.path.join(feature_folder, 'feature_map_TalkingdataAFD2018_FeatureExtraction_test_20180501_143516.p')\n",
    "\n",
    "feature_map = load_feature_map(feature_map_file_name)\n",
    "print(len(feature_map))\n",
    "print(feature_map[:5])\n",
    "\n",
    "feature_dict = {}\n",
    "for item in feature_map:\n",
    "    feature_dict[item[0]] = item[1]\n",
    "print(list(feature_dict.keys())[:5])\n",
    "print(list(feature_dict.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir(grid_search.best_estimator_.get_booster()))\n",
    "importance_score = xgboost_model.get_booster().get_fscore()\n",
    "sorted_score = []\n",
    "for key in importance_score:\n",
    "    indx = int(key[1:])\n",
    "    sorted_score.append((importance_score[key], key, indx, feature_dict[indx]))\n",
    "dtype = [('importance_score', int), ('key', 'S50'), ('indx', int), ('name', 'S50')]\n",
    "importance_table = np.array(sorted_score, dtype=dtype)\n",
    "display(importance_table[:2])\n",
    "importance_table = np.sort(importance_table, axis=0, order=['importance_score'])\n",
    "importance_table = importance_table[::-1]\n",
    "display(importance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "del y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_acc = run_name + '_' + str(int(roc_val*10000)).zfill(4)\n",
    "print(run_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_proba_file = os.path.join(feature_folder, 'feature_%s_test.p' % feature_run_name)\n",
    "feature_files.append(y_proba_file)\n",
    "x_test, click_ids = load_test_feature(y_proba_file)\n",
    "\n",
    "describe(x_test)\n",
    "describe(click_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba = predict_proba(x_test)\n",
    "\n",
    "print(y_test_proba.shape)\n",
    "print(y_test_proba[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_proba(y_val_proba, y_val, y_test_proba, click_ids, file_name):\n",
    "    print(click_ids[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: %s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "#         h.create_dataset('y_train_proba', data=y_train_proba)\n",
    "#         h.create_dataset('y_train', data=y_train)\n",
    "        h.create_dataset('y_val_proba', data=y_val_proba)\n",
    "        h.create_dataset('y_val', data=y_val)\n",
    "        h.create_dataset('y_test_proba', data=y_test_proba)\n",
    "        h.create_dataset('click_ids', data=click_ids)\n",
    "    print('File saved:   %s' % file_name)\n",
    "\n",
    "def load_proba(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "#         y_train_proba = np.array(h['y_train_proba'])\n",
    "#         y_train = np.array(h['y_train'])\n",
    "        y_val_proba = np.array(h['y_val_proba'])\n",
    "        y_val = np.array(h['y_val'])\n",
    "        y_test_proba = np.array(h['y_test_proba'])\n",
    "        click_ids = np.array(h['click_ids'])\n",
    "    print('File loaded:  %s' % file_name)\n",
    "    print(click_ids[:5])\n",
    "    \n",
    "    return y_val_proba, y_val, y_test_proba, click_ids\n",
    "\n",
    "\n",
    "y_proba_file = os.path.join(model_folder, 'proba_%s.p' % run_name_acc)\n",
    "print(y_proba_file)\n",
    "save_proba(\n",
    "#     y_train_proba, \n",
    "#     y_train, \n",
    "    y_val_proba, \n",
    "    y_val, \n",
    "    y_test_proba, \n",
    "    np.array(sample_submission_csv['click_id']), \n",
    "    y_proba_file\n",
    ")\n",
    "y_val_proba_true, y_val, y_test_proba_true, click_ids = load_proba(y_proba_file)\n",
    "\n",
    "# print(y_train_proba_true.shape)\n",
    "# print(y_train.shape)\n",
    "print(y_val_proba_true.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test_proba_true.shape)\n",
    "print(len(click_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "submission_csv_file = os.path.join(output_folder, 'pred_%s.csv' % run_name_acc)\n",
    "print(submission_csv_file)\n",
    "submission_csv = pd.DataFrame({ 'click_id': click_ids , 'is_attributed': y_test_proba_true })\n",
    "submission_csv.to_csv(submission_csv_file, index = False)\n",
    "display(submission_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time cost: %.2f s' % (time.time() - t0))\n",
    "\n",
    "print('random_num: ', random_num)\n",
    "print('date: ', date)\n",
    "print(run_name_acc)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
