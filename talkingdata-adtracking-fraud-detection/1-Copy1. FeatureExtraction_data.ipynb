{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FeatureExtraction_data\n",
    "\n",
    "Reference:\n",
    "- https://www.kaggle.com/asraful70/talkingdata-new-features-in-lightgbm-lb-0-9784\n",
    "- https://www.kaggle.com/danieleewww/talkingdata-added-new-features-in-lightg-50cf9b/code\n",
    "- https://www.kaggle.com/anttip/talkingdata-wordbatch-fm-ftrl-lb-0-9769\n",
    "- https://www.kaggle.com/pranav84/talkingdata-eda-to-model-evaluation-lb-0-9683\n",
    "- https://www.kaggle.com/aharless/kaggle-runnable-version-of-baris-kanber-s-lightgbm\n",
    "- https://www.kaggle.com/pranav84/lgb-entire-dataset-in-2-hrs-lb-0-9718\n",
    "- https://www.kaggle.com/panjianning/talkingdata-simple-lightgbm-0-9772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: TalkingdataAFD2018_FeatureExtraction_data_20180502_163811\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'TalkingdataAFD2018'\n",
    "step_name = 'FeatureExtraction_data'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = '%s_%s_%s' % (project_name, step_name, time_str)\n",
    "print('run_name: %s' % run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_debug: False\n"
     ]
    }
   ],
   "source": [
    "date = 7\n",
    "# print('date: ', date)\n",
    "\n",
    "is_debug = False\n",
    "print('is_debug: %s' % is_debug)\n",
    "\n",
    "\n",
    "if is_debug:\n",
    "    test_n_rows = 1 * 10000\n",
    "else:\n",
    "    test_n_rows = None\n",
    "#     test_n_rows = 18790469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_rows = {\n",
    "    0: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 1 * 10000\n",
    "    },\n",
    "    1: {\n",
    "        'n_skiprows': 1 * 10000,\n",
    "        'n_rows': 2 * 10000\n",
    "    },\n",
    "    6: {\n",
    "        'n_skiprows': 1,\n",
    "        'n_rows': 9308568\n",
    "    },\n",
    "    7: {\n",
    "        'n_skiprows': 1 + 9308568,\n",
    "        'n_rows': 59633310\n",
    "    },\n",
    "    8: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310,\n",
    "        'n_rows': 62945075\n",
    "    },\n",
    "    9: {\n",
    "        'n_skiprows': 1 + 9308568 + 59633310 + 62945075,\n",
    "        'n_rows': 53016937\n",
    "    }\n",
    "}\n",
    "# n_skiprows = day_rows[date]['n_skiprows']\n",
    "# n_rows = day_rows[date]['n_rows']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_num: 9431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import h5py\n",
    "import pickle\n",
    "import math\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_num = np.random.randint(10000)\n",
    "print('random_num: %s' % random_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_folder: \t\t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/input\n",
      "output_folder: \t\t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/output\n",
      "model_folder: \t\t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/model\n",
      "feature_folder: \t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/feature\n",
      "log_folder: \t\t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/log\n",
      "\n",
      "train_csv_file: \t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/input/train.csv\n",
      "train_sample_csv_file: \t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/input/train_sample.csv\n",
      "test_csv_file: \t\t\t/data1/kaggle/talkingdata-adtracking-fraud-detection/input/test.csv\n",
      "sample_submission_csv_file: \t/data1/kaggle/talkingdata-adtracking-fraud-detection/input/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t%s' % input_folder)\n",
    "print('output_folder: \\t\\t\\t%s' % output_folder)\n",
    "print('model_folder: \\t\\t\\t%s' % model_folder)\n",
    "print('feature_folder: \\t\\t%s' % feature_folder)\n",
    "print('log_folder: \\t\\t\\t%s' % log_folder)\n",
    "\n",
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "train_sample_csv_file = os.path.join(input_folder, 'train_sample.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_csv_file = os.path.join(input_folder, 'sample_submission.csv')\n",
    "\n",
    "print('\\ntrain_csv_file: \\t\\t%s' % train_csv_file)\n",
    "print('train_sample_csv_file: \\t\\t%s' % train_sample_csv_file)\n",
    "print('test_csv_file: \\t\\t\\t%s' % test_csv_file)\n",
    "print('sample_submission_csv_file: \\t%s' % sample_submission_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_columns  = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'click_id']\n",
    "dtypes = {\n",
    "    'ip'            : 'uint32',\n",
    "    'app'           : 'uint16',\n",
    "    'device'        : 'uint16',\n",
    "    'os'            : 'uint16',\n",
    "    'channel'       : 'uint16',\n",
    "    'is_attributed' : 'uint8',\n",
    "    'click_id'      : 'uint32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission_csv.shape: \t (18790469, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_id</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click_id  is_attributed\n",
       "0         0              0\n",
       "1         1              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv: 286.72 Mb\n"
     ]
    }
   ],
   "source": [
    "sample_submission_csv = pd.read_csv(sample_submission_csv_file)\n",
    "print('sample_submission_csv.shape: \\t', sample_submission_csv.shape)\n",
    "display(sample_submission_csv.head(2))\n",
    "\n",
    "print('train_csv: %.2f Mb' % (sys.getsizeof(sample_submission_csv)/1024./1024.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_click_time(df):\n",
    "    df['day'] = df['click_time'].dt.day.astype('uint8')\n",
    "    df['hour'] = df['click_time'].dt.hour.astype('uint8')\n",
    "    df['minute'] = df['click_time'].dt.minute.astype('uint8')\n",
    "    df['second'] = df['click_time'].dt.second.astype('uint8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prev_click(df, group_cols, agg_type='float32'):\n",
    "    agg_suffix = 'prevClick'\n",
    "    new_feature = new_feature = '{}_{}'.format('_'.join(group_cols), agg_suffix)\n",
    "    all_features = group_cols + ['click_time']\n",
    "    df[new_feature] = (df.click_time - df[all_features].groupby(group_cols).click_time.shift(+1) ).dt.seconds.astype(agg_type)\n",
    "    return df\n",
    "    \n",
    "def do_next_click(df, group_cols, agg_type='float32'):\n",
    "    agg_suffix = 'nextClick'\n",
    "    new_feature = new_feature = '{}_{}'.format('_'.join(group_cols), agg_suffix)\n",
    "    all_features = group_cols + ['click_time']\n",
    "    df[new_feature] = (df[all_features].groupby(group_cols).click_time.shift(-1) - df.click_time).dt.seconds.astype(agg_type)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below a function is written to extract count feature by aggregating different cols\n",
    "def do_count( df, group_cols, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name='{}_count'.format('_'.join(group_cols))\n",
    "    if show_agg:\n",
    "        print( \"Aggregating by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "    \n",
    "##  Below a function is written to extract unique count feature from different cols\n",
    "def do_countuniq( df, group_cols, counted, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_countuniq'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"Counting unqiue \", counted, \" by \", group_cols ,  '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "### Below a function is written to extract cumulative count feature  from different cols    \n",
    "def do_cumcount( df, group_cols, counted,agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_cumcount'.format(('_'.join(group_cols)),(counted)) \n",
    "    if show_agg:\n",
    "        print( \"Cumulative count by \", group_cols , '... and saved in', agg_name  )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n",
    "    df[agg_name]=gp.values\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "### Below a function is written to extract mean feature  from different cols\n",
    "def do_mean( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_mean'.format(('_'.join(group_cols)),(counted))  \n",
    "    if show_agg:\n",
    "        print( \"Calculating mean of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )\n",
    "\n",
    "def do_var( df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    agg_name= '{}_by_{}_var'.format(('_'.join(group_cols)),(counted)) \n",
    "    if show_agg:\n",
    "        print( \"Calculating variance of \", counted, \" by \", group_cols , '... and saved in', agg_name )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "#     predictors.append(agg_name)\n",
    "#     print('predictors',predictors)\n",
    "    gc.collect()\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature(x_data, y_data, file_name):\n",
    "    print(y_data[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('x_data', data=x_data)\n",
    "        h.create_dataset('y_data', data=y_data)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_feature(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        x_data = np.array(h['x_data'])\n",
    "        y_data = np.array(h['y_data'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    print(y_data[:5])\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def save_test_feature(x_test, click_ids, file_name):\n",
    "    print(click_ids[:5])\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('x_test', data=x_test)\n",
    "        h.create_dataset('click_ids', data=click_ids)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_test_feature(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        x_test = np.array(h['x_test'])\n",
    "        click_ids = np.array(h['click_ids'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    print(click_ids[:5])\n",
    "    \n",
    "    return x_test, click_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_map(feature_map, file_name):\n",
    "    print(feature_map[:5])\n",
    "    feature_map_encode = []\n",
    "    for item in feature_map:\n",
    "        feature_name_encode = item[1].encode('UTF-8')\n",
    "        feature_map_encode.append((item[0], feature_name_encode))\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print('File removed: \\t%s' % file_name)\n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset('feature_map', data=feature_map_encode)\n",
    "    print('File saved: \\t%s' % file_name)\n",
    "\n",
    "def load_feature_map(file_name):\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        feature_map_encode = np.array(h['feature_map'])\n",
    "    print('File loaded: \\t%s' % file_name)\n",
    "    feature_map = []\n",
    "    for item in feature_map_encode:\n",
    "        feature_name = item[1].decode('UTF-8')\n",
    "        feature_map.append((int(item[0]), feature_name))\n",
    "    print(feature_map[:5])\n",
    "    \n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feature(train_csv):\n",
    "    train_csv = do_click_time(train_csv)\n",
    "    \n",
    "    for cols in do_prev_click_cols:\n",
    "        print('>> ', cols)\n",
    "        train_csv = do_prev_click( train_csv, cols ); gc.collect()\n",
    "    \n",
    "    for cols in do_next_click_cols:\n",
    "        print('>> ', cols)\n",
    "        train_csv = do_next_click( train_csv, cols ); gc.collect()\n",
    "    \n",
    "    for cols in do_count_cols:\n",
    "        print('>> ', cols)\n",
    "        train_csv = do_count( train_csv, cols ); gc.collect()\n",
    "        \n",
    "    for cols in do_countuniq_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_countuniq( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "        \n",
    "    for cols in do_cumcount_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_cumcount( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "        \n",
    "    for cols in do_mean_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_mean( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "        \n",
    "    for cols in do_var_cols:\n",
    "        print('>> ', cols[:-1], cols[-1])\n",
    "        train_csv = do_var( train_csv, cols[:-1], cols[-1] ); gc.collect()\n",
    "    \n",
    "    train_csv.drop(['click_time'], axis=1, inplace=True)\n",
    "    print(train_csv.shape)\n",
    "\n",
    "    display(train_csv.head())\n",
    "\n",
    "    print(train_csv.columns)\n",
    "    print('data_size: %.2f Mb' % (sys.getsizeof(train_csv)/1024./1024.))\n",
    "    return train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = [\n",
    "    # 5 choice 2\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'device'],\n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'channel'],\n",
    "    ['app', 'device'],\n",
    "    ['app', 'os'],\n",
    "    ['app', 'channel'],\n",
    "    ['device', 'os'],\n",
    "    ['device', 'channel'],\n",
    "    ['os', 'channel'],\n",
    "    # 5 choice 3\n",
    "    ['device', 'os', 'channel'],\n",
    "    ['app', 'os', 'channel'],\n",
    "    ['app', 'device', 'channel'],\n",
    "    ['app', 'device', 'os'],\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'device', 'os'],\n",
    "    ['ip', 'app', 'channel'],\n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'app', 'device'],\n",
    "]\n",
    "\n",
    "template_hour = [\n",
    "    # 5 choice 2\n",
    "    ['ip', 'app', 'hour'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['ip', 'os', 'hour'],\n",
    "    ['ip', 'channel', 'hour'],\n",
    "    ['app', 'device', 'hour'],\n",
    "    ['app', 'os', 'hour'],\n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['device', 'os', 'hour'],\n",
    "    ['device', 'channel', 'hour'],\n",
    "    ['os', 'channel', 'hour'],\n",
    "    # 5 choice 3\n",
    "    ['device', 'os', 'channel', 'hour'],\n",
    "    ['app', 'os', 'channel', 'hour'],\n",
    "    ['app', 'device', 'channel', 'hour'],\n",
    "    ['app', 'device', 'os', 'hour'],\n",
    "    ['ip', 'os', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    ['ip', 'app', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'os', 'hour'],\n",
    "    ['ip', 'app', 'device', 'hour'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ip'] device\n",
      "['app'] channel\n",
      "['device', 'os', 'channel'] hour\n",
      "['ip', 'device'] hour\n",
      "['app', 'device'] os\n",
      "['app', 'os', 'channel'] hour\n",
      "['app'] os\n",
      "['app'] hour\n",
      "['ip', 'day'] hour\n",
      "['ip'] app\n",
      "['ip', 'app'] os\n"
     ]
    }
   ],
   "source": [
    "do_prev_click_cols = [\n",
    "    ['ip', 'device'],\n",
    "    ['ip', 'app', 'device'],\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'device', 'os'],\n",
    "    ['ip', 'app', 'device', 'os', 'channel'],\n",
    "    ['ip', 'app', 'os', 'channel'],\n",
    "    ['ip', 'device', 'os', 'channel'],\n",
    "    \n",
    "    \n",
    "    \n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'channel'], # ref\n",
    "]\n",
    "\n",
    "do_next_click_cols = [\n",
    "    ['ip', 'device'],\n",
    "    ['ip', 'app', 'device'],\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'device', 'os'], # ref\n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'device', 'os'], # ref\n",
    "    ['ip', 'device', 'os', 'channel'],\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'app', 'os', 'channel'],\n",
    "    \n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    \n",
    "    ['ip', 'app', 'device', 'os', 'channel'], # ref\n",
    "    ['device', 'channel'], # ref\n",
    "    ['app', 'device', 'channel'], # ref\n",
    "    ['device', 'hour'], # ref\n",
    "    \n",
    "#     ['ip', 'device'],\n",
    "#     ['ip', 'app', 'device', 'channel'],\n",
    "#     ['ip', 'os'],\n",
    "#     ['ip', 'app', 'channel'],\n",
    "#     ['ip' ,'channel'],\n",
    "]\n",
    "\n",
    "do_count_cols = [\n",
    "    ['ip', 'device'],\n",
    "    ['app', 'channel'],\n",
    "    ['device', 'os', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['app', 'device', 'os'],\n",
    "    ['app', 'os', 'channel', 'hour'],\n",
    "    ['app', 'os'],\n",
    "    \n",
    "    \n",
    "    ['app', 'hour'],\n",
    "    ['ip', 'day', 'hour'], # ref\n",
    "    ['ip', 'app'], # ref\n",
    "    ['ip', 'app', 'os'], # ref\n",
    "]\n",
    "do_countuniq_cols = [\n",
    "    ['ip', 'app'], # ref\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'device', 'os'], # ref\n",
    "    \n",
    "    \n",
    "    ['ip', 'channel'], # ref\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    \n",
    "    ['ip', 'day', 'hour'], # ref\n",
    "    ['ip', 'app', 'os'], # ref\n",
    "    ['ip', 'device'],\n",
    "    ['app', 'channel'],\n",
    "]\n",
    "do_cumcount_cols = [\n",
    "    ['app', 'os', 'hour'],\n",
    "    ['app', 'device', 'channel'],\n",
    "    ['app', 'device'],\n",
    "    ['app', 'device', 'os'],\n",
    "    ['device', 'os'],\n",
    "    \n",
    "    \n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['os', 'channel'],\n",
    "    ['device', 'channel', 'hour'],\n",
    "    ['device', 'os', 'channel'],\n",
    "    ['os', 'channel', 'hour'],\n",
    "    ['device', 'os', 'hour'],\n",
    "    ['app', 'device', 'channel', 'hour'],\n",
    "    ['app', 'os', 'channel'],\n",
    "    \n",
    "    ['ip', 'os'], # ref\n",
    "    ['ip', 'device', 'os'], # ref\n",
    "]\n",
    "do_mean_cols = [\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'channel'],\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'device', 'os'],\n",
    "    ['ip', 'os'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['ip', 'channel'],\n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['os', 'channel', 'hour'],\n",
    "    ['app', 'os', 'channel'],\n",
    "    ['device', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'hour'],\n",
    "    ['ip', 'os', 'hour'],\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    ['ip', 'os', 'channel', 'hour'],\n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['app', 'device', 'os', 'hour'],\n",
    "    \n",
    "\n",
    "    \n",
    "#     ['ip', 'os', 'channel'],\n",
    "#     ['ip', 'app', 'os'],\n",
    "#     ['ip', 'device', 'channel']\n",
    "]\n",
    "do_var_cols = [\n",
    "    ['ip', 'os', 'channel'],\n",
    "    ['ip', 'app'],\n",
    "    ['ip', 'app', 'channel'],\n",
    "    ['ip', 'device', 'hour'],\n",
    "    ['ip', 'device', 'channel'],\n",
    "    ['ip', 'app', 'os'],\n",
    "    ['ip', 'device', 'os'],\n",
    "    ['ip', 'channel'],\n",
    "    ['ip', 'os'],\n",
    "    ['app', 'os', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'os', 'hour'],\n",
    "    ['device', 'os', 'channel', 'hour'],\n",
    "    ['os', 'channel'],\n",
    "    ['app', 'channel', 'hour'],\n",
    "    ['ip', 'device', 'channel', 'hour'],\n",
    "    ['ip', 'app', 'device'],\n",
    "    ['app', 'os', 'hour'],\n",
    "    \n",
    "    ['ip', 'app', 'hour'],\n",
    "    ['ip', 'app', 'device', 'hour'],\n",
    "#     ['ip', 'os', 'hour'],\n",
    "    ['app', 'os', 'channel'],\n",
    "#     ['ip', 'channel']\n",
    "]\n",
    "\n",
    "for cols in do_count_cols:\n",
    "    print(cols[:-1], cols[-1])\n",
    "\n",
    "feature_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# test_csv = pd.read_csv(\n",
    "#     test_csv_file, \n",
    "#     nrows=test_n_rows, \n",
    "#     usecols=test_columns,\n",
    "#     dtype=dtypes,\n",
    "#     parse_dates=['click_time']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# print('test_csv.shape: \\t\\t', test_csv.shape)\n",
    "# display(test_csv.head(2))\n",
    "# print('test_csv:  %.2f Mb' % (sys.getsizeof(test_csv)/1024./1024.))\n",
    "# # print('*' * 80)\n",
    "\n",
    "# click_ids = test_csv['click_id']\n",
    "# test_csv.drop(['click_id'], axis=1, inplace=True)\n",
    "# display(click_ids.head())\n",
    "\n",
    "# display(test_csv.head())\n",
    "# # print('*' * 80)\n",
    "\n",
    "# test_csv = do_feature(test_csv)\n",
    "    \n",
    "# y_proba_file = os.path.join(feature_folder, 'feature_%s_test.p' % run_name)\n",
    "# feature_files.append(y_proba_file)\n",
    "# save_test_feature(\n",
    "#     test_csv, \n",
    "#     click_ids, \n",
    "#     y_proba_file\n",
    "# )\n",
    "# x_test, click_ids = load_test_feature(y_proba_file)\n",
    "\n",
    "# print(x_test.shape)\n",
    "# print(len(click_ids))\n",
    "\n",
    "# feature_map = []\n",
    "# print('[')\n",
    "# for i, col in enumerate(test_csv.columns):\n",
    "#     feature_map.append((i, col))\n",
    "#     print('  (%s,\\t\"%s\")' % (i, col))\n",
    "# print(']')\n",
    "# feature_map_file_name = y_proba_file = os.path.join(feature_folder, 'feature_map_%s.p' % run_name)\n",
    "# save_feature_map(feature_map, feature_map_file_name)\n",
    "# feature_map1 = load_feature_map(feature_map_file_name)\n",
    "# print(len(feature_map1))\n",
    "# print(feature_map1[:5])\n",
    "\n",
    "# # del test_csv\n",
    "# # del x_test\n",
    "# # del click_ids\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date key: 0\n",
      "is_debug=False, skip date: 0\n",
      "date key: 1\n",
      "is_debug=False, skip date: 1\n",
      "date key: 6\n",
      "date key: 7\n",
      "date key: 8\n",
      "date key: 9\n"
     ]
    }
   ],
   "source": [
    "for key in day_rows.keys():\n",
    "    key_str = str(key)\n",
    "    print('date key: %s' % key_str)\n",
    "    if is_debug and key > 1:\n",
    "        print('is_debug=%s, skip date: %s' % (is_debug, key_str))\n",
    "        continue\n",
    "    if not is_debug and key <= 1:\n",
    "        print('is_debug=%s, skip date: %s' % (is_debug, key_str))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "date key: 7\n",
      "train_csv.shape: \t\t (59633310, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70712</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>237</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45892</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>424</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ip  app  device  os  channel click_time  is_attributed\n",
       "0  70712    2       1  32      237 2017-11-07              0\n",
       "1  45892    3       1  25      424 2017-11-07              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv: 1194.29 Mb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: is_attributed, dtype: uint8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70712</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>237</td>\n",
       "      <td>2017-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45892</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>424</td>\n",
       "      <td>2017-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37774</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>2017-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41179</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>122</td>\n",
       "      <td>2017-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83111</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ip  app  device  os  channel click_time\n",
       "0  70712    2       1  32      237 2017-11-07\n",
       "1  45892    3       1  25      424 2017-11-07\n",
       "2  37774    8       2  13      145 2017-11-07\n",
       "3  41179    2       1  13      122 2017-11-07\n",
       "4  83111   15       1   8      245 2017-11-07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  ['ip', 'device']\n",
      ">>  ['ip', 'app', 'device']\n",
      ">>  ['ip', 'app']\n",
      ">>  ['ip', 'app', 'device', 'os']\n",
      ">>  ['ip', 'app', 'device', 'os', 'channel']\n",
      ">>  ['ip', 'app', 'os', 'channel']\n",
      ">>  ['ip', 'device', 'os', 'channel']\n",
      ">>  ['ip', 'os']\n",
      ">>  ['ip', 'device', 'channel']\n",
      ">>  ['ip', 'channel']\n",
      ">>  ['ip', 'device']\n",
      ">>  ['ip', 'app', 'device']\n",
      ">>  ['ip', 'app']\n",
      ">>  ['ip', 'app', 'device', 'os']\n",
      ">>  ['ip', 'os']\n",
      ">>  ['ip', 'device', 'os']\n",
      ">>  ['ip', 'device', 'os', 'channel']\n",
      ">>  ['ip', 'os', 'channel']\n",
      ">>  ['ip', 'app', 'os', 'channel']\n",
      ">>  ['ip', 'app', 'os']\n",
      ">>  ['ip', 'device', 'channel']\n",
      ">>  ['ip', 'app', 'device', 'os', 'channel']\n",
      ">>  ['device', 'channel']\n",
      ">>  ['app', 'device', 'channel']\n",
      ">>  ['device', 'hour']\n",
      ">>  ['ip', 'device']\n",
      "Aggregating by  ['ip', 'device'] ... and saved in ip_device_count\n",
      ">>  ['app', 'channel']\n",
      "Aggregating by  ['app', 'channel'] ... and saved in app_channel_count\n",
      ">>  ['device', 'os', 'channel', 'hour']\n",
      "Aggregating by  ['device', 'os', 'channel', 'hour'] ... and saved in device_os_channel_hour_count\n",
      ">>  ['ip', 'device', 'hour']\n",
      "Aggregating by  ['ip', 'device', 'hour'] ... and saved in ip_device_hour_count\n",
      ">>  ['app', 'device', 'os']\n",
      "Aggregating by  ['app', 'device', 'os'] ... and saved in app_device_os_count\n",
      ">>  ['app', 'os', 'channel', 'hour']\n",
      "Aggregating by  ['app', 'os', 'channel', 'hour'] ... and saved in app_os_channel_hour_count\n",
      ">>  ['app', 'os']\n",
      "Aggregating by  ['app', 'os'] ... and saved in app_os_count\n",
      ">>  ['app', 'hour']\n",
      "Aggregating by  ['app', 'hour'] ... and saved in app_hour_count\n",
      ">>  ['ip', 'day', 'hour']\n",
      "Aggregating by  ['ip', 'day', 'hour'] ... and saved in ip_day_hour_count\n",
      ">>  ['ip', 'app']\n",
      "Aggregating by  ['ip', 'app'] ... and saved in ip_app_count\n",
      ">>  ['ip', 'app', 'os']\n",
      "Aggregating by  ['ip', 'app', 'os'] ... and saved in ip_app_os_count\n",
      ">>  ['ip'] app\n",
      "Counting unqiue  app  by  ['ip'] ... and saved in ip_by_app_countuniq\n",
      ">>  ['ip', 'device'] channel\n",
      "Counting unqiue  channel  by  ['ip', 'device'] ... and saved in ip_device_by_channel_countuniq\n",
      ">>  ['ip', 'device'] os\n",
      "Counting unqiue  os  by  ['ip', 'device'] ... and saved in ip_device_by_os_countuniq\n",
      ">>  ['ip'] channel\n",
      "Counting unqiue  channel  by  ['ip'] ... and saved in ip_by_channel_countuniq\n",
      ">>  ['ip', 'device', 'os'] hour\n",
      "Counting unqiue  hour  by  ['ip', 'device', 'os'] ... and saved in ip_device_os_by_hour_countuniq\n",
      ">>  ['ip', 'day'] hour\n",
      "Counting unqiue  hour  by  ['ip', 'day'] ... and saved in ip_day_by_hour_countuniq\n",
      ">>  ['ip', 'app'] os\n",
      "Counting unqiue  os  by  ['ip', 'app'] ... and saved in ip_app_by_os_countuniq\n",
      ">>  ['ip'] device\n",
      "Counting unqiue  device  by  ['ip'] ... and saved in ip_by_device_countuniq\n",
      ">>  ['app'] channel\n",
      "Counting unqiue  channel  by  ['app'] ... and saved in app_by_channel_countuniq\n",
      ">>  ['app', 'os'] hour\n",
      "Cumulative count by  ['app', 'os'] ... and saved in app_os_by_hour_cumcount\n",
      ">>  ['app', 'device'] channel\n",
      "Cumulative count by  ['app', 'device'] ... and saved in app_device_by_channel_cumcount\n",
      ">>  ['app'] device\n",
      "Cumulative count by  ['app'] ... and saved in app_by_device_cumcount\n",
      ">>  ['app', 'device'] os\n",
      "Cumulative count by  ['app', 'device'] ... and saved in app_device_by_os_cumcount\n",
      ">>  ['device'] os\n",
      "Cumulative count by  ['device'] ... and saved in device_by_os_cumcount\n",
      ">>  ['app', 'channel'] hour\n",
      "Cumulative count by  ['app', 'channel'] ... and saved in app_channel_by_hour_cumcount\n",
      ">>  ['os'] channel\n",
      "Cumulative count by  ['os'] ... and saved in os_by_channel_cumcount\n",
      ">>  ['device', 'channel'] hour\n",
      "Cumulative count by  ['device', 'channel'] ... and saved in device_channel_by_hour_cumcount\n",
      ">>  ['device', 'os'] channel\n",
      "Cumulative count by  ['device', 'os'] ... and saved in device_os_by_channel_cumcount\n",
      ">>  ['os', 'channel'] hour\n",
      "Cumulative count by  ['os', 'channel'] ... and saved in os_channel_by_hour_cumcount\n",
      ">>  ['device', 'os'] hour\n",
      "Cumulative count by  ['device', 'os'] ... and saved in device_os_by_hour_cumcount\n",
      ">>  ['app', 'device', 'channel'] hour\n",
      "Cumulative count by  ['app', 'device', 'channel'] ... and saved in app_device_channel_by_hour_cumcount\n",
      ">>  ['app', 'os'] channel\n",
      "Cumulative count by  ['app', 'os'] ... and saved in app_os_by_channel_cumcount\n",
      ">>  ['ip'] os\n",
      "Cumulative count by  ['ip'] ... and saved in ip_by_os_cumcount\n",
      ">>  ['ip', 'device'] os\n",
      "Cumulative count by  ['ip', 'device'] ... and saved in ip_device_by_os_cumcount\n",
      ">>  ['ip'] app\n",
      "Calculating mean of  app  by  ['ip'] ... and saved in ip_by_app_mean\n",
      ">>  ['ip', 'app'] channel\n",
      "Calculating mean of  channel  by  ['ip', 'app'] ... and saved in ip_app_by_channel_mean\n",
      ">>  ['ip', 'os'] channel\n",
      "Calculating mean of  channel  by  ['ip', 'os'] ... and saved in ip_os_by_channel_mean\n",
      ">>  ['ip', 'device'] os\n",
      "Calculating mean of  os  by  ['ip', 'device'] ... and saved in ip_device_by_os_mean\n",
      ">>  ['ip'] os\n",
      "Calculating mean of  os  by  ['ip'] ... and saved in ip_by_os_mean\n",
      ">>  ['ip', 'device'] hour\n",
      "Calculating mean of  hour  by  ['ip', 'device'] ... and saved in ip_device_by_hour_mean\n",
      ">>  ['ip'] channel\n",
      "Calculating mean of  channel  by  ['ip'] ... and saved in ip_by_channel_mean\n",
      ">>  ['ip', 'app'] os\n",
      "Calculating mean of  os  by  ['ip', 'app'] ... and saved in ip_app_by_os_mean\n",
      ">>  ['ip', 'device'] channel\n",
      "Calculating mean of  channel  by  ['ip', 'device'] ... and saved in ip_device_by_channel_mean\n",
      ">>  ['os', 'channel'] hour\n",
      "Calculating mean of  hour  by  ['os', 'channel'] ... and saved in os_channel_by_hour_mean\n",
      ">>  ['app', 'os'] channel\n",
      "Calculating mean of  channel  by  ['app', 'os'] ... and saved in app_os_by_channel_mean\n",
      ">>  ['device', 'channel'] hour\n",
      "Calculating mean of  hour  by  ['device', 'channel'] ... and saved in device_channel_by_hour_mean\n",
      ">>  ['ip', 'app', 'channel'] hour\n",
      "Calculating mean of  hour  by  ['ip', 'app', 'channel'] ... and saved in ip_app_channel_by_hour_mean\n",
      ">>  ['ip', 'app'] hour\n",
      "Calculating mean of  hour  by  ['ip', 'app'] ... and saved in ip_app_by_hour_mean\n",
      ">>  ['ip', 'os'] hour\n",
      "Calculating mean of  hour  by  ['ip', 'os'] ... and saved in ip_os_by_hour_mean\n",
      ">>  ['ip', 'device', 'os'] hour\n",
      "Calculating mean of  hour  by  ['ip', 'device', 'os'] ... and saved in ip_device_os_by_hour_mean\n",
      ">>  ['ip', 'os', 'channel'] hour\n",
      "Calculating mean of  hour  by  ['ip', 'os', 'channel'] ... and saved in ip_os_channel_by_hour_mean\n",
      ">>  ['app', 'channel'] hour\n",
      "Calculating mean of  hour  by  ['app', 'channel'] ... and saved in app_channel_by_hour_mean\n",
      ">>  ['app', 'device', 'os'] hour\n",
      "Calculating mean of  hour  by  ['app', 'device', 'os'] ... and saved in app_device_os_by_hour_mean\n",
      ">>  ['ip', 'os'] channel\n",
      "Calculating variance of  channel  by  ['ip', 'os'] ... and saved in ip_os_by_channel_var\n",
      ">>  ['ip'] app\n",
      "Calculating variance of  app  by  ['ip'] ... and saved in ip_by_app_var\n",
      ">>  ['ip', 'app'] channel\n",
      "Calculating variance of  channel  by  ['ip', 'app'] ... and saved in ip_app_by_channel_var\n",
      ">>  ['ip', 'device'] hour\n",
      "Calculating variance of  hour  by  ['ip', 'device'] ... and saved in ip_device_by_hour_var\n",
      ">>  ['ip', 'device'] channel\n",
      "Calculating variance of  channel  by  ['ip', 'device'] ... and saved in ip_device_by_channel_var\n",
      ">>  ['ip', 'app'] os\n",
      "Calculating variance of  os  by  ['ip', 'app'] ... and saved in ip_app_by_os_var\n",
      ">>  ['ip', 'device'] os\n",
      "Calculating variance of  os  by  ['ip', 'device'] ... and saved in ip_device_by_os_var\n",
      ">>  ['ip'] channel\n",
      "Calculating variance of  channel  by  ['ip'] ... and saved in ip_by_channel_var\n",
      ">>  ['ip'] os\n",
      "Calculating variance of  os  by  ['ip'] ... and saved in ip_by_os_var\n",
      ">>  ['app', 'os', 'channel'] hour\n",
      "Calculating variance of  hour  by  ['app', 'os', 'channel'] ... and saved in app_os_channel_by_hour_var\n",
      ">>  ['ip', 'device', 'os'] hour\n",
      "Calculating variance of  hour  by  ['ip', 'device', 'os'] ... and saved in ip_device_os_by_hour_var\n",
      ">>  ['device', 'os', 'channel'] hour\n",
      "Calculating variance of  hour  by  ['device', 'os', 'channel'] ... and saved in device_os_channel_by_hour_var\n",
      ">>  ['os'] channel\n",
      "Calculating variance of  channel  by  ['os'] ... and saved in os_by_channel_var\n",
      ">>  ['app', 'channel'] hour\n",
      "Calculating variance of  hour  by  ['app', 'channel'] ... and saved in app_channel_by_hour_var\n",
      ">>  ['ip', 'device', 'channel'] hour\n",
      "Calculating variance of  hour  by  ['ip', 'device', 'channel'] ... and saved in ip_device_channel_by_hour_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  ['ip', 'app'] device\n",
      "Calculating variance of  device  by  ['ip', 'app'] ... and saved in ip_app_by_device_var\n",
      ">>  ['app', 'os'] hour\n",
      "Calculating variance of  hour  by  ['app', 'os'] ... and saved in app_os_by_hour_var\n",
      ">>  ['ip', 'app'] hour\n",
      "Calculating variance of  hour  by  ['ip', 'app'] ... and saved in ip_app_by_hour_var\n",
      ">>  ['ip', 'app', 'device'] hour\n",
      "Calculating variance of  hour  by  ['ip', 'app', 'device'] ... and saved in ip_app_device_by_hour_var\n",
      ">>  ['app', 'os'] channel\n",
      "Calculating variance of  channel  by  ['app', 'os'] ... and saved in app_os_by_channel_var\n",
      "(59633310, 108)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>ip_device_prevClick</th>\n",
       "      <th>...</th>\n",
       "      <th>ip_device_os_by_hour_var</th>\n",
       "      <th>device_os_channel_by_hour_var</th>\n",
       "      <th>os_by_channel_var</th>\n",
       "      <th>app_channel_by_hour_var</th>\n",
       "      <th>ip_device_channel_by_hour_var</th>\n",
       "      <th>ip_app_by_device_var</th>\n",
       "      <th>app_os_by_hour_var</th>\n",
       "      <th>ip_app_by_hour_var</th>\n",
       "      <th>ip_app_device_by_hour_var</th>\n",
       "      <th>app_os_by_channel_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70712</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>237</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>67.890907</td>\n",
       "      <td>36.480629</td>\n",
       "      <td>17559.767578</td>\n",
       "      <td>32.625835</td>\n",
       "      <td>39.108669</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>37.185417</td>\n",
       "      <td>37.464363</td>\n",
       "      <td>37.600922</td>\n",
       "      <td>16347.711914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45892</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>424</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>61.739147</td>\n",
       "      <td>38.926994</td>\n",
       "      <td>16570.046875</td>\n",
       "      <td>39.924187</td>\n",
       "      <td>36.998493</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>34.566875</td>\n",
       "      <td>33.283192</td>\n",
       "      <td>33.364620</td>\n",
       "      <td>14225.310547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37774</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32.731190</td>\n",
       "      <td>36.997520</td>\n",
       "      <td>16420.923828</td>\n",
       "      <td>51.256840</td>\n",
       "      <td>51.111897</td>\n",
       "      <td>0.099950</td>\n",
       "      <td>47.651089</td>\n",
       "      <td>57.411449</td>\n",
       "      <td>48.552586</td>\n",
       "      <td>1341.653442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41179</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>122</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.243753</td>\n",
       "      <td>43.203209</td>\n",
       "      <td>16420.923828</td>\n",
       "      <td>44.056442</td>\n",
       "      <td>34.711113</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>38.905201</td>\n",
       "      <td>32.760025</td>\n",
       "      <td>32.669804</td>\n",
       "      <td>16304.954102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83111</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>245</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.454546</td>\n",
       "      <td>38.491219</td>\n",
       "      <td>16522.845703</td>\n",
       "      <td>38.126648</td>\n",
       "      <td>38.357769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.837791</td>\n",
       "      <td>39.207779</td>\n",
       "      <td>39.207779</td>\n",
       "      <td>10488.250977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ip  app  device  os  channel  day  hour  minute  second  \\\n",
       "0  70712    2       1  32      237    7     0       0       0   \n",
       "1  45892    3       1  25      424    7     0       0       0   \n",
       "2  37774    8       2  13      145    7     0       0       0   \n",
       "3  41179    2       1  13      122    7     0       0       0   \n",
       "4  83111   15       1   8      245    7     0       0       0   \n",
       "\n",
       "   ip_device_prevClick          ...            ip_device_os_by_hour_var  \\\n",
       "0                  NaN          ...                           67.890907   \n",
       "1                  NaN          ...                           61.739147   \n",
       "2                  NaN          ...                           32.731190   \n",
       "3                  NaN          ...                           19.243753   \n",
       "4                  NaN          ...                           19.454546   \n",
       "\n",
       "   device_os_channel_by_hour_var  os_by_channel_var  app_channel_by_hour_var  \\\n",
       "0                      36.480629       17559.767578                32.625835   \n",
       "1                      38.926994       16570.046875                39.924187   \n",
       "2                      36.997520       16420.923828                51.256840   \n",
       "3                      43.203209       16420.923828                44.056442   \n",
       "4                      38.491219       16522.845703                38.126648   \n",
       "\n",
       "   ip_device_channel_by_hour_var  ip_app_by_device_var  app_os_by_hour_var  \\\n",
       "0                      39.108669              0.003953           37.185417   \n",
       "1                      36.998493              0.003024           34.566875   \n",
       "2                      51.111897              0.099950           47.651089   \n",
       "3                      34.711113              0.017544           38.905201   \n",
       "4                      38.357769              0.000000           39.837791   \n",
       "\n",
       "   ip_app_by_hour_var  ip_app_device_by_hour_var  app_os_by_channel_var  \n",
       "0           37.464363                  37.600922           16347.711914  \n",
       "1           33.283192                  33.364620           14225.310547  \n",
       "2           57.411449                  48.552586            1341.653442  \n",
       "3           32.760025                  32.669804           16304.954102  \n",
       "4           39.207779                  39.207779           10488.250977  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ip', 'app', 'device', 'os', 'channel', 'day', 'hour', 'minute',\n",
      "       'second', 'ip_device_prevClick',\n",
      "       ...\n",
      "       'ip_device_os_by_hour_var', 'device_os_channel_by_hour_var',\n",
      "       'os_by_channel_var', 'app_channel_by_hour_var',\n",
      "       'ip_device_channel_by_hour_var', 'ip_app_by_device_var',\n",
      "       'app_os_by_hour_var', 'ip_app_by_hour_var', 'ip_app_device_by_hour_var',\n",
      "       'app_os_by_channel_var'],\n",
      "      dtype='object', length=108)\n",
      "data_size: 23885.72 Mb\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: is_attributed, dtype: uint8\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4b03e1cf959d>\u001b[0m in \u001b[0;36msave_feature\u001b[0;34m(x_data, y_data, file_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File removed: \\t%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File saved: \\t%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Validate shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# for key in day_rows.keys():\n",
    "print('*' * 80)\n",
    "key = date\n",
    "key_str = str(key)\n",
    "print('date key: %s' % key_str)\n",
    "\n",
    "n_skiprows = day_rows[key]['n_skiprows']\n",
    "n_rows = day_rows[key]['n_rows']\n",
    "\n",
    "train_csv = pd.read_csv(\n",
    "    train_csv_file, \n",
    "    skiprows=range(1, n_skiprows), \n",
    "    nrows=n_rows, \n",
    "    usecols=train_columns,\n",
    "    dtype=dtypes,\n",
    "    parse_dates=['click_time']\n",
    ")\n",
    "\n",
    "print('train_csv.shape: \\t\\t', train_csv.shape)\n",
    "display(train_csv.head(2))\n",
    "print('train_csv: %.2f Mb' % (sys.getsizeof(train_csv)/1024./1024.))\n",
    "#     print('*' * 80)\n",
    "\n",
    "y_data = train_csv['is_attributed']\n",
    "train_csv.drop(['is_attributed'], axis=1, inplace=True)\n",
    "display(y_data.head())\n",
    "\n",
    "display(train_csv.head())\n",
    "#     print('*' * 80)\n",
    "\n",
    "train_csv = do_feature(train_csv)\n",
    "\n",
    "y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s.p' % (run_name, key_str))\n",
    "feature_files.append(y_proba_file)\n",
    "save_feature(\n",
    "    train_csv, \n",
    "    y_data, \n",
    "    y_proba_file\n",
    ")\n",
    "x_data, y_data = load_feature(y_proba_file)\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "print('[')\n",
    "for i, col in enumerate(train_csv.columns):\n",
    "    print('  (%s,\\t\"%s\")' % (i, col))\n",
    "print('')\n",
    "#     del train_csv\n",
    "del x_data\n",
    "del y_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: is_attributed, dtype: uint8\n",
      "File saved: \t/data1/kaggle/talkingdata-adtracking-fraud-detection/feature/feature_TalkingdataAFD2018_FeatureExtraction_data_20180502_163811_date7_p1.p\n",
      "20000000    0\n",
      "20000001    0\n",
      "20000002    0\n",
      "20000003    0\n",
      "20000004    0\n",
      "Name: is_attributed, dtype: uint8\n",
      "File saved: \t/data1/kaggle/talkingdata-adtracking-fraud-detection/feature/feature_TalkingdataAFD2018_FeatureExtraction_data_20180502_163811_date7_p2.p\n",
      "40000000    0\n",
      "40000001    0\n",
      "40000002    0\n",
      "40000003    0\n",
      "40000004    0\n",
      "Name: is_attributed, dtype: uint8\n",
      "File saved: \t/data1/kaggle/talkingdata-adtracking-fraud-detection/feature/feature_TalkingdataAFD2018_FeatureExtraction_data_20180502_163811_date7_p3.p\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0b642a0ba191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# x_data, y_data = load_feature(y_proba_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_data' is not defined"
     ]
    }
   ],
   "source": [
    "y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s_p1.p' % (run_name, key_str))\n",
    "feature_files.append(y_proba_file)\n",
    "save_feature(\n",
    "    train_csv[: 2000*10000], \n",
    "    y_data[: 2000*10000], \n",
    "    y_proba_file\n",
    ")\n",
    "\n",
    "y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s_p2.p' % (run_name, key_str))\n",
    "feature_files.append(y_proba_file)\n",
    "save_feature(\n",
    "    train_csv[2000*10000: 4000*10000], \n",
    "    y_data[2000*10000: 4000*10000], \n",
    "    y_proba_file\n",
    ")\n",
    "\n",
    "y_proba_file = os.path.join(feature_folder, 'feature_%s_date%s_p3.p' % (run_name, key_str))\n",
    "feature_files.append(y_proba_file)\n",
    "save_feature(\n",
    "    train_csv[4000*10000: ], \n",
    "    y_data[4000*10000: ], \n",
    "    y_proba_file\n",
    ")\n",
    "\n",
    "# x_data, y_data = load_feature(y_proba_file)\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_data.shape)\n",
    "# print(y_data.shape)\n",
    "# print(x_test.shape)\n",
    "# print(click_ids.shape)\n",
    "for name in feature_files:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time cost: %.2f s' % (time.time() - t0))\n",
    "print(run_name)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
