{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_3channel_VGG19_FineTune\n",
    "\n",
    "Abstract:\n",
    "- single channel: band_avg\n",
    "- CNN, small net\n",
    "\n",
    "Result:\n",
    "- 增大网络的各个参数，只是把train数据的拟合程度增加了（到了99%），而cv数据的泛化能力仍然只有87%，说明，特征还需要进一步的提取来增加系统的泛化能力。\n",
    "\n",
    "References:\n",
    "- https://www.kaggle.com/ivalmian/simple-svd-xgboost-baseline-lb-35\n",
    "- https://www.kaggle.com/arieltci/a-keras-prototype-0-21174-on-pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, Binarizer\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import lzma\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from shutil import copy2\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: SC_Iceberg_Classifier_CNN_3channel_VGG19_FineTune_20180110_084730\n"
     ]
    }
   ],
   "source": [
    "project_name = 'SC_Iceberg_Classifier'\n",
    "step_name = 'CNN_3channel_VGG19_FineTune'\n",
    "date_str = time.strftime(\"%Y%m%d\", time.localtime())\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = project_name + '_' + step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_path: D:\\Kaggle\\statoil-iceberg-classifier-challenge\\input\n",
      "log_path: D:\\Kaggle\\statoil-iceberg-classifier-challenge\\log\n",
      "model_path: D:\\Kaggle\\statoil-iceberg-classifier-challenge\\model\n",
      "output_path: D:\\Kaggle\\statoil-iceberg-classifier-challenge\\output\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "input_path = os.path.join(cwd, 'input')\n",
    "log_path = os.path.join(cwd, 'log')\n",
    "model_path = os.path.join(cwd, 'model')\n",
    "output_path = os.path.join(cwd, 'output')\n",
    "print('input_path: ' + input_path)\n",
    "print('log_path: ' + log_path)\n",
    "print('model_path: ' + model_path)\n",
    "print('output_path: ' + output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with lzma.open(\"train.json.7z\") as f:\n",
    "#     file_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip(input_path, os.path.join(input_path, 'sample_submission.csv.7z'))\n",
    "# Unzip(input_path, os.path.join(input_path, 'test.json.7z'))\n",
    "# Unzip(input_path, os.path.join(input_path, 'train.json.7z'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def describe(arr):\n",
    "    print(arr.shape, arr.min(), arr.max(), sys.getsizeof(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  is_iceberg\n",
       "0  5941774d         0.5\n",
       "1  4023181e         0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_path = os.path.join(input_path, 'sample_submission.csv')\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "print(sample_submission.shape)\n",
    "sample_submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604,) 0 1 12936\n"
     ]
    }
   ],
   "source": [
    "is_iceberg_path = os.path.join(input_path, 'is_iceberg.p')\n",
    "y_data = pickle.load(open(is_iceberg_path, mode='rb'))\n",
    "describe(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604,) 0.0 45.9375 12936\n",
      "(8424,) 23.0805 50.66178518 67496\n",
      "Wall time: 7.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load orignal data\n",
    "inc_angle_data_path = os.path.join(input_path, 'inc_angle_data.p')\n",
    "inc_angle_test_path = os.path.join(input_path, 'inc_angle_test.p')\n",
    "\n",
    "inc_angle_data = pickle.load(open(inc_angle_data_path, mode='rb'))\n",
    "inc_angle_test = pickle.load(open(inc_angle_test_path, mode='rb'))\n",
    "\n",
    "describe(inc_angle_data)\n",
    "describe(inc_angle_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75) 0.0 1.0 128\n",
      "(1604, 75, 75) 0.0 1.0 128\n",
      "(8424, 75, 75) 0.0 1.0 128\n",
      "(8424, 75, 75) 0.0 1.0 128\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load orignal data\n",
    "band1_data_path = os.path.join(input_path, 'band1_data_gray.p')\n",
    "band2_data_path = os.path.join(input_path, 'band2_data_gray.p')\n",
    "band1_test_path = os.path.join(input_path, 'band1_test_gray.p')\n",
    "band2_test_path = os.path.join(input_path, 'band2_test_gray.p')\n",
    "\n",
    "band1_data = pickle.load(open(band1_data_path, mode='rb'))\n",
    "band2_data = pickle.load(open(band2_data_path, mode='rb'))\n",
    "band1_test = pickle.load(open(band1_test_path, mode='rb'))\n",
    "band2_test = pickle.load(open(band2_test_path, mode='rb'))\n",
    "\n",
    "describe(band1_data)\n",
    "describe(band2_data)\n",
    "describe(band1_test)\n",
    "describe(band2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75) 0.0 0.979400825264 72180128\n",
      "(8424, 75, 75) 0.0 0.930272493331 379080128\n",
      "(1604, 75, 75) 0.0 1.0 72180128\n",
      "(8424, 75, 75) 0.0 1.0 379080128\n",
      "(1604, 75, 75) 0.0210369138693 1.0 72180128\n",
      "(8424, 75, 75) 0.0 1.0 379080128\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load orignal data\n",
    "band_sub_data = np.fabs(np.subtract(band1_data, band2_data))\n",
    "band_sub_test = np.fabs(np.subtract(band1_test, band2_test))\n",
    "band_min_data = np.minimum(band1_data, band2_data)\n",
    "band_min_test = np.minimum(band1_test, band2_test)\n",
    "band_max_data = np.maximum(band1_data, band2_data)\n",
    "band_max_test = np.maximum(band1_test, band2_test)\n",
    "\n",
    "describe(band_sub_data)\n",
    "describe(band_sub_test)\n",
    "describe(band_min_data)\n",
    "describe(band_min_test)\n",
    "describe(band_max_data)\n",
    "describe(band_max_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75) 0.0 0.979400825264 72180128\n",
      "(8424, 75, 75) 0.0 0.930272493331 379080128\n",
      "(1604, 75, 75) 0.0 1.0 72180128\n",
      "(8424, 75, 75) 0.0 1.0 379080128\n",
      "(1604, 75, 75) 0.0210369138693 1.0 72180128\n",
      "(8424, 75, 75) 0.0 1.0 379080128\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load orignal data\n",
    "band_sub_data = np.fabs(np.subtract(band1_data, band2_data))\n",
    "band_sub_test = np.fabs(np.subtract(band1_test, band2_test))\n",
    "band_min_data = np.minimum(band1_data, band2_data)\n",
    "band_min_test = np.minimum(band1_test, band2_test)\n",
    "band_max_data = np.maximum(band1_data, band2_data)\n",
    "band_max_test = np.maximum(band1_test, band2_test)\n",
    "\n",
    "describe(band_sub_data)\n",
    "describe(band_sub_test)\n",
    "describe(band_min_data)\n",
    "describe(band_min_test)\n",
    "describe(band_max_data)\n",
    "describe(band_max_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75, 3) 0.0 1.0 216540144\n",
      "(8424, 75, 75, 3) 0.0 1.0 1137240144\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_data = np.concatenate([band1_data[:, :, :, np.newaxis],\n",
    "                         band2_data[:, :, :, np.newaxis],\n",
    "                         band_max_data[:, :, :, np.newaxis]], axis=-1)\n",
    "describe(x_data)\n",
    "# del band1_data\n",
    "# del band2_data\n",
    "# del band_avg_data\n",
    "gc.collect()\n",
    "\n",
    "x_test = np.concatenate([band1_test[:, :, :, np.newaxis],\n",
    "                         band2_test[:, :, :, np.newaxis],\n",
    "                         band_max_test[:, :, :, np.newaxis]], axis=-1)\n",
    "describe(x_test)\n",
    "# del band1_test\n",
    "# del band2_test\n",
    "# del band_avg_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1443, 75, 75, 3) 0.0 1.0 194805144\n",
      "(161, 75, 75, 3) 0.0 1.0 21735144\n",
      "(1443,) 0.0 45.9375 23112\n",
      "(161,) 0.0 45.2814 2600\n",
      "(1443,) 0 1 23112\n",
      "(161,) 0 1 2600\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, x_val, inc_angle_train, inc_angle_val, y_train, y_val = train_test_split(x_data, inc_angle_data, y_data, test_size=0.1, random_state=31)\n",
    "describe(x_train)\n",
    "describe(x_val)\n",
    "describe(inc_angle_train)\n",
    "describe(inc_angle_val)\n",
    "describe(y_train)\n",
    "describe(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    base_model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 20,582,593\n",
      "Trainable params: 20,582,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, run_name):\n",
    "    cwd = os.getcwd()\n",
    "    modelPath = os.path.join(cwd, 'model')\n",
    "    if not os.path.isdir(modelPath):\n",
    "        os.mkdir(modelPath)\n",
    "    weigthsFile = os.path.join(modelPath, run_name + '.h5')\n",
    "    model.save(weigthsFile)\n",
    "# saveModel(model, 'saveModel_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir:D:\\Kaggle\\statoil-iceberg-classifier-challenge\\log\\SC_Iceberg_Classifier_CNN_3channel_VGG19_FineTune_20180110_084730\n"
     ]
    }
   ],
   "source": [
    "def get_lr(x):\n",
    "    lr = round(1e-4 * 0.995 ** x, 6)\n",
    "    if lr < 1e-5:\n",
    "        lr = 1e-5\n",
    "    print(lr, end='  ')\n",
    "    return lr\n",
    "\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "annealer = LearningRateScheduler(get_lr)\n",
    "\n",
    "log_dir = os.path.join(log_path, run_name)\n",
    "print('log_dir:' + log_dir)\n",
    "tensorBoard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    zoom_range = 0.2,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001  Epoch 1/300\n",
      "100/100 [==============================] - 94s 940ms/step - loss: 0.7094 - acc: 0.5304 - val_loss: 0.6334 - val_acc: 0.6211\n",
      "0.0001  Epoch 2/300\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.6614 - acc: 0.5750 - val_loss: 0.6947 - val_acc: 0.5466\n",
      "9.9e-05  Epoch 3/300\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.6911 - acc: 0.5238 - val_loss: 0.6924 - val_acc: 0.5466\n",
      "9.9e-05  Epoch 4/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.7018 - acc: 0.4804 - val_loss: 0.6928 - val_acc: 0.5466\n",
      "9.8e-05  Epoch 5/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6650 - acc: 0.5562 - val_loss: 0.6043 - val_acc: 0.6460\n",
      "9.8e-05  Epoch 6/300\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.7217 - acc: 0.5725 - val_loss: 0.6799 - val_acc: 0.5466\n",
      "9.7e-05  Epoch 7/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6902 - acc: 0.5446 - val_loss: 0.6725 - val_acc: 0.5342\n",
      "9.7e-05  Epoch 8/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6627 - acc: 0.5925 - val_loss: 0.6238 - val_acc: 0.5963\n",
      "9.6e-05  Epoch 9/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6747 - acc: 0.5950 - val_loss: 0.6598 - val_acc: 0.5466\n",
      "9.6e-05  Epoch 10/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6207 - acc: 0.6225 - val_loss: 0.6692 - val_acc: 0.6273\n",
      "9.5e-05  Epoch 11/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6601 - acc: 0.5671 - val_loss: 0.6487 - val_acc: 0.6894\n",
      "9.5e-05  Epoch 12/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6295 - acc: 0.6404 - val_loss: 0.6143 - val_acc: 0.6708\n",
      "9.4e-05  Epoch 13/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6081 - acc: 0.6600 - val_loss: 0.5678 - val_acc: 0.6584\n",
      "9.4e-05  Epoch 14/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.5775 - acc: 0.6879 - val_loss: 0.5651 - val_acc: 0.7143\n",
      "9.3e-05  Epoch 15/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.7058 - acc: 0.5787 - val_loss: 0.6948 - val_acc: 0.4534\n",
      "9.3e-05  Epoch 16/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6937 - acc: 0.4863 - val_loss: 0.6942 - val_acc: 0.4534\n",
      "9.2e-05  Epoch 17/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6938 - acc: 0.4662 - val_loss: 0.6935 - val_acc: 0.4534\n",
      "9.2e-05  Epoch 18/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6934 - acc: 0.4642 - val_loss: 0.6932 - val_acc: 0.4534\n",
      "9.1e-05  Epoch 19/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6931 - acc: 0.5025 - val_loss: 0.6929 - val_acc: 0.5466\n",
      "9.1e-05  Epoch 20/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6931 - acc: 0.5296 - val_loss: 0.6928 - val_acc: 0.5466\n",
      "9e-05  Epoch 21/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6928 - acc: 0.5404 - val_loss: 0.6925 - val_acc: 0.5466\n",
      "9e-05  Epoch 22/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6930 - acc: 0.5187 - val_loss: 0.6924 - val_acc: 0.5466\n",
      "9e-05  Epoch 23/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6926 - acc: 0.5321 - val_loss: 0.6921 - val_acc: 0.5466\n",
      "8.9e-05  Epoch 24/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6925 - acc: 0.5288 - val_loss: 0.6919 - val_acc: 0.5466\n",
      "8.9e-05  Epoch 25/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6921 - acc: 0.5383 - val_loss: 0.6914 - val_acc: 0.5466\n",
      "8.8e-05  Epoch 26/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6935 - acc: 0.4975 - val_loss: 0.6917 - val_acc: 0.5466\n",
      "8.8e-05  Epoch 27/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6914 - acc: 0.5508 - val_loss: 0.6912 - val_acc: 0.5466\n",
      "8.7e-05  Epoch 28/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6918 - acc: 0.5325 - val_loss: 0.6909 - val_acc: 0.5466\n",
      "8.7e-05  Epoch 29/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6926 - acc: 0.5192 - val_loss: 0.6908 - val_acc: 0.5466\n",
      "8.6e-05  Epoch 30/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6915 - acc: 0.5363 - val_loss: 0.6906 - val_acc: 0.5466\n",
      "8.6e-05  Epoch 31/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6924 - acc: 0.5242 - val_loss: 0.6905 - val_acc: 0.5466\n",
      "8.6e-05  Epoch 32/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6918 - acc: 0.5283 - val_loss: 0.6904 - val_acc: 0.5466\n",
      "8.5e-05  Epoch 33/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6911 - acc: 0.5387 - val_loss: 0.6902 - val_acc: 0.5466\n",
      "8.5e-05  Epoch 34/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6904 - acc: 0.5425 - val_loss: 0.6900 - val_acc: 0.5466\n",
      "8.4e-05  Epoch 35/300\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.6946 - acc: 0.4929 - val_loss: 0.6902 - val_acc: 0.5466\n",
      "8.4e-05  Epoch 36/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6895 - acc: 0.5537 - val_loss: 0.6899 - val_acc: 0.5466\n",
      "8.3e-05  Epoch 37/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6920 - acc: 0.5225 - val_loss: 0.6900 - val_acc: 0.5466\n",
      "8.3e-05  Epoch 38/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6926 - acc: 0.5179 - val_loss: 0.6900 - val_acc: 0.5466\n",
      "8.3e-05  Epoch 39/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6923 - acc: 0.5221 - val_loss: 0.6900 - val_acc: 0.5466\n",
      "8.2e-05  Epoch 40/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6906 - acc: 0.5413 - val_loss: 0.6898 - val_acc: 0.5466\n",
      "8.2e-05  Epoch 41/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6917 - acc: 0.5262 - val_loss: 0.6898 - val_acc: 0.5466\n",
      "8.1e-05  Epoch 42/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6911 - acc: 0.5309 - val_loss: 0.6897 - val_acc: 0.5466\n",
      "8.1e-05  Epoch 43/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6913 - acc: 0.5312 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "8.1e-05  Epoch 44/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6929 - acc: 0.5167 - val_loss: 0.6898 - val_acc: 0.5466\n",
      "8e-05  Epoch 45/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6920 - acc: 0.5288 - val_loss: 0.6898 - val_acc: 0.5466\n",
      "8e-05  Epoch 46/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6922 - acc: 0.5225 - val_loss: 0.6898 - val_acc: 0.5466\n",
      "7.9e-05  Epoch 47/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6900 - acc: 0.5437 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "7.9e-05  Epoch 48/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6923 - acc: 0.5212 - val_loss: 0.6897 - val_acc: 0.5466\n",
      "7.9e-05  Epoch 49/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6905 - acc: 0.5412 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "7.8e-05  Epoch 50/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6934 - acc: 0.5138 - val_loss: 0.6897 - val_acc: 0.5466\n",
      "7.8e-05  Epoch 51/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6917 - acc: 0.5263 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "7.7e-05  Epoch 52/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6915 - acc: 0.5296 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "7.7e-05  Epoch 53/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6895 - acc: 0.5475 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "7.7e-05  Epoch 54/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6942 - acc: 0.5062 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "7.6e-05  Epoch 55/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6900 - acc: 0.5413 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "7.6e-05  Epoch 56/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6918 - acc: 0.5250 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "7.6e-05  Epoch 57/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6910 - acc: 0.5329 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "7.5e-05  Epoch 58/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6916 - acc: 0.5300 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "7.5e-05  Epoch 59/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6918 - acc: 0.5283 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "7.4e-05  Epoch 60/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6918 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "7.4e-05  Epoch 61/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6891 - acc: 0.5500 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "7.4e-05  Epoch 62/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6917 - acc: 0.5254 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "7.3e-05  Epoch 63/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6916 - acc: 0.5275 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "7.3e-05  Epoch 64/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6913 - acc: 0.5312 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "7.3e-05  Epoch 65/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6926 - acc: 0.5196 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "7.2e-05  Epoch 66/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6937 - acc: 0.5125 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "7.2e-05  Epoch 67/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6897 - acc: 0.5429 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "7.1e-05  Epoch 68/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6935 - acc: 0.5129 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "7.1e-05  Epoch 69/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6902 - acc: 0.5425 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "7.1e-05  Epoch 70/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6901 - acc: 0.5408 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "7e-05  Epoch 71/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6944 - acc: 0.5012 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "7e-05  Epoch 72/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6909 - acc: 0.5358 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "7e-05  Epoch 73/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6901 - acc: 0.5392 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "6.9e-05  Epoch 74/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6925 - acc: 0.5200 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "6.9e-05  Epoch 75/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6887 - acc: 0.5537 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "6.9e-05  Epoch 76/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6933 - acc: 0.5142 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.8e-05  Epoch 77/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6928 - acc: 0.5175 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "6.8e-05  Epoch 78/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6902 - acc: 0.5404 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.8e-05  Epoch 79/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6905 - acc: 0.5375 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "6.7e-05  Epoch 80/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6920 - acc: 0.5250 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "6.7e-05  Epoch 81/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6910 - acc: 0.5350 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "6.7e-05  Epoch 82/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6929 - acc: 0.5192 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.6e-05  Epoch 83/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6909 - acc: 0.5333 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.6e-05  Epoch 84/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6919 - acc: 0.5271 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.6e-05  Epoch 85/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6919 - acc: 0.5238 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.5e-05  Epoch 86/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6913 - acc: 0.5300 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.5e-05  Epoch 87/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6911 - acc: 0.5329 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.5e-05  Epoch 88/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6925 - acc: 0.5212 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.4e-05  Epoch 89/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6922 - acc: 0.5242 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.4e-05  Epoch 90/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6910 - acc: 0.5350 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.4e-05  Epoch 91/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6917 - acc: 0.5263 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.3e-05  Epoch 92/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6906 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.3e-05  Epoch 93/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6932 - acc: 0.5146 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "6.3e-05  Epoch 94/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6889 - acc: 0.5500 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "6.2e-05  Epoch 95/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6930 - acc: 0.5175 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.2e-05  Epoch 96/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6926 - acc: 0.5208 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "6.2e-05  Epoch 97/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6879 - acc: 0.5567 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "6.1e-05  Epoch 98/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6936 - acc: 0.5121 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "6.1e-05  Epoch 99/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6915 - acc: 0.5325 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.1e-05  Epoch 100/300\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.6917 - acc: 0.5288 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "6.1e-05  Epoch 101/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6928 - acc: 0.5204 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "6e-05  Epoch 102/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6927 - acc: 0.5167 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "6e-05  Epoch 103/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6918 - acc: 0.5262 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "6e-05  Epoch 104/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6936 - acc: 0.5088 - val_loss: 0.6897 - val_acc: 0.5466\n",
      "5.9e-05  Epoch 105/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6883 - acc: 0.5575 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.9e-05  Epoch 106/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6905 - acc: 0.5392 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.9e-05  Epoch 107/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6934 - acc: 0.5088 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.8e-05  Epoch 108/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6920 - acc: 0.5258 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.8e-05  Epoch 109/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6900 - acc: 0.5438 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "5.8e-05  Epoch 110/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6908 - acc: 0.5354 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "5.8e-05  Epoch 111/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6922 - acc: 0.5238 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "5.7e-05  Epoch 112/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6912 - acc: 0.5288 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.7e-05  Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 87s 875ms/step - loss: 0.6913 - acc: 0.5304 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.7e-05  Epoch 114/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6921 - acc: 0.5238 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.6e-05  Epoch 115/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6911 - acc: 0.5333 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.6e-05  Epoch 116/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6931 - acc: 0.5162 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.6e-05  Epoch 117/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6899 - acc: 0.5437 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.6e-05  Epoch 118/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6921 - acc: 0.5238 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.5e-05  Epoch 119/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6924 - acc: 0.5179 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.5e-05  Epoch 120/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6904 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.5e-05  Epoch 121/300\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.6916 - acc: 0.5275 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.5e-05  Epoch 122/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6905 - acc: 0.5400 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.4e-05  Epoch 123/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6926 - acc: 0.5200 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.4e-05  Epoch 124/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6921 - acc: 0.5258 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.4e-05  Epoch 125/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6915 - acc: 0.5300 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.3e-05  Epoch 126/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6891 - acc: 0.5467 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "5.3e-05  Epoch 127/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6917 - acc: 0.5262 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "5.3e-05  Epoch 128/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6923 - acc: 0.5229 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.3e-05  Epoch 129/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6925 - acc: 0.5233 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.2e-05  Epoch 130/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6920 - acc: 0.5275 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.2e-05  Epoch 131/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6923 - acc: 0.5246 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.2e-05  Epoch 132/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6915 - acc: 0.5300 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.2e-05  Epoch 133/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6925 - acc: 0.5200 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5.1e-05  Epoch 134/300\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.6905 - acc: 0.5392 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.1e-05  Epoch 135/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6923 - acc: 0.5212 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5.1e-05  Epoch 136/300\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.6886 - acc: 0.5550 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "5.1e-05  Epoch 137/300\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.6939 - acc: 0.5038 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "5e-05  Epoch 138/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6897 - acc: 0.5413 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "5e-05  Epoch 139/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6920 - acc: 0.5233 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5e-05  Epoch 140/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6908 - acc: 0.5337 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "5e-05  Epoch 141/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6928 - acc: 0.5179 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.9e-05  Epoch 142/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6904 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.9e-05  Epoch 143/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6919 - acc: 0.5246 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.9e-05  Epoch 144/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6921 - acc: 0.5225 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.9e-05  Epoch 145/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6901 - acc: 0.5450 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.8e-05  Epoch 146/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6925 - acc: 0.5250 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.8e-05  Epoch 147/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6918 - acc: 0.5276 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.8e-05  Epoch 148/300\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.6934 - acc: 0.5125 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.8e-05  Epoch 149/300\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6904 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.7e-05  Epoch 150/300\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.6916 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.7e-05  Epoch 151/300\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.6910 - acc: 0.5304 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.7e-05  Epoch 152/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6913 - acc: 0.5300 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.7e-05  Epoch 153/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6915 - acc: 0.5296 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.6e-05  Epoch 154/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6912 - acc: 0.5337 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.6e-05  Epoch 155/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6943 - acc: 0.5037 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.6e-05  Epoch 156/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6898 - acc: 0.5488 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.6e-05  Epoch 157/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6908 - acc: 0.5325 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.6e-05  Epoch 158/300\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.6951 - acc: 0.4963 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "4.5e-05  Epoch 159/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6902 - acc: 0.5400 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.5e-05  Epoch 160/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6900 - acc: 0.5429 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.5e-05  Epoch 161/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6905 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.5e-05  Epoch 162/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6928 - acc: 0.5142 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.4e-05  Epoch 163/300\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.6906 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.4e-05  Epoch 164/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6934 - acc: 0.5087 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.4e-05  Epoch 165/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6903 - acc: 0.5400 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.4e-05  Epoch 166/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6909 - acc: 0.5358 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.4e-05  Epoch 167/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6935 - acc: 0.5108 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.3e-05  Epoch 168/300\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.6915 - acc: 0.5275 - val_loss: 0.6896 - val_acc: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3e-05  Epoch 169/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6896 - acc: 0.5467 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.3e-05  Epoch 170/300\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.6919 - acc: 0.5238 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.3e-05  Epoch 171/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6905 - acc: 0.5392 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.2e-05  Epoch 172/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6920 - acc: 0.5262 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.2e-05  Epoch 173/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6928 - acc: 0.5183 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.2e-05  Epoch 174/300\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.6904 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.2e-05  Epoch 175/300\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.6909 - acc: 0.5337 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.2e-05  Epoch 176/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6908 - acc: 0.5396 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.1e-05  Epoch 177/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6945 - acc: 0.5037 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.1e-05  Epoch 178/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6925 - acc: 0.5187 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.1e-05  Epoch 179/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6885 - acc: 0.5579 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4.1e-05  Epoch 180/300\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.6929 - acc: 0.5187 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4.1e-05  Epoch 181/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6916 - acc: 0.5296 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4e-05  Epoch 182/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6917 - acc: 0.5283 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4e-05  Epoch 183/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6909 - acc: 0.5337 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4e-05  Epoch 184/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6908 - acc: 0.5363 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "4e-05  Epoch 185/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6929 - acc: 0.5154 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "4e-05  Epoch 186/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6919 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.9e-05  Epoch 187/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6939 - acc: 0.5050 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "3.9e-05  Epoch 188/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6881 - acc: 0.5587 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.9e-05  Epoch 189/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6916 - acc: 0.5262 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.9e-05  Epoch 190/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6916 - acc: 0.5258 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.9e-05  Epoch 191/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6914 - acc: 0.5287 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.8e-05  Epoch 192/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6915 - acc: 0.5304 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.8e-05  Epoch 193/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6922 - acc: 0.5200 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.8e-05  Epoch 194/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6909 - acc: 0.5362 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.8e-05  Epoch 195/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6923 - acc: 0.5250 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.8e-05  Epoch 196/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6906 - acc: 0.5367 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.7e-05  Epoch 197/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6900 - acc: 0.5413 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.7e-05  Epoch 198/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6916 - acc: 0.5275 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "3.7e-05  Epoch 199/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6929 - acc: 0.5229 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.7e-05  Epoch 200/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6919 - acc: 0.5258 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.7e-05  Epoch 201/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6917 - acc: 0.5288 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.7e-05  Epoch 202/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6937 - acc: 0.5112 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "3.6e-05  Epoch 203/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6888 - acc: 0.5525 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.6e-05  Epoch 204/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6914 - acc: 0.5313 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.6e-05  Epoch 205/300\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.6924 - acc: 0.5225 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.6e-05  Epoch 206/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6919 - acc: 0.5233 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.6e-05  Epoch 207/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6932 - acc: 0.5138 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "3.5e-05  Epoch 208/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6898 - acc: 0.5467 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.5e-05  Epoch 209/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6926 - acc: 0.5175 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.5e-05  Epoch 210/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6895 - acc: 0.5467 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.5e-05  Epoch 211/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6929 - acc: 0.5183 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.5e-05  Epoch 212/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6931 - acc: 0.5150 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.5e-05  Epoch 213/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6898 - acc: 0.5475 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.4e-05  Epoch 214/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6905 - acc: 0.5421 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.4e-05  Epoch 215/300\n",
      "100/100 [==============================] - 89s 894ms/step - loss: 0.6908 - acc: 0.5367 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.4e-05  Epoch 216/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6927 - acc: 0.5200 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.4e-05  Epoch 217/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6933 - acc: 0.5117 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.4e-05  Epoch 218/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6902 - acc: 0.5425 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.4e-05  Epoch 219/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6917 - acc: 0.5250 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.3e-05  Epoch 220/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6918 - acc: 0.5254 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.3e-05  Epoch 221/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6905 - acc: 0.5392 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.3e-05  Epoch 222/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6905 - acc: 0.5363 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.3e-05  Epoch 223/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6888 - acc: 0.5475 - val_loss: 0.6893 - val_acc: 0.5466\n",
      "3.3e-05  Epoch 224/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6922 - acc: 0.5225 - val_loss: 0.6893 - val_acc: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3e-05  Epoch 225/300\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.6964 - acc: 0.4862 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.2e-05  Epoch 226/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6906 - acc: 0.5400 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.2e-05  Epoch 227/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6924 - acc: 0.5217 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.2e-05  Epoch 228/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6910 - acc: 0.5337 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.2e-05  Epoch 229/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6932 - acc: 0.5138 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "3.2e-05  Epoch 230/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6893 - acc: 0.5479 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.2e-05  Epoch 231/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6947 - acc: 0.4975 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "3.1e-05  Epoch 232/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6886 - acc: 0.5588 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.1e-05  Epoch 233/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6916 - acc: 0.5267 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.1e-05  Epoch 234/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6916 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3.1e-05  Epoch 235/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6900 - acc: 0.5438 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.1e-05  Epoch 236/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6923 - acc: 0.5233 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3.1e-05  Epoch 237/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6913 - acc: 0.5325 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3e-05  Epoch 238/300\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.6921 - acc: 0.5233 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "3e-05  Epoch 239/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6919 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3e-05  Epoch 240/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6918 - acc: 0.5275 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3e-05  Epoch 241/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6922 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3e-05  Epoch 242/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6905 - acc: 0.5371 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3e-05  Epoch 243/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6924 - acc: 0.5200 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "3e-05  Epoch 244/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6914 - acc: 0.5300 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.9e-05  Epoch 245/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6917 - acc: 0.5263 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.9e-05  Epoch 246/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6919 - acc: 0.5233 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.9e-05  Epoch 247/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6900 - acc: 0.5400 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.9e-05  Epoch 248/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6926 - acc: 0.5208 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.9e-05  Epoch 249/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6915 - acc: 0.5275 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.9e-05  Epoch 250/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6917 - acc: 0.5275 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.9e-05  Epoch 251/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6926 - acc: 0.5200 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "2.8e-05  Epoch 252/300\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.6882 - acc: 0.5613 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.8e-05  Epoch 253/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6938 - acc: 0.5112 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.8e-05  Epoch 254/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6910 - acc: 0.5337 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.8e-05  Epoch 255/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6920 - acc: 0.5217 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.8e-05  Epoch 256/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6924 - acc: 0.5225 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.8e-05  Epoch 257/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6905 - acc: 0.5354 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.8e-05  Epoch 258/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6910 - acc: 0.5363 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.7e-05  Epoch 259/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6932 - acc: 0.5121 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.7e-05  Epoch 260/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6921 - acc: 0.5225 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "2.7e-05  Epoch 261/300\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.6890 - acc: 0.5524 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.7e-05  Epoch 262/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6919 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.7e-05  Epoch 263/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6918 - acc: 0.5250 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.7e-05  Epoch 264/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6912 - acc: 0.5325 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.7e-05  Epoch 265/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6914 - acc: 0.5300 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 266/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6924 - acc: 0.5221 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 267/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6929 - acc: 0.5129 - val_loss: 0.6896 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 268/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6900 - acc: 0.5450 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 269/300\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.6905 - acc: 0.5367 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 270/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6912 - acc: 0.5312 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 271/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6918 - acc: 0.5250 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 272/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6915 - acc: 0.5275 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.6e-05  Epoch 273/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6918 - acc: 0.5242 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.5e-05  Epoch 274/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6940 - acc: 0.5088 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.5e-05  Epoch 275/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6898 - acc: 0.5438 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.5e-05  Epoch 276/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6897 - acc: 0.5429 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.5e-05  Epoch 277/300\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.6938 - acc: 0.5125 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.5e-05  Epoch 278/300\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.6899 - acc: 0.5408 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.5e-05  Epoch 279/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6930 - acc: 0.5175 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.5e-05  Epoch 280/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6910 - acc: 0.5317 - val_loss: 0.6895 - val_acc: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5e-05  Epoch 281/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6907 - acc: 0.5350 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 282/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6924 - acc: 0.5158 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 283/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6905 - acc: 0.5413 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 284/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6913 - acc: 0.5312 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 285/300\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.6923 - acc: 0.5229 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 286/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6915 - acc: 0.5300 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 287/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6911 - acc: 0.5338 - val_loss: 0.6894 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 288/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6925 - acc: 0.5225 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.4e-05  Epoch 289/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6910 - acc: 0.5300 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 290/300\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.6932 - acc: 0.5154 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 291/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6925 - acc: 0.5225 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 292/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6909 - acc: 0.5337 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 293/300\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.6912 - acc: 0.5342 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 294/300\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.6918 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 295/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6907 - acc: 0.5362 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 296/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6919 - acc: 0.5250 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 297/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6916 - acc: 0.5279 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.3e-05  Epoch 298/300\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.6924 - acc: 0.5208 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.2e-05  Epoch 299/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6902 - acc: 0.5413 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "2.2e-05  Epoch 300/300\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.6913 - acc: 0.5312 - val_loss: 0.6895 - val_acc: 0.5466\n",
      "Wall time: 7h 17min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 8\n",
    "# steps_per_epoch = 1 * len(x_train) / batch_size\n",
    "hist = model.fit_generator(\n",
    "    datagen.flow(x_train, y_train, batch_size=batch_size, shuffle=True, seed=2019),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=300, #1 for ETA, 0 for silent\n",
    "    verbose=1,\n",
    "    max_queue_size=128,\n",
    "    callbacks=[annealer],\n",
    "    workers=32,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = model.fit(x_train, y_train, \n",
    "#                  batch_size = 128, \n",
    "#                  verbose= 1,\n",
    "#                  epochs = 100, #1 for ETA, 0 for silent\n",
    "#                  validation_data=(x_val, y_val),\n",
    "#                  callbacks=[annealer, tensorBoard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 4s 28ms/step\n",
      "Final loss: 0.6895, final accuracy: 0.5466\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_acc = model.evaluate(x_val, y_val, verbose=1)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val log_loss: 0.6894669440222083\n",
      "Val log_loss: 0.6894669440222083\n"
     ]
    }
   ],
   "source": [
    "val_prob1 = model.predict(x_val)\n",
    "\n",
    "# print('Val log_loss: {}'.format(log_loss(y_val, val_prob1)))\n",
    "val_prob1_limit = np.clip(val_prob1, 0.00005, 0.99995)\n",
    "loss = log_loss(y_val, val_prob1_limit)\n",
    "print('Val log_loss: {}'.format(loss))\n",
    "\n",
    "val_prob1_limit = np.clip(val_prob1_limit, 0.05, 0.95)\n",
    "loss = log_loss(y_val, val_prob1_limit)\n",
    "print('Val log_loss: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC_Iceberg_Classifier_CNN_3channel_VGG19_FineTune_20180110_084730_6894\n"
     ]
    }
   ],
   "source": [
    "final_acc_str = '{0:0>4}'.format(int(loss*10000))\n",
    "run_name_acc = project_name + '_' + step_name + '_' + time_str + '_' + final_acc_str\n",
    "print(run_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['acc', 'loss', 'val_acc', 'val_loss', 'epoch'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "histories = pd.DataFrame(hist.history)\n",
    "histories['epoch'] = hist.epoch\n",
    "print(histories.columns)\n",
    "histories_file = os.path.join(model_path, run_name_acc + '.csv')\n",
    "histories.to_csv(histories_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwkTfMAwjRJjoG4ZhhIj/DzdH5HvJQUIPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2015b812908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFmMe6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2015b7ba668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(histories['loss'], color='b')\n",
    "plt.plot(histories['val_loss'], color='r')\n",
    "plt.show()\n",
    "plt.plot(histories['acc'], color='b')\n",
    "plt.plot(histories['val_acc'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model, run_name_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Kaggle\\statoil-iceberg-classifier-challenge\\output\\SC_Iceberg_Classifier_CNN_3channel_VGG19_FineTune_20180110_084730_6894.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "pred_file = os.path.join(output_path, run_name_acc + '.csv')\n",
    "print(pred_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 1)\n",
      "[[ 0.47162208]\n",
      " [ 0.47162208]]\n",
      "(8424, 1)\n",
      "[[ 0.47162208]\n",
      " [ 0.47162208]]\n"
     ]
    }
   ],
   "source": [
    "test_prob = model.predict(x_test)\n",
    "print(test_prob.shape)\n",
    "print(test_prob[0:2])\n",
    "test_prob = np.clip(test_prob, 0.05, 0.95)\n",
    "print(test_prob.shape)\n",
    "print(test_prob[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  is_iceberg\n",
      "0  5941774d    0.471622\n",
      "1  4023181e    0.471622\n",
      "(8424, 2)\n"
     ]
    }
   ],
   "source": [
    "sample_submission['is_iceberg'] = test_prob\n",
    "print(sample_submission[0:2])\n",
    "print(sample_submission.shape)\n",
    "sample_submission.to_csv(pred_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 26410.34 s\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print('time cost: %.2f s' % (t1-t0))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC_Iceberg_Classifier_CNN_3channel_VGG19_FineTune_20180110_084730_6894\n"
     ]
    }
   ],
   "source": [
    "print(run_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
