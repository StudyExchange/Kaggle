{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict-Image-Similar-FCNN-Binary[OPT]\n",
    "Optimization and acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: 4-Predict[OPT]_Google-LandMark-Rec2019_3-Image-Similar-FCNN-Binary_20190511-003528_8193_20190511-015345\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'Google-LandMark-Rec2019'\n",
    "model_name = 'Google-LandMark-Rec2019_3-Image-Similar-FCNN-Binary_20190511-003528_8193'\n",
    "step_name = '4-Predict[OPT]_%s' % model_name\n",
    "time_str = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "run_name = step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_amount:  4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cpu_amount = multiprocessing.cpu_count()\n",
    "print('cpu_amount: ', cpu_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import math\n",
    "import shutil\n",
    "import zipfile\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.applications import *\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "\n",
    "org_train_folder = os.path.join(input_folder, 'org_train')\n",
    "org_test_folder = os.path.join(input_folder, 'org_test')\n",
    "train_folder = os.path.join(input_folder, 'data_train')\n",
    "val_folder = os.path.join(input_folder, 'data_val')\n",
    "test_folder = os.path.join(input_folder, 'data_test')\n",
    "test_sub_folder = os.path.join(test_folder, 'test')\n",
    "\n",
    "vgg16_feature_file = os.path.join(feature_folder, 'feature_wrapper_171023.h5')\n",
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_folder = os.path.join(input_folder, 'sample_submission.csv')\n",
    "print(vgg16_feature_file)\n",
    "print(train_csv_file)\n",
    "print(test_csv_file)\n",
    "print(sample_submission_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dump(data, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def pickle_load(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "a = list(range(10))\n",
    "print(a)\n",
    "demo_file = os.path.join(os.getcwd(), 'temp', 'pickle_demo.pkl')\n",
    "print(demo_file)\n",
    "pickle_dump(a, demo_file)\n",
    "new_a = pickle_load(demo_file)\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_whole_classes_label(class_indices_data, item_classes_data):\n",
    "    indices2class = {}\n",
    "    for i, key in enumerate(class_indices_data.keys()):\n",
    "#         print(key, class_indices_data[key])\n",
    "        indices2class[class_indices_data[key]] = key\n",
    "#         if i >= 5:\n",
    "#             break\n",
    "#     for i, key in enumerate(class_indices_data.keys()):\n",
    "#         print(key, class_indices_data[key], '-->', class_indices_data[key], indices2class[class_indices_data[key]])\n",
    "#         if i >= 10:\n",
    "#             break\n",
    "    print(item_classes_data.shape)\n",
    "    whole_classes_data = np.zeros(item_classes_data.shape)\n",
    "    for i, class_index in enumerate(item_classes_data):\n",
    "        whole_classes_data[i] = indices2class[class_index]\n",
    "#         print(i, class_index, '-->', indices2class[class_index])\n",
    "#         if i >= 10:\n",
    "#             break\n",
    "    return whole_classes_data\n",
    "# get_whole_classes_label(class_indices_train, item_classes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./feature -hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(data_set_name, model_name, date_str, feature_amount):\n",
    "#     data_set_name = 'train'\n",
    "#     model_name = 'VGG19'\n",
    "#     date_str = '171023'\n",
    "    x_data_arr = []\n",
    "    classes_data_arr = []\n",
    "    index_data_arr = []\n",
    "    for i in range(feature_amount):\n",
    "        folder_name = 'data_%s_%02d' % (data_set_name, i)\n",
    "        class_indices_file = os.path.join(cwd, 'feature', 'feature_{0}_{1}_{2}_class_indices.pkl'.format(model_name, folder_name, date_str))\n",
    "        h5py_file_name = os.path.join(cwd, 'feature', 'feature_{0}_{1}_{2}.h5'.format(model_name, folder_name, date_str))\n",
    "        if not os.path.exists(class_indices_file):\n",
    "            print('File not exists', class_indices_file)\n",
    "            continue\n",
    "        if not os.path.exists(h5py_file_name):\n",
    "            print('File not exists', h5py_file_name)\n",
    "            continue\n",
    "        print(class_indices_file)\n",
    "        print(h5py_file_name)\n",
    "        class_indices_data = pickle_load(class_indices_file)\n",
    "        print(len(class_indices_data))\n",
    "\n",
    "        with h5py.File(h5py_file_name, 'r') as h:\n",
    "            item_x_data = np.array(h['x_data_%s_%02d' % (data_set_name, i)])\n",
    "            item_classes_data = np.array(h['classes_data_%s_%02d' % (data_set_name, i)])\n",
    "            item_index_data = np.array(h['index_data_%s_%02d' % (data_set_name, i)])\n",
    "        print(item_x_data.shape)\n",
    "        x_data_arr.append(item_x_data)\n",
    "        item_y_data = get_whole_classes_label(class_indices_data, item_classes_data)\n",
    "        print(item_y_data.shape, item_y_data[:10])\n",
    "        classes_data_arr.append(item_y_data)\n",
    "        index_data_arr.append(item_index_data)\n",
    "    x_data = np.concatenate(x_data_arr, axis=0)\n",
    "    y_data = np.concatenate(classes_data_arr, axis=0)\n",
    "    idx_data = np.concatenate(index_data_arr, axis=0)\n",
    "    print('*' * 60)\n",
    "    print(x_data.shape)\n",
    "    print(y_data.shape)\n",
    "    print(idx_data.shape)\n",
    "    return x_data, y_data, idx_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, idx_train = load_feature('train', 'VGG19', '171023', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val, idx_val = load_feature('val', 'VGG19', '171023', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(model_folder, '%s.h5' % model_name)\n",
    "print(model_file)\n",
    "model = load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "item_main_x = np.array([x_train[idx]]*x_train.shape[0])\n",
    "item_x_train = {\n",
    "    'main_input': item_main_x,\n",
    "    'library_input': x_train\n",
    "}\n",
    "y_proba = model.predict(item_x_train, batch_size=1024)\n",
    "print(y_train[idx], np.argmax(y_proba), '-->', y_train[np.argmax(y_proba)], [y_train[item[0]] for item in np.argsort(y_proba, axis=0)[-10:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "amount = 10\n",
    "top1_count = 0\n",
    "topn_count = 0\n",
    "for idx in range(amount):\n",
    "    item_main_x = np.array([x_train[idx]]*x_train.shape[0])\n",
    "    item_x_train = {\n",
    "        'main_input': item_main_x,\n",
    "        'library_input': x_train\n",
    "    }\n",
    "    y_proba = model.predict(item_x_train, batch_size=1024)\n",
    "    item_y_train = y_train[idx]\n",
    "    top1_pred = y_train[np.argmax(y_proba)]\n",
    "    topn_pred_arr = [y_train[item[0]] for item in np.argsort(y_proba, axis=0)[-topn:]]\n",
    "    if item_y_train == top1_pred:\n",
    "        top1_count += 1\n",
    "    if item_y_train in topn_pred_arr:\n",
    "        topn_count += 1\n",
    "    print(item_y_train, '-->', top1_pred, topn_pred_arr)\n",
    "\n",
    "print('*' * 80)\n",
    "print(top1_count, '%.2f' % (top1_count/amount))\n",
    "print(topn_count, '%.2f' % (topn_count/amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_topn(y_proba, topn, y_data):\n",
    "    y_proba_argsorted = [(y_data[item[0]], y_proba[item[0]][0]) for item in np.argsort(y_proba, axis=0)[-topn:]]\n",
    "    class_score_dict = {}\n",
    "    for item_class, item_proba in y_proba_argsorted:\n",
    "        if item_class in class_score_dict:\n",
    "            class_score_dict[item_class] += item_proba\n",
    "        else:\n",
    "            class_score_dict[item_class] = item_proba\n",
    "    class_score_arr = list(class_score_dict.items())\n",
    "    class_score_arr = sorted(class_score_arr, key=lambda x: x[1])\n",
    "    topn_pred_arr = [item[0] for item in class_score_arr]\n",
    "    class_score_arr = [round(item[1], 2) for item in class_score_arr]\n",
    "    return topn_pred_arr, class_score_arr\n",
    "\n",
    "y_proba = np.array([[0.1], [0.2], [0.3], [0.2], [0.2], [0.2]])\n",
    "topn = 6\n",
    "y_data = np.array([0, 0, 1, 1, 2, 2])\n",
    "print(y_proba, type(y_proba))\n",
    "print(topn)\n",
    "print(y_data, type(y_data))\n",
    "get_weight_topn(y_proba, topn, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "amount = 10\n",
    "top1_count = 0\n",
    "topn_1_count = 0\n",
    "topn_count = 0\n",
    "for idx in range(amount):\n",
    "    item_main_x = np.array([x_train[idx]]*x_train.shape[0])\n",
    "    item_x_train = {\n",
    "        'main_input': item_main_x,\n",
    "        'library_input': x_train\n",
    "    }\n",
    "    y_proba = model.predict(item_x_train, batch_size=1024)\n",
    "    item_y_train = y_train[idx]\n",
    "    top1_pred = y_train[np.argmax(y_proba)]\n",
    "#     topn_pred_arr = [y_train[item[0]] for item in np.argsort(y_proba, axis=0)[-topn:]]\n",
    "    topn_pred_arr, class_score_arr = get_weight_topn(y_proba, topn, y_train)\n",
    "    topn_1_pred = topn_pred_arr[-1]\n",
    "    if item_y_train == top1_pred:\n",
    "        top1_count += 1\n",
    "    if item_y_train == topn_1_pred:\n",
    "        topn_1_count += 1\n",
    "    if item_y_train in topn_pred_arr:\n",
    "        topn_count += 1\n",
    "    print(item_y_train, '-->', top1_pred, '\\t', topn_1_pred, topn_pred_arr, class_score_arr)\n",
    "\n",
    "print('*' * 80)\n",
    "print(top1_count, '%.2f' % (top1_count/amount))\n",
    "print(topn_1_count, '%.2f' % (topn_1_count/amount))\n",
    "print(topn_count, '%.2f' % (topn_count/amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encupsolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_pred(idx, main_x, libary_x, topn, batch_size=1024):\n",
    "    item_main_x = np.array([main_x[idx]]*libary_x.shape[0])\n",
    "    item_x = {\n",
    "        'main_input': item_main_x,\n",
    "        'library_input': libary_x\n",
    "    }\n",
    "    y_proba = model.predict(item_x, batch_size=batch_size)\n",
    "    top1_pred = y_train[np.argmax(y_proba)]\n",
    "    topn_pred_arr = [y_train[item[0]] for item in np.argsort(y_proba, axis=0)[-topn:]]\n",
    "    weighted_topn_pred_arr, class_score_arr = get_weight_topn(y_proba, topn, y_train)\n",
    "    weighted_top1_pred = weighted_topn_pred_arr[-1]\n",
    "    return top1_pred, topn_pred_arr, weighted_top1_pred, weighted_topn_pred_arr, class_score_arr\n",
    "\n",
    "idx = 0\n",
    "topn = 10\n",
    "# train\n",
    "print(y_train[idx], get_single_pred(idx, x_train, x_train, topn, batch_size=1024))\n",
    "# val\n",
    "print(y_val[idx], get_single_pred(idx, x_val, x_train, topn, batch_size=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "amount = 20\n",
    "top1_count = 0\n",
    "topn_1_count = 0\n",
    "topn_count = 0\n",
    "for idx in range(amount):\n",
    "    top1_pred, topn_pred_arr, weighted_top1_pred, weighted_topn_pred_arr, class_score_arr = get_single_pred(idx, x_train, x_train, topn, batch_size=1024)\n",
    "    \n",
    "    item_y_data = y_train[idx]\n",
    "    if item_y_data == top1_pred:\n",
    "        top1_count += 1\n",
    "    if item_y_data == weighted_top1_pred:\n",
    "        topn_1_count += 1\n",
    "    if item_y_data in topn_pred_arr:\n",
    "        topn_count += 1\n",
    "    print(item_y_data, '-->', top1_pred, weighted_top1_pred, '\\t', weighted_topn_pred_arr, class_score_arr)\n",
    "\n",
    "print('*' * 80)\n",
    "print(top1_count, '%.2f' % (top1_count/amount))\n",
    "print(topn_1_count, '%.2f' % (topn_1_count/amount))\n",
    "print(topn_count, '%.2f' % (topn_count/amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 10\n",
    "amount = 20\n",
    "top1_count = 0\n",
    "topn_1_count = 0\n",
    "topn_count = 0\n",
    "for idx in range(amount):\n",
    "    top1_pred, topn_pred_arr, weighted_top1_pred, weighted_topn_pred_arr, class_score_arr = get_single_pred(idx, x_val, x_train, topn, batch_size=1024)\n",
    "    \n",
    "    item_y_data = y_val[idx]\n",
    "    if item_y_data == top1_pred:\n",
    "        top1_count += 1\n",
    "    if item_y_data == weighted_top1_pred:\n",
    "        topn_1_count += 1\n",
    "    if item_y_data in topn_pred_arr:\n",
    "        topn_count += 1\n",
    "    print(item_y_data, '-->', top1_pred, weighted_top1_pred, '\\t', weighted_topn_pred_arr, class_score_arr)\n",
    "\n",
    "print('*' * 80)\n",
    "print(top1_count, '%.2f' % (top1_count/amount))\n",
    "print(topn_1_count, '%.2f' % (topn_1_count/amount))\n",
    "print(topn_count, '%.2f' % (topn_count/amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc10000 = int(topn_count/amount*10000)\n",
    "# acc10000 = 10000 # for test\n",
    "# print(acc10000)\n",
    "if acc10000 == 10000:\n",
    "    run_name_acc = '%s_%05d' % (run_name, acc10000)\n",
    "else:\n",
    "    run_name_acc = '%s_%04d' % (run_name, acc10000)\n",
    "print(run_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time elapsed: %.1fs' % (time.time() - t0))\n",
    "print(run_name_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
