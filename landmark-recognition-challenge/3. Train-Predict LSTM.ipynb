{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train-Predict LSTM\n",
    "\n",
    "## Result:\n",
    "- Kaggle score: \n",
    "\n",
    "## Tensorboard\n",
    "- Input at command: tensorboard --logdir=./log\n",
    "- Input at browser: http://127.0.0.1:6006\n",
    "\n",
    "## Reference\n",
    "- https://www.kaggle.com/codename007/a-very-extensive-landmark-exploratory-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: Google_LandMark_Rec_LSTM_Bidirectional_20180328_163937\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "project_name = 'Google_LandMark_Rec'\n",
    "step_name = 'LSTM_Bidirectional'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = project_name + '_' + step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/input\n",
      "output_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/output\n",
      "model_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/model\n",
      "feature_folder: \t\t/data1/kaggle/landmark-recognition-challenge/feature\n",
      "post_pca_feature_folder: \t/data1/kaggle/landmark-recognition-challenge/post_pca_feature\n",
      "log_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/log\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "post_pca_feature_folder = os.path.join(cwd, 'post_pca_feature')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t' + input_folder)\n",
    "print('output_folder: \\t\\t\\t' + output_folder)\n",
    "print('model_folder: \\t\\t\\t' + model_folder)\n",
    "print('feature_folder: \\t\\t' + feature_folder)\n",
    "print('post_pca_feature_folder: \\t' + post_pca_feature_folder)\n",
    "print('log_folder: \\t\\t\\t' + log_folder)\n",
    "\n",
    "org_train_folder = os.path.join(input_folder, 'org_train')\n",
    "org_test_folder = os.path.join(input_folder, 'org_test')\n",
    "train_folder = os.path.join(input_folder, 'data_train')\n",
    "test_folder = os.path.join(input_folder, 'data_test')\n",
    "test_sub_folder = os.path.join(test_folder, 'test')\n",
    "\n",
    "if not os.path.exists(post_pca_feature_folder):\n",
    "    os.mkdir(post_pca_feature_folder)\n",
    "    print('Create folder: %s' % post_pca_feature_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_folder = os.path.join(input_folder, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取landmark_id集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv.shape is (1225029, 3).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cacf8152e2d2ae60</td>\n",
       "      <td>http://static.panoramio.com/photos/original/70...</td>\n",
       "      <td>4676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a58358a2afd3e4e</td>\n",
       "      <td>http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...</td>\n",
       "      <td>6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6b2bb500b6a38aa0</td>\n",
       "      <td>http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...</td>\n",
       "      <td>11284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b399f09dee9c3c67</td>\n",
       "      <td>https://lh3.googleusercontent.com/-LOW2cjAqubA...</td>\n",
       "      <td>8429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19ace29d77a5be66</td>\n",
       "      <td>https://lh5.googleusercontent.com/-tnmSXwQcWL8...</td>\n",
       "      <td>6231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2c9c54b62f0a6a37</td>\n",
       "      <td>https://lh5.googleusercontent.com/-mEaSECO7D-4...</td>\n",
       "      <td>10400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0aac70a1de44ced6</td>\n",
       "      <td>http://lh6.ggpht.com/-cJMh9AYQGk8/SOkF_Q5PrjI/...</td>\n",
       "      <td>9779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de770bc720f8e714</td>\n",
       "      <td>https://lh4.googleusercontent.com/-Q_FvRlwaaa8...</td>\n",
       "      <td>11288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dc9457d703e612ad</td>\n",
       "      <td>https://lh3.googleusercontent.com/-Px33Q-wekRI...</td>\n",
       "      <td>13170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3060f5f75d936abb</td>\n",
       "      <td>http://lh3.ggpht.com/-KXyELwqwp_Q/Ry-qmQAqwUI/...</td>\n",
       "      <td>6051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url  \\\n",
       "0  cacf8152e2d2ae60  http://static.panoramio.com/photos/original/70...   \n",
       "1  0a58358a2afd3e4e  http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...   \n",
       "2  6b2bb500b6a38aa0  http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...   \n",
       "3  b399f09dee9c3c67  https://lh3.googleusercontent.com/-LOW2cjAqubA...   \n",
       "4  19ace29d77a5be66  https://lh5.googleusercontent.com/-tnmSXwQcWL8...   \n",
       "5  2c9c54b62f0a6a37  https://lh5.googleusercontent.com/-mEaSECO7D-4...   \n",
       "6  0aac70a1de44ced6  http://lh6.ggpht.com/-cJMh9AYQGk8/SOkF_Q5PrjI/...   \n",
       "7  de770bc720f8e714  https://lh4.googleusercontent.com/-Q_FvRlwaaa8...   \n",
       "8  dc9457d703e612ad  https://lh3.googleusercontent.com/-Px33Q-wekRI...   \n",
       "9  3060f5f75d936abb  http://lh3.ggpht.com/-KXyELwqwp_Q/Ry-qmQAqwUI/...   \n",
       "\n",
       "   landmark_id  \n",
       "0         4676  \n",
       "1         6651  \n",
       "2        11284  \n",
       "3         8429  \n",
       "4         6231  \n",
       "5        10400  \n",
       "6         9779  \n",
       "7        11288  \n",
       "8        13170  \n",
       "9         6051  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_csv.shape is (117703, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000088da12d664db</td>\n",
       "      <td>https://lh3.googleusercontent.com/-k45wfamuhT8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001623c6d808702</td>\n",
       "      <td>https://lh3.googleusercontent.com/-OQ0ywv8KVIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001bbb682d45002</td>\n",
       "      <td>https://lh3.googleusercontent.com/-kloLenz1xZk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002362830cfe3a3</td>\n",
       "      <td>https://lh3.googleusercontent.com/-N6z79jNZYTg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000270c9100de789</td>\n",
       "      <td>https://lh3.googleusercontent.com/-keriHaVOq1U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0002b0fab5d3ccc4</td>\n",
       "      <td>https://lh3.googleusercontent.com/-ciWklpsrab8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000396be3c24830a</td>\n",
       "      <td>https://lh3.googleusercontent.com/-6W9F179t59Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000506dc6ab3a40e</td>\n",
       "      <td>https://lh3.googleusercontent.com/-_XHsAXB2LZA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0005292fc4b005a3</td>\n",
       "      <td>https://lh3.googleusercontent.com/-RBZ4F1ZKNc0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0005456a82264bc8</td>\n",
       "      <td>https://lh3.googleusercontent.com/-MRK7_uiKO6A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url\n",
       "0  000088da12d664db  https://lh3.googleusercontent.com/-k45wfamuhT8...\n",
       "1  0001623c6d808702  https://lh3.googleusercontent.com/-OQ0ywv8KVIA...\n",
       "2  0001bbb682d45002  https://lh3.googleusercontent.com/-kloLenz1xZk...\n",
       "3  0002362830cfe3a3  https://lh3.googleusercontent.com/-N6z79jNZYTg...\n",
       "4  000270c9100de789  https://lh3.googleusercontent.com/-keriHaVOq1U...\n",
       "5  0002b0fab5d3ccc4  https://lh3.googleusercontent.com/-ciWklpsrab8...\n",
       "6  000396be3c24830a  https://lh3.googleusercontent.com/-6W9F179t59Q...\n",
       "7  000506dc6ab3a40e  https://lh3.googleusercontent.com/-_XHsAXB2LZA...\n",
       "8  0005292fc4b005a3  https://lh3.googleusercontent.com/-RBZ4F1ZKNc0...\n",
       "9  0005456a82264bc8  https://lh3.googleusercontent.com/-MRK7_uiKO6A..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(train_csv_file)\n",
    "print('train_csv.shape is {0}.'.format(train_csv.shape))\n",
    "display(train_csv.head(10))\n",
    "\n",
    "test_csv = pd.read_csv(test_csv_file)\n",
    "print('test_csv.shape is {0}.'.format(test_csv.shape))\n",
    "display(test_csv.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "len(unique_landmark_ids)=14951\n"
     ]
    }
   ],
   "source": [
    "train_id = train_csv['id']\n",
    "train_landmark_id = train_csv['landmark_id']\n",
    "unique_landmark_ids = list(set(train_landmark_id))\n",
    "len_unique_landmark_ids = len(unique_landmark_ids)\n",
    "print(unique_landmark_ids[:10]) # 确认landmark_id是从0开始\n",
    "print('len(unique_landmark_ids)=%d' % len_unique_landmark_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预览sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission_csv.shape is (117703, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000088da12d664db</td>\n",
       "      <td>8815 0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001623c6d808702</td>\n",
       "      <td>7249 0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  landmarks\n",
       "0  000088da12d664db  8815 0.03\n",
       "1  0001623c6d808702  7249 0.61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_submission_csv = pd.read_csv(sample_submission_folder)\n",
    "print('sample_submission_csv.shape is {0}.'.format(sample_submission_csv.shape))\n",
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 8 ms, total: 132 ms\n",
      "Wall time: 186 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(2018)\n",
    "\n",
    "\n",
    "def load_post_pac_feature(model_name, n_components, image_size, time_str, post_pca_feature_folder):\n",
    "    x_data = {}\n",
    "    y_data = {}\n",
    "    x_test = {}\n",
    "    \n",
    "    feature_model = os.path.join(post_pca_feature_folder, 'post_pca_feature_%s_%s_%s_%s.h5' % (model_name, n_components, image_size, time_str))\n",
    "    for filename in [feature_model]:\n",
    "        with h5py.File(filename, 'r') as h:\n",
    "            x_data = np.array(h['train'])\n",
    "            y_data = np.array(h['train_labels'])\n",
    "            x_test = np.array(h['test'])\n",
    "    return x_data, y_data, x_test\n",
    "\n",
    "def load_feature(model_name, image_size, time_str, feature_folder):\n",
    "    x_data = {}\n",
    "    y_data = {}\n",
    "    x_test = {}\n",
    "    \n",
    "    feature_model = os.path.join(feature_folder, 'feature_%s_%s_%s.h5' % (model_name, image_size, time_str))\n",
    "    for filename in [feature_model]:\n",
    "        with h5py.File(filename, 'r') as h:\n",
    "            x_data = np.array(h['train'])\n",
    "            y_data = np.array(h['train_labels'])\n",
    "            x_test = np.array(h['test'])\n",
    "    return x_data, y_data, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "(1219426, 2048)\n",
      "1219426\n",
      "(115942, 2048)\n",
      "************************************************************\n",
      "(1219426, 2048)\n",
      "1219426\n",
      "(115942, 2048)\n",
      "CPU times: user 6.52 s, sys: 25.5 s, total: 32 s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_data_Xception, y_data_Xception, x_test_Xception = load_feature('Xception', 200, '20180312-050926', feature_folder)\n",
    "print('*' * 60)\n",
    "print(x_data_Xception.shape)\n",
    "print(len(y_data_Xception))\n",
    "print(x_test_Xception.shape)\n",
    "\n",
    "x_data_InceptionV3, y_data_InceptionV3, x_test_InceptionV3 = load_feature('InceptionV3', 200, '20180312-050926', feature_folder)\n",
    "print('*' * 60)\n",
    "print(x_data_InceptionV3.shape)\n",
    "print(len(y_data_InceptionV3))\n",
    "print(x_test_InceptionV3.shape)\n",
    "\n",
    "# x_data_InceptionResNetV2, y_data_InceptionResNetV2, x_test_InceptionResNetV2 = load_feature('InceptionResNetV2', 200, '20180312-050926', feature_folder)\n",
    "# print('*' * 60)\n",
    "# print(x_data_InceptionResNetV2.shape)\n",
    "# print(len(y_data_InceptionResNetV2))\n",
    "# print(x_test_InceptionResNetV2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def describe(arr):\n",
    "    print(arr.shape, arr.min(), arr.max(), sys.getsizeof(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1219426, 2, 2048) 0.0 22.508612 19979075712\n",
      "(115942, 2, 2048) 0.0 17.827248 1899593856\n",
      "(1219426,) 0 14950 4877800\n",
      "CPU times: user 8.98 s, sys: 9.01 s, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_data = np.concatenate([x_data_Xception[:, np.newaxis, :],\n",
    "                         x_data_InceptionV3[:, np.newaxis, :]], axis=1)\n",
    "describe(x_data)\n",
    "del x_data_Xception\n",
    "del x_data_InceptionV3\n",
    "# del band_avg_data\n",
    "gc.collect()\n",
    "\n",
    "x_test = np.concatenate([x_test_Xception[:, np.newaxis, :],\n",
    "                         x_test_InceptionV3[:, np.newaxis, :]], axis=1)\n",
    "describe(x_test)\n",
    "del x_test_Xception\n",
    "del x_test_InceptionV3\n",
    "# del band_avg_test\n",
    "gc.collect()\n",
    "\n",
    "y_data = y_data_Xception\n",
    "describe(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1207231, 2, 2048)\n",
      "(12195, 2, 2048)\n",
      "(1207231,)\n",
      "(12195,)\n",
      "CPU times: user 3.98 s, sys: 7.42 s, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.01, random_state=5)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def data_generator(x_train, y_train, batch_size, num_classes):\n",
    "    len_x_train = len(x_train)\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    while(1):\n",
    "        end_index = start_index + batch_size\n",
    "        if end_index < len_x_train:\n",
    "#             print(start_index, end_index, end=' ')\n",
    "            x_batch = x_train[start_index: end_index, :]\n",
    "            y_batch = y_train[start_index: end_index]\n",
    "            y_batch_cat = to_categorical(y_batch, num_classes)\n",
    "            \n",
    "            start_index = start_index + batch_size\n",
    "#             print(x_batch.shape, y_batch_cat.shape)\n",
    "            yield x_batch, y_batch_cat\n",
    "        else:\n",
    "            end_index = end_index-len_x_train\n",
    "#             print(start_index, end_index, end=' ')\n",
    "            x_batch = np.vstack((x_train[start_index:, :], x_train[:end_index, :]))\n",
    "            y_batch = np.concatenate([y_train[start_index:], y_train[:end_index]])\n",
    "            y_batch_cat = to_categorical(y_batch, num_classes)\n",
    "            \n",
    "            start_index = end_index\n",
    "#             print(x_batch.shape, y_batch_cat.shape)\n",
    "            yield x_batch, y_batch_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 2 14951\n",
      "data_dim: \t2048\n",
      "timesteps: \t2\n",
      "num_classes: \t14951\n",
      "************************************************************\n",
      "train data: 1207231 2048 589\n",
      "(2048, 2, 2048) (2048, 14951)\n",
      "************************************************************\n",
      "val data: 12195 2048 5\n",
      "(2048, 2, 2048) (2048, 14951)\n"
     ]
    }
   ],
   "source": [
    "data_dim = x_data.shape[2]\n",
    "timesteps = x_data.shape[1]\n",
    "num_classes = len(list(set(y_data)))\n",
    "print(data_dim, timesteps, num_classes)\n",
    "print('data_dim: \\t%s' % data_dim)\n",
    "print('timesteps: \\t%s' % timesteps)\n",
    "print('num_classes: \\t%s' % num_classes)\n",
    "\n",
    "\n",
    "print('*' * 60)\n",
    "len_train = len(y_train)\n",
    "batch_size = 2048\n",
    "steps_per_epoch_train = int(len_train/batch_size)\n",
    "print('train data: %s %s %s' % (len_train, batch_size, steps_per_epoch_train))\n",
    "\n",
    "train_gen = data_generator(x_train, y_train, batch_size, num_classes)\n",
    "batch_data = next(train_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "\n",
    "\n",
    "print('*' * 60)\n",
    "len_val = len(y_val)\n",
    "steps_per_epoch_val = int(len_val/batch_size)\n",
    "print('val data: %s %s %s' % (len_val, batch_size, steps_per_epoch_val))\n",
    "\n",
    "val_gen = data_generator(x_val, y_val, batch_size, num_classes)\n",
    "batch_data = next(val_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, Flatten, Conv2D, MaxPooling2D, BatchNormalization, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir:/data1/kaggle/landmark-recognition-challenge/log/Google_LandMark_Rec_LSTM_Bidirectional_20180328_163937\n"
     ]
    }
   ],
   "source": [
    "def get_lr(x):\n",
    "    lr = round(3e-4 * 0.98 ** x, 6)\n",
    "    if lr < 1e-5:\n",
    "        lr = 1e-5\n",
    "    print(lr, end='  ')\n",
    "    return lr\n",
    "\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "annealer = LearningRateScheduler(get_lr)\n",
    "\n",
    "log_dir = os.path.join(log_folder, run_name)\n",
    "print('log_dir:' + log_dir)\n",
    "tensorBoard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(512, return_sequences=True), input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Bidirectional(LSTM(512, return_sequences=True)))  # returns a sequence of vectors of dimension 32\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(512)))  # return a single vector of dimension 32\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 2, 1024)           10489856  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1024)              6295552   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14951)             15324775  \n",
      "=================================================================\n",
      "Total params: 32,110,183\n",
      "Trainable params: 32,110,183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 17/589 [..............................] - ETA: 1:59:30 - loss: 8.7378 - acc: 0.0512"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hist = model.fit_generator(train_gen,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=20, #Increase this when not on Kaggle kernel\n",
    "    verbose=1,  #1 for ETA, 0 for silent\n",
    "    callbacks=[annealer],\n",
    "    max_queue_size=2,\n",
    "    workers=4,\n",
    "    use_multiprocessing=False,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=steps_per_epoch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_acc = model.evaluate_generator(val_gen, steps=steps_per_epoch_val)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_acc = run_name + '_' + str(int(final_acc*10000)).zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = pd.DataFrame(hist.history)\n",
    "histories['epoch'] = hist.epoch\n",
    "print(histories.columns)\n",
    "histories_file = os.path.join(model_folder, run_name_acc + '.csv')\n",
    "histories.to_csv(histories_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(hist.history['loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='r')\n",
    "plt.show()\n",
    "plt.plot(hist.history['acc'], color='b')\n",
    "plt.plot(hist.history['val_acc'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, run_name):\n",
    "    cwd = os.getcwd()\n",
    "    modelPath = os.path.join(cwd, 'model')\n",
    "    if not os.path.isdir(modelPath):\n",
    "        os.mkdir(modelPath)\n",
    "    weigthsFile = os.path.join(modelPath, run_name + '.h5')\n",
    "    model.save(weigthsFile)\n",
    "saveModel(model, run_name_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=128)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里证明os.listdir()得到的图片名称list不正确\n",
    "files = os.listdir(os.path.join(cwd, 'input', 'data_test', 'test'))\n",
    "print(files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里证明ImageDataGenerator()得到的图片名称list才是正确\n",
    "gen = ImageDataGenerator()\n",
    "image_size = (299, 299)\n",
    "batch_size = 128\n",
    "test_generator  = gen.flow_from_directory(test_folder, image_size, shuffle=False, batch_size=batch_size)\n",
    "print('test_generator')\n",
    "print(len(test_generator.filenames))\n",
    "print(test_generator.filenames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_indexes = np.argmax(y_pred, -1)\n",
    "print(max_indexes.shape)\n",
    "\n",
    "test_dict = {}\n",
    "for i, paire in enumerate(zip(test_generator.filenames, max_indexes)):\n",
    "    image_name, indx = paire[0], paire[1]\n",
    "    image_id = image_name[5:-4]\n",
    "#     test_dict[image_id] = '%d %.4f' % (indx, y_pred[i, indx])\n",
    "    test_dict[image_id] = '%d %.4f' % (indx, y_pred[i, indx])\n",
    "\n",
    "#确认图片的id是否能与ImageDataGenerator()对应上\n",
    "for key in list(test_dict.keys())[:10]:\n",
    "    print('%s  %s' % (key, test_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len_sample_submission_csv = len(sample_submission_csv)\n",
    "print('len(len_sample_submission_csv)=%d' % len_sample_submission_csv)\n",
    "count = 0\n",
    "for i in range(len_sample_submission_csv):\n",
    "    image_id = sample_submission_csv.iloc[i, 0]\n",
    "#     landmarks = sample_submission_csv.iloc[i, 1]\n",
    "    if image_id in test_dict:\n",
    "        pred_landmarks = test_dict[image_id]\n",
    "#         print('%s  %s' % (image_id, pred_landmarks))\n",
    "        sample_submission_csv.iloc[i, 1] = pred_landmarks\n",
    "    else:\n",
    "#         print(image_id)\n",
    "#         sample_submission_csv.iloc[i, 1] = '9633 1.0' # 属于9633的类最多，所以全都设置成这个类，可能会比设置成空得到的结果好\n",
    "        sample_submission_csv.iloc[i, 1] = '' # 设置成空\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(int(count/10000), end=' ')\n",
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = os.path.join(output_folder, 'pred_' + run_name_acc + '.csv')\n",
    "sample_submission_csv.to_csv(pred_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_name_acc)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
