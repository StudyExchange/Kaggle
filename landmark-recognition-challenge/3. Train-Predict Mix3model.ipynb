{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train-Predict Mix3model\n",
    "\n",
    "## Result:\n",
    "- Kaggle score: \n",
    "\n",
    "## Tensorboard\n",
    "- Input at command: tensorboard --logdir=./log\n",
    "- Input at browser: http://127.0.0.1:6006\n",
    "\n",
    "## Reference\n",
    "- https://www.kaggle.com/codename007/a-very-extensive-landmark-exploratory-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "import math\n",
    "import pdb\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: Google_LandMark_Rec_3. Train-Predict_Mix3model_20180410_123455\n"
     ]
    }
   ],
   "source": [
    "project_name = 'Google_LandMark_Rec'\n",
    "step_name = '3. Train-Predict_Mix3model'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = project_name + '_' + step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/input\n",
      "output_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/output\n",
      "model_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/model\n",
      "feature_folder: \t\t/data1/kaggle/landmark-recognition-challenge/feature\n",
      "post_pca_feature_folder: \t/data1/kaggle/landmark-recognition-challenge/post_pca_feature\n",
      "log_folder: \t\t\t/data1/kaggle/landmark-recognition-challenge/log\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "post_pca_feature_folder = os.path.join(cwd, 'post_pca_feature')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t' + input_folder)\n",
    "print('output_folder: \\t\\t\\t' + output_folder)\n",
    "print('model_folder: \\t\\t\\t' + model_folder)\n",
    "print('feature_folder: \\t\\t' + feature_folder)\n",
    "print('post_pca_feature_folder: \\t' + post_pca_feature_folder)\n",
    "print('log_folder: \\t\\t\\t' + log_folder)\n",
    "\n",
    "org_train_folder = os.path.join(input_folder, 'org_train')\n",
    "org_test_folder = os.path.join(input_folder, 'org_test')\n",
    "train_folder = os.path.join(input_folder, 'data_train')\n",
    "test_folder = os.path.join(input_folder, 'data_test')\n",
    "test_sub_folder = os.path.join(test_folder, 'test')\n",
    "\n",
    "if not os.path.exists(post_pca_feature_folder):\n",
    "    os.mkdir(post_pca_feature_folder)\n",
    "    print('Create folder: %s' % post_pca_feature_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_folder = os.path.join(input_folder, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取landmark_id集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv.shape is (1225029, 3).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cacf8152e2d2ae60</td>\n",
       "      <td>http://static.panoramio.com/photos/original/70...</td>\n",
       "      <td>4676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a58358a2afd3e4e</td>\n",
       "      <td>http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...</td>\n",
       "      <td>6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6b2bb500b6a38aa0</td>\n",
       "      <td>http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...</td>\n",
       "      <td>11284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b399f09dee9c3c67</td>\n",
       "      <td>https://lh3.googleusercontent.com/-LOW2cjAqubA...</td>\n",
       "      <td>8429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19ace29d77a5be66</td>\n",
       "      <td>https://lh5.googleusercontent.com/-tnmSXwQcWL8...</td>\n",
       "      <td>6231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2c9c54b62f0a6a37</td>\n",
       "      <td>https://lh5.googleusercontent.com/-mEaSECO7D-4...</td>\n",
       "      <td>10400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0aac70a1de44ced6</td>\n",
       "      <td>http://lh6.ggpht.com/-cJMh9AYQGk8/SOkF_Q5PrjI/...</td>\n",
       "      <td>9779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de770bc720f8e714</td>\n",
       "      <td>https://lh4.googleusercontent.com/-Q_FvRlwaaa8...</td>\n",
       "      <td>11288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dc9457d703e612ad</td>\n",
       "      <td>https://lh3.googleusercontent.com/-Px33Q-wekRI...</td>\n",
       "      <td>13170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3060f5f75d936abb</td>\n",
       "      <td>http://lh3.ggpht.com/-KXyELwqwp_Q/Ry-qmQAqwUI/...</td>\n",
       "      <td>6051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url  \\\n",
       "0  cacf8152e2d2ae60  http://static.panoramio.com/photos/original/70...   \n",
       "1  0a58358a2afd3e4e  http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...   \n",
       "2  6b2bb500b6a38aa0  http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...   \n",
       "3  b399f09dee9c3c67  https://lh3.googleusercontent.com/-LOW2cjAqubA...   \n",
       "4  19ace29d77a5be66  https://lh5.googleusercontent.com/-tnmSXwQcWL8...   \n",
       "5  2c9c54b62f0a6a37  https://lh5.googleusercontent.com/-mEaSECO7D-4...   \n",
       "6  0aac70a1de44ced6  http://lh6.ggpht.com/-cJMh9AYQGk8/SOkF_Q5PrjI/...   \n",
       "7  de770bc720f8e714  https://lh4.googleusercontent.com/-Q_FvRlwaaa8...   \n",
       "8  dc9457d703e612ad  https://lh3.googleusercontent.com/-Px33Q-wekRI...   \n",
       "9  3060f5f75d936abb  http://lh3.ggpht.com/-KXyELwqwp_Q/Ry-qmQAqwUI/...   \n",
       "\n",
       "   landmark_id  \n",
       "0         4676  \n",
       "1         6651  \n",
       "2        11284  \n",
       "3         8429  \n",
       "4         6231  \n",
       "5        10400  \n",
       "6         9779  \n",
       "7        11288  \n",
       "8        13170  \n",
       "9         6051  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_csv.shape is (117703, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000088da12d664db</td>\n",
       "      <td>https://lh3.googleusercontent.com/-k45wfamuhT8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001623c6d808702</td>\n",
       "      <td>https://lh3.googleusercontent.com/-OQ0ywv8KVIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001bbb682d45002</td>\n",
       "      <td>https://lh3.googleusercontent.com/-kloLenz1xZk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002362830cfe3a3</td>\n",
       "      <td>https://lh3.googleusercontent.com/-N6z79jNZYTg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000270c9100de789</td>\n",
       "      <td>https://lh3.googleusercontent.com/-keriHaVOq1U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0002b0fab5d3ccc4</td>\n",
       "      <td>https://lh3.googleusercontent.com/-ciWklpsrab8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000396be3c24830a</td>\n",
       "      <td>https://lh3.googleusercontent.com/-6W9F179t59Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000506dc6ab3a40e</td>\n",
       "      <td>https://lh3.googleusercontent.com/-_XHsAXB2LZA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0005292fc4b005a3</td>\n",
       "      <td>https://lh3.googleusercontent.com/-RBZ4F1ZKNc0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0005456a82264bc8</td>\n",
       "      <td>https://lh3.googleusercontent.com/-MRK7_uiKO6A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url\n",
       "0  000088da12d664db  https://lh3.googleusercontent.com/-k45wfamuhT8...\n",
       "1  0001623c6d808702  https://lh3.googleusercontent.com/-OQ0ywv8KVIA...\n",
       "2  0001bbb682d45002  https://lh3.googleusercontent.com/-kloLenz1xZk...\n",
       "3  0002362830cfe3a3  https://lh3.googleusercontent.com/-N6z79jNZYTg...\n",
       "4  000270c9100de789  https://lh3.googleusercontent.com/-keriHaVOq1U...\n",
       "5  0002b0fab5d3ccc4  https://lh3.googleusercontent.com/-ciWklpsrab8...\n",
       "6  000396be3c24830a  https://lh3.googleusercontent.com/-6W9F179t59Q...\n",
       "7  000506dc6ab3a40e  https://lh3.googleusercontent.com/-_XHsAXB2LZA...\n",
       "8  0005292fc4b005a3  https://lh3.googleusercontent.com/-RBZ4F1ZKNc0...\n",
       "9  0005456a82264bc8  https://lh3.googleusercontent.com/-MRK7_uiKO6A..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(train_csv_file)\n",
    "print('train_csv.shape is {0}.'.format(train_csv.shape))\n",
    "display(train_csv.head(10))\n",
    "\n",
    "test_csv = pd.read_csv(test_csv_file)\n",
    "print('test_csv.shape is {0}.'.format(test_csv.shape))\n",
    "display(test_csv.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "len(unique_landmark_ids)=14951\n"
     ]
    }
   ],
   "source": [
    "train_id = train_csv['id']\n",
    "train_landmark_id = train_csv['landmark_id']\n",
    "unique_landmark_ids = list(set(train_landmark_id))\n",
    "len_unique_landmark_ids = len(unique_landmark_ids)\n",
    "print(unique_landmark_ids[:10]) # 确认landmark_id是从0开始\n",
    "print('len(unique_landmark_ids)=%d' % len_unique_landmark_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预览sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission_csv.shape is (117703, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000088da12d664db</td>\n",
       "      <td>8815 0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001623c6d808702</td>\n",
       "      <td>7249 0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  landmarks\n",
       "0  000088da12d664db  8815 0.03\n",
       "1  0001623c6d808702  7249 0.61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_submission_csv = pd.read_csv(sample_submission_folder)\n",
    "print('sample_submission_csv.shape is {0}.'.format(sample_submission_csv.shape))\n",
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(file_name):\n",
    "    print(file_name)\n",
    "    with h5py.File(file_name, 'r') as h:\n",
    "        x_train = np.array(np.array(h['train']))\n",
    "        y_train = np.array(h['train_labels'])\n",
    "#         x_val.append(np.array(h['val']))\n",
    "#         y_val = np.array(h['val_labels'])\n",
    "        x_test = np.array(np.array(h['test']))\n",
    "        return x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionV3_200_20180312-050926.h5\n",
      "(1219426, 2048)\n",
      "(1219426,)\n",
      "(115942, 2048)\n",
      "len_data: 1219426\n",
      "len_test: 115942\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "CPU times: user 5.91 s, sys: 19.1 s, total: 25 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_name = 'InceptionV3'\n",
    "time_str = '200_20180312-050926'\n",
    "feature_Xception = os.path.join(feature_folder, 'feature_%s_%s.h5' % (model_name, time_str))\n",
    "\n",
    "x_train, y_train, x_test = load_h5(feature_Xception)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "len_data = x_train.shape[0]\n",
    "len_test = x_test.shape[0]\n",
    "print('len_data: %s' % len_data)\n",
    "print('len_test: %s' % len_test)\n",
    "\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[600092, 1157496, 502486, 314777, 24106, 601449, 1142329, 877254, 506515, 750788]\n",
      "CPU times: user 528 ms, sys: 32 ms, total: 560 ms\n",
      "Wall time: 557 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.utils import shuffle\n",
    "data_indeces = list(range(len_data))\n",
    "print(data_indeces[:10])\n",
    "\n",
    "data_indeces = shuffle(data_indeces)\n",
    "print(data_indeces[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_indeces)=297000\n",
      "[600092, 1157496, 502486, 314777, 24106, 601449, 1142329, 877254, 506515, 750788]\n",
      "len(val_indeces)=3000\n",
      "[122593, 80372, 90019, 1105459, 984448, 308365, 821931, 528256, 579445, 141403]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_indeces, val_indeces = train_test_split(data_indeces[:30*10000], test_size=0.01, random_state=2018, shuffle=False)\n",
    "\n",
    "print('len(train_indeces)=%s' % len(train_indeces))\n",
    "print(train_indeces[:10])\n",
    "print('len(val_indeces)=%s' % len(val_indeces))\n",
    "print(val_indeces[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_data(feature_folder, file_reg, model_name, time_str):\n",
    "    x_data = {}\n",
    "    y_data = {}\n",
    "    \n",
    "    feature_model = os.path.join(feature_folder, file_reg % (model_name, time_str))\n",
    "    with h5py.File(feature_model, 'r') as h:\n",
    "        x_data = np.array(h['train'])\n",
    "        y_data = np.array(h['train_labels'])\n",
    "    return x_data, y_data\n",
    "\n",
    "def load_h5_test(feature_folder, file_reg, model_name, time_str):\n",
    "    x_test = {}\n",
    "    \n",
    "    feature_model = os.path.join(feature_folder, file_reg % (model_name, time_str))\n",
    "    with h5py.File(feature_model, 'r') as h:\n",
    "        x_test = np.array(h['test'])\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_Xception_200_20180312-050926.h5\n",
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_Xception_150_20180311-151108.h5\n",
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionV3_200_20180312-050926.h5\n",
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionV3_150_20180311-151108.h5\n",
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionResNetV2_200_20180312-050926.h5\n",
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionResNetV2_150_20180311-151108.h5\n",
      "True\n",
      "  200_20180312-050926\n",
      "  150_20180311-151108\n",
      "  200_20180312-050926\n",
      "  150_20180311-151108\n",
      "  200_20180312-050926\n",
      "  150_20180311-151108\n",
      "  200_20180312-050926\n",
      "  150_20180311-151108\n",
      "  200_20180312-050926\n",
      "  150_20180311-151108\n"
     ]
    }
   ],
   "source": [
    "def is_files_existed(feature_folder, file_reg, model_names, time_strs):\n",
    "    for model_name in model_names:\n",
    "        for time_str in time_strs:\n",
    "            file_name = file_reg % (model_name, time_str)\n",
    "            file_path = os.path.join(feature_folder, file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                print('File not existed: %s' % file_path)\n",
    "                return False\n",
    "            else:\n",
    "                print('File existed: %s' % file_path)\n",
    "    return True\n",
    "\n",
    "# Test\n",
    "file_reg = 'feature_%s_%s.h5'\n",
    "model_names = [\n",
    "#     'MobileNet', \n",
    "#     'VGG16',\n",
    "#     'VGG19',\n",
    "#     'ResNet50',\n",
    "#     'DenseNet121',\n",
    "#     'DenseNet169',\n",
    "#     'DenseNet201',\n",
    "    'Xception', \n",
    "    'InceptionV3', \n",
    "    'InceptionResNetV2'\n",
    "]\n",
    "time_strs = [\n",
    "    '200_20180312-050926', \n",
    "    '150_20180311-151108'\n",
    "]\n",
    "\n",
    "print(is_files_existed(feature_folder, file_reg, model_names, time_strs))\n",
    "\n",
    "\n",
    "def time_str_generator(time_strs):\n",
    "    while(1):\n",
    "        for time_str in time_strs:\n",
    "            print('  ' + time_str)\n",
    "            yield time_str\n",
    "            \n",
    "# Test\n",
    "time_str_gen = time_str_generator(time_strs)\n",
    "for i in range(10):\n",
    "    next(time_str_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 14951\n",
      "************************************************************\n",
      "timesteps: 1\n",
      "len(train_data): 297000\n",
      "batch_size: 8\n",
      "steps_per_epoch_train: 37125\n",
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionResNetV2_200_20180312-050926.h5\n",
      "  200_20180312-050926\n",
      "(8, 1536) (8, 14951)\n",
      "(8, 1536) (8, 14951)\n",
      "************************************************************\n",
      "len(val_data): 3000\n",
      "batch_size: 8\n",
      "steps_per_epoch_val: 376\n",
      "File existed: /data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionResNetV2_200_20180312-050926.h5\n",
      "  200_20180312-050926\n",
      "(8, 1536) (8, 14951)\n",
      "(8, 1536) (8, 14951)\n",
      "********************************************************************************\n",
      "data_dim: 1536\n",
      "CPU times: user 5.96 s, sys: 20.4 s, total: 26.4 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_time_str_feature_data(data_indeces, feature_folder, file_reg, model_names, time_str):\n",
    "    x_data_time_strs = []\n",
    "    y_data_time_strs = None\n",
    "    for model_name in model_names:\n",
    "        x_data_time_str, y_data_time_str = load_h5_data(feature_folder, file_reg, model_name, time_str)\n",
    "        x_data_time_str, y_data_time_str = x_data_time_str[data_indeces], y_data_time_str[data_indeces]\n",
    "        # Around data to 3 decimals to calculate computation\n",
    "#         x_data_time_str = np.round(x_data_time_str, decimals=3) # 导致后面的全链接神经网络分类器不收敛\n",
    "        x_data_time_strs.append(x_data_time_str)\n",
    "        y_data_time_strs = y_data_time_str\n",
    "    x_data_time_strs = np.concatenate(x_data_time_strs, axis=-1)\n",
    "#     print(x_data_time_strs.shape)\n",
    "#     print(y_data_time_strs.shape)\n",
    "    return x_data_time_strs, y_data_time_strs\n",
    "\n",
    "def data_generator_folder(data_indeces, feature_folder, file_reg, model_names, time_strs, batch_size, num_classes):\n",
    "    assert is_files_existed(feature_folder, file_reg, model_names, time_strs)\n",
    "\n",
    "    time_str_gen = time_str_generator(time_strs)\n",
    "    x_data, y_data = load_time_str_feature_data(data_indeces, feature_folder, file_reg, model_names, next(time_str_gen))\n",
    "    len_x_data = len(x_data)\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    while(1):\n",
    "        end_index = start_index + batch_size\n",
    "        if end_index < len_x_data:\n",
    "#             print(start_index, end_index, end=' ')\n",
    "            x_batch = x_data[start_index: end_index, :]\n",
    "            y_batch = y_data[start_index: end_index]\n",
    "            y_batch_cat = to_categorical(y_batch, num_classes)\n",
    "            \n",
    "            start_index = start_index + batch_size\n",
    "#             print(x_batch.shape, y_batch_cat.shape)\n",
    "            yield x_batch, y_batch_cat\n",
    "        else:\n",
    "            end_index = end_index-len_x_data\n",
    "#             print(start_index, end_index, end=' ')\n",
    "            x_data_old = np.array(x_data[start_index:, :], copy=True)\n",
    "            y_data_old = np.array(y_data[start_index:], copy=True)\n",
    "            # Load new datas\n",
    "            x_data, y_data = load_time_str_feature_data(data_indeces, feature_folder, file_reg, model_names, next(time_str_gen))\n",
    "#             x_data, y_data = shuffle(x_data, y_data, random_state=2018)\n",
    "            len_x_data = len(x_data)\n",
    "            gc.collect()\n",
    "            x_batch = np.vstack((x_data_old, x_data[:end_index, :]))\n",
    "            y_batch = np.concatenate([y_data_old, y_data[:end_index]])\n",
    "            y_batch_cat = to_categorical(y_batch, num_classes)\n",
    "            \n",
    "            start_index = end_index\n",
    "#             print(x_batch.shape, y_batch_cat.shape)\n",
    "            yield x_batch, y_batch_cat\n",
    "        \n",
    "    \n",
    "# x_train = np.concatenate([x_train_Xception, x_train_InceptionV3, x_train_InceptionResNetV2], axis=-1)\n",
    "\n",
    "num_classes = len_unique_landmark_ids\n",
    "print('num_classes: %s' % num_classes)\n",
    "\n",
    "file_reg = 'feature_%s_%s.h5'\n",
    "model_names = [\n",
    "#     'MobileNet', \n",
    "#     'VGG16',\n",
    "#     'VGG19',\n",
    "#     'ResNet50',\n",
    "#     'DenseNet121',\n",
    "#     'DenseNet169',\n",
    "#     'DenseNet201',\n",
    "#     'Xception', \n",
    "#     'InceptionV3', \n",
    "    'InceptionResNetV2'\n",
    "]\n",
    "time_strs = [\n",
    "    '200_20180312-050926', \n",
    "#     '150_20180311-151108'\n",
    "]\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "print('*' * 60)\n",
    "timesteps = len(model_names)\n",
    "len_train_csv = len(train_indeces)\n",
    "steps_per_epoch_train = int(len_train_csv/batch_size)\n",
    "print('timesteps: %s' % timesteps)\n",
    "print('len(train_data): %s' % len_train_csv)\n",
    "print('batch_size: %s' % batch_size)\n",
    "print('steps_per_epoch_train: %s' % steps_per_epoch_train)\n",
    "\n",
    "train_gen = data_generator_folder(train_indeces, feature_folder, file_reg, model_names, time_strs, batch_size, num_classes)\n",
    "batch_data = next(train_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "batch_data = next(train_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "# for i in range(steps_per_epoch_train*5):\n",
    "#     next(train_gen)\n",
    "\n",
    "print('*' * 60)\n",
    "len_val_csv = len(val_indeces)\n",
    "steps_per_epoch_val = int(len_val_csv/batch_size) + 1\n",
    "print('len(val_data): %s' % len_val_csv)\n",
    "print('batch_size: %s' % batch_size)\n",
    "print('steps_per_epoch_val: %s' % steps_per_epoch_val)\n",
    "val_gen = data_generator_folder(val_indeces, feature_folder, file_reg, model_names, time_strs, batch_size, num_classes)\n",
    "batch_data = next(val_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "batch_data = next(val_gen)\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "\n",
    "print('*' * 80)\n",
    "data_dim = batch_data[0].shape[-1]\n",
    "print('data_dim: %s' % data_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115942, 1536)\n",
      "CPU times: user 396 ms, sys: 1.27 s, total: 1.67 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_time_str_feature_test(feature_folder, file_reg, model_names, time_str):\n",
    "    x_test_time_strs = []\n",
    "    for model_name in model_names:\n",
    "        file_name = file_reg % (model_name, time_str)\n",
    "        file_path = os.path.join(feature_folder, file_name)\n",
    "        x_test_time_str = load_h5_test(feature_folder, file_reg, model_name, time_str)\n",
    "#         x_test_time_str = np.round(x_test_time_str, decimals=3) # 导致后面的全链接神经网络分类器不收敛\n",
    "        x_test_time_strs.append(x_test_time_str)\n",
    "    x_test_time_strs = np.concatenate(x_test_time_strs, axis=-1)\n",
    "#     print(x_test_time_strs.shape)\n",
    "    return x_test_time_strs\n",
    "\n",
    "x_test = load_time_str_feature_test(feature_folder, file_reg, model_names, time_strs[0])\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1219426, 2048)\n",
      "(1219426,)\n",
      "(115942, 1536)\n",
      "len_data: 1219426\n",
      "len_test: 115942\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "len_data = x_train.shape[0]\n",
    "len_test = x_test.shape[0]\n",
    "print('len_data: %s' % len_data)\n",
    "print('len_test: %s' % len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir:/data1/kaggle/landmark-recognition-challenge/log/Google_LandMark_Rec_3. Train-Predict_Mix3model_20180410_123455\n"
     ]
    }
   ],
   "source": [
    "def get_lr(x):\n",
    "    lr = round(3e-4 * 0.97 ** x, 6)\n",
    "    if lr < 5e-4:\n",
    "        lr = 5e-4\n",
    "    print(lr, end='  ')\n",
    "    return lr\n",
    "\n",
    "# annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "annealer = LearningRateScheduler(get_lr)\n",
    "callbacks = []\n",
    "callbacks = [annealer]\n",
    "\n",
    "log_dir = os.path.join(log_folder, run_name)\n",
    "print('log_dir:' + log_dir)\n",
    "tensorBoard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8192, input_shape=x_test.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len_unique_landmark_ids, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              12591104  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14951)             61254247  \n",
      "=================================================================\n",
      "Total params: 107,453,031\n",
      "Trainable params: 107,428,455\n",
      "Non-trainable params: 24,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# hist = model.fit(\n",
    "#     x=x_train,\n",
    "#     y=y_train,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=20, #Increase this when not on Kaggle kernel\n",
    "#     verbose=1,  #1 for ETA, 0 for silent\n",
    "#     callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 5.6397 - acc: 0.2443  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 5.6397 - acc: 0.2443  200_20180312-050926\n",
      "37125/37125 [==============================] - 5072s 137ms/step - loss: 5.6396 - acc: 0.2443 - val_loss: 4.3697 - val_acc: 0.3647\n",
      "Epoch 2/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 4.5737 - acc: 0.3318  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 4.5736 - acc: 0.3318  200_20180312-050926\n",
      "37125/37125 [==============================] - 5077s 137ms/step - loss: 4.5736 - acc: 0.3318 - val_loss: 4.4138 - val_acc: 0.3787\n",
      "Epoch 3/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 4.3592 - acc: 0.3544  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 4.3592 - acc: 0.3544  200_20180312-050926\n",
      "37125/37125 [==============================] - 5082s 137ms/step - loss: 4.3591 - acc: 0.3544 - val_loss: 4.5057 - val_acc: 0.3833\n",
      "Epoch 4/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 4.2075 - acc: 0.3672  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 4.2074 - acc: 0.3672  200_20180312-050926\n",
      "37125/37125 [==============================] - 5080s 137ms/step - loss: 4.2074 - acc: 0.3672 - val_loss: 4.7812 - val_acc: 0.3700\n",
      "Epoch 5/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 4.1299 - acc: 0.3727  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 4.1299 - acc: 0.3727  200_20180312-050926\n",
      "37125/37125 [==============================] - 5077s 137ms/step - loss: 4.1298 - acc: 0.3727 - val_loss: 4.9044 - val_acc: 0.3707\n",
      "Epoch 6/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 4.0501 - acc: 0.3786  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 4.0501 - acc: 0.3786  200_20180312-050926\n",
      "37125/37125 [==============================] - 5076s 137ms/step - loss: 4.0500 - acc: 0.3786 - val_loss: 5.1547 - val_acc: 0.3680\n",
      "Epoch 7/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 3.9810 - acc: 0.3833  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 3.9810 - acc: 0.3833  200_20180312-050926\n",
      "37125/37125 [==============================] - 5076s 137ms/step - loss: 3.9810 - acc: 0.3833 - val_loss: 5.1469 - val_acc: 0.3617\n",
      "Epoch 8/20\n",
      "37120/37125 [============================>.] - ETA: 0s - loss: 3.9354 - acc: 0.3863  200_20180312-050926\n",
      "37124/37125 [============================>.] - ETA: 0s - loss: 3.9354 - acc: 0.3863  200_20180312-050926\n",
      "37125/37125 [==============================] - 5079s 137ms/step - loss: 3.9353 - acc: 0.3863 - val_loss: 5.0836 - val_acc: 0.3680\n",
      "Epoch 9/20\n",
      "17540/37125 [=============>................] - ETA: 44:22 - loss: 3.8715 - acc: 0.3920"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hist = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=20, #Increase this when not on Kaggle kernel\n",
    "    verbose=1,  #1 for ETA, 0 for silent\n",
    "    callbacks=callbacks,\n",
    "    max_queue_size=2,\n",
    "    workers=4,\n",
    "    use_multiprocessing=False,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=steps_per_epoch_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_acc = model.evaluate_generator(val_gen, steps=steps_per_epoch_val)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name_acc = run_name + '_' + str(int(final_acc*10000)).zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = pd.DataFrame(hist.history)\n",
    "histories['epoch'] = hist.epoch\n",
    "print(histories.columns)\n",
    "histories_file = os.path.join(model_folder, run_name_acc + '.csv')\n",
    "histories.to_csv(histories_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(hist.history['loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='r')\n",
    "plt.show()\n",
    "plt.plot(hist.history['acc'], color='b')\n",
    "plt.plot(hist.history['val_acc'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, run_name):\n",
    "    cwd = os.getcwd()\n",
    "    modelPath = os.path.join(cwd, 'model')\n",
    "    if not os.path.isdir(modelPath):\n",
    "        os.mkdir(modelPath)\n",
    "    weigthsFile = os.path.join(modelPath, run_name + '.h5')\n",
    "    model.save(weigthsFile)\n",
    "saveModel(model, run_name_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=batch_size)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_file = os.path.join(model_folder, '%s.npy' % run_name_acc)\n",
    "# np.save(y_pred_file, y_pred)\n",
    "\n",
    "# y_pred_reload = np.load(y_pred_file)\n",
    "# print(y_pred_file)\n",
    "# print(y_pred_reload.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里证明os.listdir()得到的图片名称list不正确\n",
    "files = os.listdir(os.path.join(cwd, 'input', 'data_test', 'test'))\n",
    "print(files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里证明ImageDataGenerator()得到的图片名称list才是正确\n",
    "gen = ImageDataGenerator()\n",
    "image_size = (299, 299)\n",
    "# batch_size = 128\n",
    "test_generator  = gen.flow_from_directory(test_folder, image_size, shuffle=False, batch_size=batch_size)\n",
    "print('test_generator')\n",
    "print(len(test_generator.filenames))\n",
    "print(test_generator.filenames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_indexes = np.argmax(y_pred, -1)\n",
    "print(max_indexes.shape)\n",
    "\n",
    "test_dict = {}\n",
    "for i, paire in enumerate(zip(test_generator.filenames, max_indexes)):\n",
    "    image_name, indx = paire[0], paire[1]\n",
    "    image_id = image_name[5:-4]\n",
    "#     test_dict[image_id] = '%d %.4f' % (indx, y_pred[i, indx])\n",
    "    test_dict[image_id] = '%d %.4f' % (indx, y_pred[i, indx])\n",
    "\n",
    "#确认图片的id是否能与ImageDataGenerator()对应上\n",
    "for key in list(test_dict.keys())[:10]:\n",
    "    print('%s  %s' % (key, test_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len_sample_submission_csv = len(sample_submission_csv)\n",
    "print('len(len_sample_submission_csv)=%d' % len_sample_submission_csv)\n",
    "count = 0\n",
    "for i in range(len_sample_submission_csv):\n",
    "    image_id = sample_submission_csv.iloc[i, 0]\n",
    "#     landmarks = sample_submission_csv.iloc[i, 1]\n",
    "    if image_id in test_dict:\n",
    "        pred_landmarks = test_dict[image_id]\n",
    "#         print('%s  %s' % (image_id, pred_landmarks))\n",
    "        sample_submission_csv.iloc[i, 1] = pred_landmarks\n",
    "    else:\n",
    "#         print(image_id)\n",
    "#         sample_submission_csv.iloc[i, 1] = '9633 1.0' # 属于9633的类最多，所以全都设置成这个类，可能会比设置成空得到的结果好\n",
    "        sample_submission_csv.iloc[i, 1] = '' # 设置成空\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(int(count/10000), end=' ')\n",
    "display(sample_submission_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = os.path.join(output_folder, 'pred_' + run_name_acc + '.csv')\n",
    "sample_submission_csv.to_csv(pred_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_name_acc)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
