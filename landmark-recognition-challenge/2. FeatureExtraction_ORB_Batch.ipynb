{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FeatureExtraction_ORB_Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: Google_LandMark_Rec_FeatureExtraction_ORB_Batch_20180512_043724\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'Google_LandMark_Rec'\n",
    "step_name = 'FeatureExtraction_ORB_Batch'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = project_name + '_' + step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PKGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu_amount:  4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import math\n",
    "import shutil\n",
    "import zipfile\n",
    "import pickle\n",
    "import h5py\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cpu_amount = multiprocessing.cpu_count()\n",
    "print('cpu_amount: ', cpu_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "\n",
    "org_train_folder = os.path.join(input_folder, 'org_train')\n",
    "org_test_folder = os.path.join(input_folder, 'org_test')\n",
    "train_folder = os.path.join(input_folder, 'data_train')\n",
    "val_folder = os.path.join(input_folder, 'data_val')\n",
    "test_folder = os.path.join(input_folder, 'data_test')\n",
    "test_sub_folder = os.path.join(test_folder, 'test')\n",
    "\n",
    "train_csv_file = os.path.join(input_folder, 'train.csv')\n",
    "test_csv_file = os.path.join(input_folder, 'test.csv')\n",
    "sample_submission_folder = os.path.join(input_folder, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv.shape is (1225029, 3).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cacf8152e2d2ae60</td>\n",
       "      <td>http://static.panoramio.com/photos/original/70...</td>\n",
       "      <td>4676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a58358a2afd3e4e</td>\n",
       "      <td>http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...</td>\n",
       "      <td>6651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url  \\\n",
       "0  cacf8152e2d2ae60  http://static.panoramio.com/photos/original/70...   \n",
       "1  0a58358a2afd3e4e  http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...   \n",
       "\n",
       "   landmark_id  \n",
       "0         4676  \n",
       "1         6651  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_csv.shape is (117703, 2).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000088da12d664db</td>\n",
       "      <td>https://lh3.googleusercontent.com/-k45wfamuhT8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001623c6d808702</td>\n",
       "      <td>https://lh3.googleusercontent.com/-OQ0ywv8KVIA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url\n",
       "0  000088da12d664db  https://lh3.googleusercontent.com/-k45wfamuhT8...\n",
       "1  0001623c6d808702  https://lh3.googleusercontent.com/-OQ0ywv8KVIA..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(train_csv_file)\n",
    "print('train_csv.shape is {0}.'.format(train_csv.shape))\n",
    "display(train_csv.head(2))\n",
    "\n",
    "test_csv = pd.read_csv(test_csv_file)\n",
    "print('test_csv.shape is {0}.'.format(test_csv.shape))\n",
    "display(test_csv.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_landmark_id) = \t14951\n",
      "len(id_2_landmark_id_dict) = \t1225029\n",
      "id: cacf8152e2d2ae60, \tlandmark_id:4676\n",
      "id: 0a58358a2afd3e4e, \tlandmark_id:6651\n"
     ]
    }
   ],
   "source": [
    "train_id = train_csv['id']\n",
    "train_landmark_id = train_csv['landmark_id']\n",
    "print('len(train_landmark_id) = \\t%s' % len(list(set(train_landmark_id))))\n",
    "\n",
    "id_2_landmark_id_dict = dict(zip(train_id, train_landmark_id))\n",
    "print('len(id_2_landmark_id_dict) = \\t%d' % len(id_2_landmark_id_dict))\n",
    "\n",
    "index = 0\n",
    "print('id: %s, \\tlandmark_id:%s' % (train_id[index], id_2_landmark_id_dict[train_id[index]]))\n",
    "index = 1\n",
    "print('id: %s, \\tlandmark_id:%s' % (train_id[index], id_2_landmark_id_dict[train_id[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureExtraction_ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "/data1/kaggle/landmark-recognition-challenge/input/org_train/69b846bd58c3f09a.jpg\n"
     ]
    }
   ],
   "source": [
    "def image_detect_and_compute(image_file, clf):\n",
    "    \"\"\"Detect and compute interest points and their descriptors.\"\"\"\n",
    "    img = cv2.imread(image_file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kp, des = clf.detectAndCompute(img, None)\n",
    "    return des\n",
    "\n",
    "n_features = 500\n",
    "clf = cv2.ORB_create(n_features)\n",
    "\n",
    "org_train_images = os.listdir(org_train_folder)[:10]\n",
    "print(len(org_train_images))\n",
    "image_file = os.path.join(org_train_folder, org_train_images[0])\n",
    "print(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.68 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def dump_pickle_feature_batch(run_name, dataset_name, image_features, batch_num):\n",
    "    image_features_file = os.path.join(feature_folder, 'feature_%s_%s_b%s.pickle' % (run_name, dataset_name, batch_num))\n",
    "    print('Dump: ', image_features_file, end='  ')\n",
    "    print(len(image_features.keys()))\n",
    "    pickle.dump(image_features, open(image_features_file, \"wb\"), True)\n",
    "\n",
    "def load_pickle_feature_batch(run_name, dataset_name, batch_num):\n",
    "    image_features_file = os.path.join(feature_folder, 'feature_%s_%s_b%s.pickle' % (run_name, dataset_name, batch_num))\n",
    "    image_features = pickle.load(open(image_features_file, \"rb\"))\n",
    "    print('Load: ', image_features_file, end='  ')\n",
    "    print(len(image_features.keys()))\n",
    "    return image_features\n",
    "\n",
    "# dump_pickle_feature(run_name, image_features)\n",
    "# image_features = load_pickle_feature(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(folder, dataset_name, run_name=run_name, batch_size=500):\n",
    "    image_names = os.listdir(folder)\n",
    "    amount = len(image_names)\n",
    "#     amount = 1000\n",
    "    batch_count = int(amount / batch_size) + 1\n",
    "    print('amount: %s, batch_count: %s' % (amount, batch_count))\n",
    "    \n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        image_features = {}\n",
    "        for j, image_name in enumerate(image_names[i*batch_size: (i+1)*batch_size]):\n",
    "            image_id = image_name[:-4]\n",
    "            image_file = os.path.join(folder, image_name)\n",
    "            des = image_detect_and_compute(image_file, clf)\n",
    "            image_features[image_id] = des\n",
    "            if j < 3:\n",
    "                print(image_name, image_id, end=' ')\n",
    "                print(des.shape, end=' ')\n",
    "                print(des[0][:10])\n",
    "            if (j+1) % 1000 == 0:\n",
    "                print(int((j+1)/1000), end=' ')\n",
    "        dump_pickle_feature_batch(run_name, dataset_name, image_features, i)\n",
    "        del image_features\n",
    "        gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount: 1217684, batch_count: 25\n",
      "69b846bd58c3f09a.jpg 69b846bd58c3f09a (500, 32) [128 230 157 251 108 201 109 205  86 119]\n",
      "19a1de4f08cd0305.jpg 19a1de4f08cd0305 (500, 32) [139 120 170 191  44 191 187 208  30 235]\n",
      "4ee821754ef5fd83.jpg 4ee821754ef5fd83 (500, 32) [148 167   6 171 154  11 110 105  46  49]\n",
      "1 2 3 4 5 6 7 8 9 10 "
     ]
    }
   ],
   "source": [
    "feature_extraction(org_train_folder, 'train', run_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = load_pickle_feature_batch(run_name, 'train', 1)\n",
    "\n",
    "print('*'*80)\n",
    "print('len_image_features=', len(image_features.keys()))\n",
    "for i, image_id in enumerate(list(image_features.keys())[:3]):\n",
    "    print('image_id: %s,\\t landmark_id:%s,\\t feature_shape: ' % (image_id, id_2_landmark_id_dict[image_id]), image_features[image_id].shape, end=' ')\n",
    "    print(image_features[image_id][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction(org_test_folder, 'test', run_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = load_pickle_feature_batch(run_name, 'test', 1)\n",
    "\n",
    "print('*'*80)\n",
    "print('len_image_features=', len(image_features.keys()))\n",
    "for i, image_id in enumerate(list(image_features.keys())[:3]):\n",
    "    print('image_id: %s,\\t feature_shape: %s' % (image_id, image_features[image_id].shape), end=' ')\n",
    "    print(image_features[image_id][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time cost: %.2f' % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
