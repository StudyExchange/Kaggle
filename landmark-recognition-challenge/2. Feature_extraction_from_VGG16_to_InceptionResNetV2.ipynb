{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 特征提取_从VGG16到InceptionResNetV2\n",
    "\n",
    "**References**:\n",
    "- https://github.com/ypwhs/dogs_vs_cats\n",
    "- https://www.kaggle.com/yangpeiwen/keras-inception-xception-0-47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: Google_LandMark_Rec_FeatureExtraction_20180421_054321\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "project_name = 'Google_LandMark_Rec'\n",
    "step_name = 'FeatureExtraction'\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "run_name = project_name + '_' + step_name + '_' + time_str\n",
    "print('run_name: ' + run_name)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import shutil\n",
    "import zipfile\n",
    "import pickle\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_folder: \t\t\t /data1/kaggle/landmark-recognition-challenge/input\n",
      "output_folder: \t\t\t /data1/kaggle/landmark-recognition-challenge/output\n",
      "model_folder: \t\t\t /data1/kaggle/landmark-recognition-challenge/model\n",
      "feature_folder: \t\t /data1/kaggle/landmark-recognition-challenge/feature\n",
      "post_pca_feature_folder: \t /data1/kaggle/landmark-recognition-challenge/post_pca_feature\n",
      "log_folder: \t\t\t /data1/kaggle/landmark-recognition-challenge/log\n",
      "\n",
      "org_train_folder: \t\t /data1/kaggle/landmark-recognition-challenge/input/org_train\n",
      "org_test_folder: \t\t /data1/kaggle/landmark-recognition-challenge/input/org_test\n",
      "train_folder: \t\t\t /data1/kaggle/landmark-recognition-challenge/input/data_train\n",
      "val_folder: \t\t\t /data1/kaggle/landmark-recognition-challenge/input/val_train\n",
      "test_folder: \t\t\t /data1/kaggle/landmark-recognition-challenge/input/data_test\n",
      "test_sub_folder: \t\t /data1/kaggle/landmark-recognition-challenge/input/data_test/test\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "input_folder = os.path.join(cwd, 'input')\n",
    "output_folder = os.path.join(cwd, 'output')\n",
    "model_folder = os.path.join(cwd, 'model')\n",
    "feature_folder = os.path.join(cwd, 'feature')\n",
    "post_pca_feature_folder = os.path.join(cwd, 'post_pca_feature')\n",
    "log_folder = os.path.join(cwd, 'log')\n",
    "print('input_folder: \\t\\t\\t', input_folder)\n",
    "print('output_folder: \\t\\t\\t', output_folder)\n",
    "print('model_folder: \\t\\t\\t', model_folder)\n",
    "print('feature_folder: \\t\\t', feature_folder)\n",
    "print('post_pca_feature_folder: \\t', post_pca_feature_folder)\n",
    "print('log_folder: \\t\\t\\t', log_folder)\n",
    "\n",
    "org_train_folder = os.path.join(input_folder, 'org_train')\n",
    "org_test_folder = os.path.join(input_folder, 'org_test')\n",
    "train_folder = os.path.join(input_folder, 'data_train')\n",
    "val_folder = os.path.join(input_folder, 'val_train')\n",
    "test_folder = os.path.join(input_folder, 'data_test')\n",
    "test_sub_folder = os.path.join(test_folder, 'test')\n",
    "print('\\norg_train_folder: \\t\\t', org_train_folder)\n",
    "print('org_test_folder: \\t\\t', org_test_folder)\n",
    "print('train_folder: \\t\\t\\t', train_folder)\n",
    "print('val_folder: \\t\\t\\t', val_folder)\n",
    "print('test_folder: \\t\\t\\t', test_folder)\n",
    "print('test_sub_folder: \\t\\t', test_sub_folder)\n",
    "\n",
    "if not os.path.exists(post_pca_feature_folder):\n",
    "    os.mkdir(post_pca_feature_folder)\n",
    "    print('Create folder: %s' % post_pca_feature_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用预训练权重的VGG16、VGG19、ResNet50、Xception、InceptionV3和InceptionResNetV2模型提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.applications import *\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(MODEL, image_size, date_str, lambda_func=None, batch_size=1, is_aug=False):\n",
    "    print('{0} start.'.format(MODEL.__name__))\n",
    "    cpu_amount = cpu_count()\n",
    "    print('cpu_amount: ', cpu_amount)\n",
    "    start_time = time.time()\n",
    "    width = image_size\n",
    "    height = image_size\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    folder_path = os.path.join(cwd, 'feature')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "        print('Created folder: %s' % folder_path)\n",
    "    else:\n",
    "        print('Existed folder: %s' % folder_path)\n",
    "    file_name = os.path.join(folder_path, 'feature_{0}_{1}_{2}.h5'.format(MODEL.__name__, width, date_str))\n",
    "    print(file_name)\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    \n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        print(lambda_func.__name__)\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', input_shape=(height, width, 3), include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "    \n",
    "    train_folder = os.path.join(cwd, 'input', 'data_train')\n",
    "    val_folder = os.path.join(cwd, 'input', 'data_val')\n",
    "    test_folder  = os.path.join(cwd, 'input', 'data_test')\n",
    "    \n",
    "    if is_aug:\n",
    "        print('have augumentation')\n",
    "        train_gen = ImageDataGenerator(zoom_range = 0.2,\n",
    "                                 height_shift_range = 0.2,\n",
    "                                 width_shift_range = 0.2,\n",
    "                                 rotation_range = 20)\n",
    "    else:\n",
    "        print('do not have augumentation')\n",
    "        train_gen = ImageDataGenerator()\n",
    "    val_gen = ImageDataGenerator()\n",
    "    test_gen = ImageDataGenerator()\n",
    "\n",
    "    train_generator = train_gen.flow_from_directory(\n",
    "        train_folder, \n",
    "        (image_size, image_size), \n",
    "        shuffle=False, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    val_generator = val_gen.flow_from_directory(\n",
    "        val_folder, \n",
    "        (image_size, image_size), \n",
    "        shuffle=False, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    test_generator  = test_gen.flow_from_directory(\n",
    "        test_folder,  \n",
    "        (image_size, image_size), \n",
    "        shuffle=False, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print('train_generator')\n",
    "    print(len(train_generator.filenames))\n",
    "    print(train_generator.filenames[:10])\n",
    "    train_generator_steps = math.ceil(len(train_generator.filenames)/batch_size)\n",
    "    print('train_generator_steps=%d / %d = %d' % (len(train_generator.filenames), batch_size, train_generator_steps))\n",
    "    train = model.predict_generator(train_generator, verbose=1, steps=train_generator_steps, max_queue_size=2048, workers=cpu_amount, use_multiprocessing=True)\n",
    "\n",
    "    print('val_generator')\n",
    "    print(len(val_generator.filenames))\n",
    "    print(val_generator.filenames[:10])\n",
    "    val_generator_steps = math.ceil(len(val_generator.filenames)/batch_size)\n",
    "    print('val_generator_steps=%d' % val_generator_steps)\n",
    "    print('val_generator_steps=%d / %d = %d' % (len(val_generator.filenames), batch_size, val_generator_steps))\n",
    "    val = model.predict_generator(val_generator, verbose=1, steps=val_generator_steps, max_queue_size=2048, workers=cpu_amount, use_multiprocessing=True)\n",
    "\n",
    "    print('test_generator')\n",
    "    print(len(test_generator.filenames))\n",
    "    print(test_generator.filenames[:10])\n",
    "    test_generator_steps = math.ceil(len(test_generator.filenames)/batch_size)\n",
    "    print('test_generator_steps=%d' % test_generator_steps)\n",
    "    print('test_generator_steps=%d / %d = %d' % (len(test_generator.filenames), batch_size, test_generator_steps))\n",
    "    test = model.predict_generator(test_generator, verbose=1, steps=test_generator_steps, max_queue_size=2048, workers=cpu_amount, use_multiprocessing=True)\n",
    "\n",
    "    \n",
    "    with h5py.File(file_name) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"train_labels\", data=train_generator.classes)\n",
    "        h.create_dataset(\"val\", data=val)\n",
    "        h.create_dataset(\"val_labels\", data=val_generator.classes)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "    \n",
    "    print(train.shape)\n",
    "    print(train_generator.classes)\n",
    "    print(val.shape)\n",
    "    print(val_generator.classes)\n",
    "    print(test.shape)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Spend time: {0} s'.format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(image_size=150, batch_size=1, is_aug=False):\n",
    "    time_str = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "    print('*' * 60)\n",
    "    print(time_str)\n",
    "#     get_features(VGG16, image_size, time_str, vgg16.preprocess_input, batch_size, is_aug)\n",
    "#     get_features(Xception, image_size, time_str, xception.preprocess_input, batch_size, is_aug)\n",
    "#     get_features(InceptionV3, image_size, time_str, inception_v3.preprocess_input, batch_size, is_aug)\n",
    "    get_features(InceptionResNetV2, image_size, time_str, inception_resnet_v2.preprocess_input, batch_size, is_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "20180421-054322\n",
      "InceptionResNetV2 start.\n",
      "cpu_amount:  8\n",
      "Existed folder: /data1/kaggle/landmark-recognition-challenge/feature\n",
      "/data1/kaggle/landmark-recognition-challenge/feature/feature_InceptionResNetV2_300_20180421-054322.h5\n",
      "preprocess_input\n",
      "do not have augumentation\n",
      "Found 1193691 images belonging to 14951 classes.\n",
      "Found 24362 images belonging to 14951 classes.\n",
      "Found 115619 images belonging to 1 classes.\n",
      "train_generator\n",
      "1193691\n",
      "['00000/0439f888c5af0e99.jpg', '00000/05aaa786f5c9e0d1.jpg', '00000/063e56d977e00da1.jpg', '00000/06ac932cbf89ce44.jpg', '00000/126ee1b60065dbd4.jpg', '00000/14d86ba0c00b16d1.jpg', '00000/17a57bb3fa8c2d4e.jpg', '00000/1886e9f023806d4a.jpg', '00000/1f7e7418023935ee.jpg', '00000/22e28089dac709f0.jpg']\n",
      "train_generator_steps=1193691 / 512 = 2332\n",
      "2332/2332 [==============================] - 14705s 6s/step\n",
      "val_generator\n",
      "24362\n",
      "['00000/4e8ab93c1620e8a3.jpg', '00000/90187c0b6f3fa112.jpg', '00000/e8f5d139190cf632.jpg', '00003/1ecb7b8fbe3ad95f.jpg', '00003/2a71be02ed724c64.jpg', '00003/2aae948adfc18746.jpg', '00003/61276bb730b3b2ce.jpg', '00003/88effeacfe9d031c.jpg', '00003/deb6b65a01c11ce3.jpg', '00005/0825204942d307da.jpg']\n",
      "val_generator_steps=48\n",
      "val_generator_steps=24362 / 512 = 48\n",
      "48/48 [==============================] - 569s 12s/step\n",
      "test_generator\n",
      "115619\n",
      "['test/000088da12d664db.jpg', 'test/0001623c6d808702.jpg', 'test/0001bbb682d45002.jpg', 'test/0002362830cfe3a3.jpg', 'test/000270c9100de789.jpg', 'test/0002b0fab5d3ccc4.jpg', 'test/000396be3c24830a.jpg', 'test/000506dc6ab3a40e.jpg', 'test/0005292fc4b005a3.jpg', 'test/0005456a82264bc8.jpg']\n",
      "test_generator_steps=226\n",
      "test_generator_steps=115619 / 512 = 226\n",
      "226/226 [==============================] - 2761s 12s/step\n",
      "(1193691, 1536)\n",
      "[    0     0     0 ... 14950 14950 14950]\n",
      "(24362, 1536)\n",
      "[    0     0     0 ... 14945 14947 14950]\n",
      "(115619, 1536)\n",
      "Spend time: 18164.60090994835 s\n"
     ]
    }
   ],
   "source": [
    "get_all_features(300, 512, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 18166.43 s\n",
      "Google_LandMark_Rec_FeatureExtraction_20180421_054321\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Time cost: %.2f s' % (time.time() - t0))\n",
    "\n",
    "print(run_name)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
